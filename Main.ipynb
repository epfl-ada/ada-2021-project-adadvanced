{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone 2\n",
    "\n",
    "Here we will describe the whole pipeline to get all the results we would like to include in the final story (on the final website). We will go through all the different steps and describe as detailed as possible the operations needed. \n",
    "\n",
    "For the final story we decided to focus on the influence of the Brexit. More precisely we would like to assess how the Brexit was perceived and how it evolves along the years. All the different visualizations we aim at providing in the final story are well detailed in this [Section](#Results).\n",
    "\n",
    "## **[Preprocessing steps](#Preprocessing)**\n",
    "\n",
    "As usual the first step consist in several substeps that aims at cleaning and transforming the data. By clicking on the task link, you can access the respective pipeline.\n",
    "- *[Data exploration and Sanity check](#Sanity_check)* : Explore the dataset, check its consistency and get familiar with the different features/information provided into.\n",
    "    - Collaborators assigned to that task: ALL.\n",
    "- *[Data extraction](#extraction)* : Extract the datas of interest that will be further used to perform the tasks related to each idea.\n",
    "    - Collaborators assigned to that task: Arnaud.\n",
    "- *[Data augmentation](#augmentation)* : Perform a data augmentation to get more features about the quotations such as the quote field, the nationality of the speaker and so on... These new features will be further used to perform the tasks related to each idea.\n",
    "    - Collaborators assigned to that task: Jean & Gaelle. \n",
    "- *[Data cleaning](#augmentation)* \n",
    "- *[Quotations and speakers clustering](#clustering)* : Cluster the quotations and the speakers according to the a quotation vector and the added features (data augmentation). This clustering will be further mainly used to develop a recommandation tool.\n",
    "    - Collaborators assigned to that task: Raffaele.\n",
    "\n",
    "## **[Generate the results for the final story](#Results)**\n",
    "\n",
    "- [General Statitics](#Statistics) : \n",
    "- [Country map](#Country) : \n",
    "- [Sector map](#Sector) : \n",
    "- [Visualize speakers evolution](#2Dplot) :\n",
    "- [Recommandation Tool](#Recommandation) :\n",
    "- [Correlation with stocks](#Stocks) :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before diving into the code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have a `Data` folder containing the following files: \n",
    "- The quotebank datasets for each year: `quotes-yyyy.json.bz2`\n",
    "- The speaker attributes folder `speaker-attributes.parquet` as well as the associated lookup table `wikidata_labels_descriptions_quotebank.csv.bz2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import useful librairies and define useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARD LIBRAIRIES\n",
    "import bz2 \n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Dynamic graphs\n",
    "import plotly.express as px\n",
    "\n",
    "# Machine learning librairies\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from tsne_torch import TorchTSNE as TSNE\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load the lexicon for sentiment analysis\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Data files\n",
    "PATHS_TO_FILE = ['Data/quotes-20%d.json.bz2' % i for i in range(15,21)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Preprocessing'></a>\n",
    "\n",
    "# Preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Sanity_check'></a>\n",
    "\n",
    "## Data exploration and Sanity check\n",
    "\n",
    "We decided to perform the following snaity checks on the original datas: \n",
    "\n",
    "- We first check that each entry for each quotation is specified in the right format (e.g. `numOccurences` should be an integer).\n",
    "- We check that the `probas` sum to 1.\n",
    "- We check that the `numOccurences` is superior or equal to the length of the list containing the urls.\n",
    "- The `date` is consistent with the dataset they are coming from\n",
    "- We check that if a `qids` exists then a `speaker` should be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECK FUNCTIONS\n",
    "\n",
    "def check_type(instance,entry,dtype):\n",
    "    return type(instance[entry]) == dtype\n",
    "\n",
    "def check_probas(instance):\n",
    "    if len(instance) > 0:\n",
    "        proba_sum = sum([potential[1] for potential in instance[\"probas\"]])\n",
    "        if proba_sum != 1:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_numOcc(instance):\n",
    "    return (len(instance[\"urls\"]) >= instance[\"numOccurences\"])\n",
    "\n",
    "def check_date(instance,year):\n",
    "    quotation_year = int(instance[\"date\"][:4])\n",
    "    return (quotation_year == year)\n",
    "\n",
    "def check_author_qids(instance):\n",
    "    if len(instance[\"qids\"]) > 0 and instance[\"speaker\"] is None:\n",
    "        return False\n",
    "    else: \n",
    "        return True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the types for each entry\n",
    "TYPES = {\"quoteID\":str,\n",
    "         \"quotation\":str,\n",
    "         \"speaker\":str,\n",
    "         \"qids\":list,\n",
    "         \"date\":str,\n",
    "         \"numOccurrences\":int,\n",
    "         \"probas\":list,\n",
    "         \"urls\":list,\n",
    "         \"phase\":str}\n",
    "\n",
    "error_file = \"Data/error_file.json.bz2\"\n",
    "\n",
    "with bz2.open(error_file, 'wb') as e_file:\n",
    "    # Loop over the different files that we will read\n",
    "    for quotebank_data in PATHS_TO_FILE:\n",
    "        print(\"Reading \",quotebank_data,\" file...\")\n",
    "        # Open the file we want to read\n",
    "        with bz2.open(quotebank_data, 'rb') as s_file:\n",
    "            # Loop over the samples\n",
    "            for instance in s_file:\n",
    "                potential_error = \"\"\n",
    "                # Loading a sample\n",
    "                instance = json.loads(instance)\n",
    "                #### CHECK THE TYPES ####\n",
    "                for key, value in TYPES.items():\n",
    "                    if not check_type(instance,key,value):\n",
    "                        potential_error += \"| Type problem: \" + key + \" |\"\n",
    "                #### CHECK THE PROBAS ####\n",
    "                if not check_probas(instance):\n",
    "                    potential_error += \"| Probas problem |\"\n",
    "                #### CHECK THE DATE ####\n",
    "                if not check_date(instance):\n",
    "                    potential_error += \"| Date problem |\"\n",
    "                #### CHECK THE NUMOCCURENCES ####\n",
    "                if not check_numOcc(instance):\n",
    "                    potential_error += \"| NumOccurences problem |\"\n",
    "                #### CHECK THE AUTHOR-QIDS ####\n",
    "                if not check_author_qids(instance):\n",
    "                    potential_error += \"| Author-qids problem |\"\n",
    "                # WRITE INTO THE FILE FOR POTENTIAL ERRORS #\n",
    "                if len(potential_error) > 0:\n",
    "                    e_file.write((json.dumps(instance)+'\\n').encode('utf-8')) \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='extraction'></a>\n",
    "\n",
    "## Data extraction\n",
    "\n",
    "As mentionned previously, we are planning to analyze the way Brexit is perceived and the way it influenced other things. To be able to perform such tasks, we need first to extract the quotations that are talking about Brexit. To do so we will follow the following pipeline:\n",
    "\n",
    "1. Define a neighborhood containing all the words that are respectively closely related to Brexit. This neighborhood will be a list of words or expressions that are commonly used to refer to Brexit.\n",
    "2. Select all the quotations for which, at least, one word/expression from the vocabulary neighborhood appears in it.\n",
    "3. Store the new two datasets in the `Brexit_quotes.json.bz2` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file\n",
    "PATHS_TO_FILE = ['Data/quotes-20%d.json.bz2' % i for i in range(15,21)]\n",
    "# Output file\n",
    "PATH_TO_OUT = 'Brexit_datas/Brexit_quotes.json.bz2'\n",
    "\n",
    "# Open the file where we will write\n",
    "with bz2.open(PATH_TO_OUT, 'wb') as d_file:\n",
    "    # Loop over the different files that we will read\n",
    "    for quotebank_data in PATHS_TO_FILE:\n",
    "        print(\"Reading \",quotebank_data,\" file...\")\n",
    "        # Open the file we want to read\n",
    "        with bz2.open(quotebank_data, 'rb') as s_file:\n",
    "            # Loop over the samples\n",
    "            for instance in s_file:\n",
    "                # Loading a sample\n",
    "                instance = json.loads(instance)\n",
    "                # Extracting the quotation\n",
    "                quotation = instance['quotation']\n",
    "                # Check if the quotation contains at least one word related to Brexit\n",
    "                if \"brexit\" in quotation.lower():\n",
    "                    # Writing in the new file\n",
    "                    d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotebank_brexit = pd.read_json('Brexit_datas/Brexit_quotes.json.bz2',compression=\"bz2\",lines=True)\n",
    "quotebank_brexit.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='augmentation'></a>\n",
    "\n",
    "## Data augmentation\n",
    "\n",
    "When we will generate the results for the final story, we will need more information than the initial features we have. The further analysis will require to have access to other features such as the topic of the quotation, the sentiment that carries on the quotation, some information about the author and so on. The main idea is to add new features to the existing dataset or only to the data of interest. To do so, we will follow the following pipeline for each quotation:\n",
    "\n",
    "1. **Add features related to the author** : The first type of features one can add are the ones related to the author. Accessing at its wikipedia page gives us a lot of different information: looking carrefully at wikidata item field let us select some useful features listed below:\n",
    "    - `occupation` tells you the author domain.\n",
    "    - `member of political party` tells you the party at which the author belongs to.\n",
    "    - `educated at` tells you where the author studied.\n",
    "    - `country of citizenship` tells you the nationality of the author.\n",
    "    \n",
    "    These fields may not exist for all authors (as not all the authors are politicians), but we can actually assign a NaN value when the field does not appear for one author.\n",
    "\n",
    "2. **Add computed features** : The second type of features we can add are the ones that are directly derived from the initial ones. We selected a bunch of them that will be useful for further analysis:\n",
    "    - TO BE OPTIONALLY COMPLETED\n",
    "3. **Add features issued from a sentiment analysis** : The last feature we would like to add is the sentiment carried on by the quotation. For the sake of simplicity, we will classify each quotation in three different categories: *Negative*, *Neutral* and *Positive*. \n",
    "Performing such a text classification task can actually be done using pretrained Deep Neural Networks. We decided to use **Vader** Neural network which is described [here](https://github.com/cjhutto/vaderSentiment). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load speaker_attributes.parquet file that contains attributes in terms of QIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parquet that contains the information about speakers\n",
    "df_attributes = pd.read_parquet('Data/speaker_attributes.parquet')\n",
    "\n",
    "# we are not interested in the aliases, lastrevid, US_congress_bio_ID, id, candidacy and type.\n",
    "keep_attributes = ['id','label', 'date_of_birth', 'nationality', 'gender', 'ethnic_group', 'occupation', 'party', 'academic_degree', 'religion']\n",
    "# Set the index\n",
    "df_attributes = df_attributes[keep_attributes].set_index('id')\n",
    "# Sanity check for the qids\n",
    "print(\"Sanity check ok ? : \",df_attributes.index.is_unique)\n",
    "# Let's have a look\n",
    "df_attributes.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For speaker attributes, map the QIDs to meaningful labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionnary to use it as a lookup table \n",
    "df_map = pd.read_csv('Data/wikidata_labels_descriptions_quotebank.csv.bz2', compression='bz2', index_col='QID')\n",
    "# Dictionnary where qids are keys and values are corresponding element\n",
    "map_dict = df_map.Label.to_dict()\n",
    "\n",
    "def mapping(QIDs):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to map all the QIDs to their labels, \n",
    "    using wikidata_labels_descriptions_quotebank.csv\n",
    "    \"\"\"\n",
    "    \n",
    "    if QIDs is None:\n",
    "        return np.nan\n",
    "    else:\n",
    "        QIDs_mapped = []\n",
    "        for QID in QIDs:\n",
    "            try:\n",
    "                # If a correspondance exists\n",
    "                QIDs_mapped.append(map_dict[QID])\n",
    "            except KeyError:\n",
    "                # If no correspondance exits\n",
    "                continue\n",
    "        # If nothing was extracted\n",
    "        if len(QIDs_mapped) == 0:\n",
    "            return np.nan\n",
    "        # Things extracted\n",
    "        else:\n",
    "            return QIDs_mapped\n",
    "\n",
    "columns_to_map = ['nationality', 'gender', 'ethnic_group', 'occupation', 'party', 'academic_degree', 'religion']\n",
    "\n",
    "# For each column perform the mapping to transform qids to real value\n",
    "for column in columns_to_map:\n",
    "    df_attributes[column] = df_attributes[column].apply(mapping)\n",
    "    \n",
    "df_attributes.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add sentiment score to quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_score(quote):\n",
    "    \"\"\"The purpose of this function is to use the sentiment analysis tool VADER to find the sentiment associated with a quote.\"\"\"\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sid.polarity_scores(quote)\n",
    "    \n",
    "    # The Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between\n",
    "    # -1(most extreme negative) and +1 (most extreme positive).\n",
    "    # positive sentiment : (compound score >= 0.05) \n",
    "    # neutral sentiment : (compound score > -0.05) and (compound score < 0.05) \n",
    "    # negative sentiment : (compound score <= -0.05)\n",
    "    # see https://predictivehacks.com/how-to-run-sentiment-analysis-in-python-using-vader/\n",
    "    # or https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/\n",
    "    \n",
    "    # decide sentiment as positive, negative and neutral\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        return \"Positive\"\n",
    " \n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        return \"Negative\" \n",
    " \n",
    "    else :\n",
    "        return \"Neutral\"\n",
    "\n",
    "quotebank_brexit['sentiment_score'] = quotebank_brexit.quotation.apply(sent_score) \n",
    "quotebank_brexit.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleaning'></a>\n",
    "\n",
    "## Data merging and cleaning\n",
    "\n",
    "Depending on the different task we want to perform we will need to have the dataset in various forms, thus we will generate three types of dataset: \n",
    "- `quotebank_brexit`: original dataset cleaned\n",
    "- `aug_quotebank_brexit`: dataset filtered and augmented with the datas\n",
    "- `oneh_quotebank_brexit`: dataset here categorical values are encoded as one hot vectors\n",
    "\n",
    "Explain why we get ride of the None values rows, and why we get ride of multiple qids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotebank_brexit_filter = quotebank_brexit.loc[quotebank_brexit.qids.apply(lambda x : len(x)) == 1]\n",
    "quotebank_brexit_filter.qids = quotebank_brexit_filter.qids.apply(lambda x : x[0])\n",
    "aug_quotebank_brexit = pd.merge(quotebank_brexit_filter, df_attributes, 'inner', left_on=\"qids\", right_index=True)\n",
    "aug_quotebank_brexit.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge both dataframes to obtain final dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot vectorization of columns cotaining categorical values\n",
    "dummy_col = \"AAADummy column for the sake\"\n",
    "oneh_quotebank_brexit = aug_quotebank_brexit.copy()\n",
    "# Columns that contain categorical values\n",
    "cate_cols = [\"nationality\",\"ethnic_group\",\"occupation\",\"party\",\"academic_degree\",\"religion\"]\n",
    "unique_values = {}\n",
    "# Columns that contain binary values\n",
    "binary_cols = [\"gender\"]\n",
    "\n",
    "# Loop over categorical columns\n",
    "for col in cate_cols:\n",
    "    col_serie = aug_quotebank_brexit[col].copy()\n",
    "    col_serie.loc[col_serie.isna()] = col_serie.loc[col_serie.isna()].apply(lambda x: [dummy_col])\n",
    "    print(\"One hot vectorizing : \",col)\n",
    "    categorical_df = pd.get_dummies(col_serie.apply(pd.Series).stack()).groupby(level=0).sum()\n",
    "    categorical_df.drop(columns=dummy_col,inplace=True)\n",
    "    print(\"Number of different categories : \",len(categorical_df.columns))\n",
    "    unique_values[col] = categorical_df.columns\n",
    "    oneh_quotebank_brexit = oneh_quotebank_brexit.join(categorical_df,how=\"left\",rsuffix=col[:3])\n",
    "    oneh_quotebank_brexit.drop(columns=col,inplace=True)\n",
    "\n",
    "print(\"Shape of the final data frame\",oneh_quotebank_brexit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clustering'></a>\n",
    "\n",
    "## Quotations and speakers clustering\n",
    "\n",
    "The last preprocessing step consist in clustering the quotations as well as the speakers, this clustering will then be used to create a Recommandation Tool in the context of Brexit. The idea would be to first cluster the quotations and then the speakers such that two quotations/speakers that are in the same cluster are quotations/speakers carries on similar things/ideas. Performing such a task can be done following this pipeline:\n",
    "1. The first step is to convert sentences into vectors to be able to further perform the clustering. This task can be achieved using the [SentenceTransformer](https://www.sbert.net/docs/usage/semantic_textual_similarity.html) deep neural network. The vector obtained from this operation cab be then concatenated with the other existing features (that would be converted to one hot vectors if necessary).\n",
    "2. The second step consists in reducing the dimension of the datas before applying the clustering algorithm. This task can be achieved using the [T-stochastic neighbors embeddings](#https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) algorithm or the [Locally Linear Embeddings](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding) algorithm. These two techniques (specially the first one) are efficient non-linear dimensionality reduction methods.\n",
    "3. The third step is specific to speaker clustering. Indeed the vectorization of quotes as well as the reduction of dimensionality is only applied to quotes. Thus we need to perform an **aggregation** to be able to attribute a vector to each speaker. For each speaker, this aggregation can simply be done by taking the mean of the vectors associated with each of their quotations. \n",
    "4. The last step consist in performing the clustering operation. This task can be achieved using [Gaussian Mixture Model](https://scikit-learn.org/stable/modules/mixture.html#mixture) algorithm or  [Spectral Clustering](#https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    User-defined parameters for the task\n",
    "\"\"\"\n",
    "sentiment_amplification = 1.5 # coefficient of amplification of sentiment, amplification is applied after normalization\n",
    "normalize_tensor = True # chose whether to normalize the sentence tensor before clustering\n",
    "nb_clusters = 8   # Number of clusters to be identified\n",
    "sentence_transformer_type = 'all-MiniLM-L6-v2' # type of the sentence_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Define here all useful tools for the task.\n",
    "    \n",
    "    Given a column of a dataframe, generate a new dummy dataframe\n",
    "    take into account possible pd.NA and value multiplicity\n",
    "    \n",
    "    pd.NA correspond to a null vector\n",
    "\"\"\"\n",
    "def column_dummy_list(df, col):\n",
    "  # restrict to specified column\n",
    "  dfcol = df[col] \n",
    "\n",
    "  # add a recognizible string name to pd.NA\n",
    "  dfcol.fillna('NA', inplace=True)\n",
    "  dfcol = dfcol.apply(lambda x: ['NA'] if x == 'NA' else x) \n",
    "\n",
    "  # get dummy dataframe\n",
    "  out = dfcol.apply(pd.Series).stack().str.get_dummies().groupby(level=0).sum().add_prefix(col + '_')\n",
    "  # drop the NA dummy\n",
    "  return out.drop(col + '_NA', axis = 1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Convert sentiment_score to Positive = 1, Negative = -1, default = 0\n",
    "\"\"\"\n",
    "def sentiment_to_int(value):\n",
    "  return int(value == \"Positive\") - int(value == \"Negative\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Tools for the pandas.DataFrame to pytorch.tensor convertion\n",
    "\"\"\"\n",
    "\n",
    "# determine the supported device\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu') # don't have GPU \n",
    "    return device\n",
    "\n",
    "# convert a df to tensor to be used in pytorch\n",
    "def df_to_tensor(df):\n",
    "    device = get_device()\n",
    "    return torch.from_numpy(df.values).float().to(device)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Sanity check on data format, all elements should be lists\n",
    "\"\"\"\n",
    "def ensure_list(value):\n",
    "    if isinstance(value, list):\n",
    "      for i in range(len(value)):\n",
    "        value[i] = str(value[i])\n",
    "    elif not pd.isna(value):\n",
    "      value = [value]\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prepare dataframe for the one-hot vectorization task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure columns are present\n",
    "columns_to_map = [item for item in columns_to_map if item in list(augmented_quotebank_brexit)]\n",
    "\n",
    "# restrict dataframe to the one needed\n",
    "cluster_df = augmented_quotebank_brexit.loc[:, ['speaker', 'quotation', 'sentiment_score', *columns_to_map]]\n",
    "\n",
    "# remove duplicate quotations\n",
    "cluster_df = cluster_df.groupby(cluster_df.index).first()\n",
    "\n",
    "# apply sanity check on the elements format\n",
    "for label in columns_to_map:\n",
    "  cluster_df[label] = cluster_df[label].apply(make_list)\n",
    "\n",
    "# visualize an example\n",
    "cluster_df.loc[cluster_df['speaker'] == 'Laura Huhtasaari'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Encode quotations into their corresponding vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode quotation \n",
    "encoder = SentenceTransformer(sentence_transformer_type)\n",
    "quotes_encoded = encoder.encode(cluster_df['quotation'].values, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Compose the vectorized dataframe\n",
    "\n",
    "1. Convert sentiment score into signed integer format: \"Positive\" = 1, \"Negative\" = -1, \"Neutral\" = 0\n",
    "2. For each column concerning the speaker information, generate a dummy dataframe (see DataFrame.get_dummy)\n",
    "3. Concatenate along columns all obtained dataframe\n",
    "4. Average all rows matching the same speaker (which is set as index)\n",
    "5. Normalize dataset by row and amplify sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Compose a vectorized dataframe\n",
    "\"\"\"\n",
    "\n",
    "# replace sentiment by integer value\n",
    "info_df = cluster_df.loc[:,['sentiment_score']].copy()\n",
    "\n",
    "# convert sentiment_score into signed unitary integer\n",
    "info_df['sentiment_score'] = info_df['sentiment_score'].apply(sentiment_to_int)\n",
    "\n",
    "# for each column estract get the one-hot dummy dataframe \n",
    "for column in columns_to_map:\n",
    "  # get dummyzed column\n",
    "  dummy_column_df = column_dummy_list(cluster_df, column)\n",
    "  # concatenate it to info_df along horizontal direction\n",
    "  info_df = pd.concat([info_df, dummy_column_df], axis = 1)\n",
    "\n",
    "# replace string quotations by encoded quotations vectors\n",
    "quotes_df = pd.DataFrame(quotes_encoded, index = info_df.index)\n",
    "full_df = pd.concat([info_df, quotes_df], axis = 1)\n",
    "full_df['speaker'] = cluster_df['speaker'].values\n",
    "full_df.set_index('speaker', drop=True, inplace=True)\n",
    "\n",
    "# average over the same speaker\n",
    "full_df = full_df.groupby(level=0).agg(np.mean)\n",
    "\n",
    "# normalize dataset by row\n",
    "if normalize_tensor:\n",
    "  full_df = full_df.div(np.sqrt(np.square(full_df).sum(axis=1)), axis=0)\n",
    "\n",
    "# amplify sentiment\n",
    "full_df['sentiment_score'] *= sentiment_amplification\n",
    "\n",
    "full_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 3: Convert to *pytorch* tensor and apply TSNE aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into pytorch tensor\n",
    "full_data_tensor = df_to_tensor(full_df)\n",
    "\n",
    "# Apply T-stochastic neighboor embedding\n",
    "tsne_dim = 2      # TSNE reduction final dimension, default is 2\n",
    "data_tensor_emb = TSNE(n_components=tsne_dim, perplexity=30, n_iter=1000).fit_transform(full_data_tensor) # dim = Nxfinal_dim\n",
    "\n",
    "results = data_tensor_emb.transpose()\n",
    "\n",
    "# Visualize without clustering\n",
    "plt.scatter(results[0], results[1])\n",
    "plt.title(\"TSNE bidimensional reduction of the speaker vectorization\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: Apply the clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Clustering\n",
    "\"\"\" \n",
    "\n",
    "# Apply Clustering\n",
    "clustering = SpectralClustering(nb_clusters).fit(data_tensor_emb)\n",
    "\n",
    "\"\"\"\n",
    "    Visualization\n",
    "\"\"\" \n",
    "fig, axis = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "results = data_tensor_emb.transpose()\n",
    "\n",
    "# Visualize without clustering\n",
    "axis[0].scatter(results[0], results[1])\n",
    "\n",
    "\n",
    "for label in range(nb_clusters):\n",
    "    # select data by clustering label\n",
    "    points = data_tensor_emb[clustering.labels_ == label]\n",
    "    points = points.transpose()\n",
    "    # plot data\n",
    "    axis[1].scatter(points[0], points[1])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Results'></a>\n",
    "\n",
    "# Generate the results for the final story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Statistics'></a>\n",
    "\n",
    "## General Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_quotebank_brexit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2,figsize=[20,14])\n",
    "\n",
    "# Plot the number of quotations with respect to the date\n",
    "sns.histplot(data=aug_quotebank_brexit,x=\"date\",ax = axes[0,0])\n",
    "\n",
    "# Plot the number of speakers and the number of quotations per country\n",
    "\n",
    "\"\"\"\n",
    "# Count speakers per country\n",
    "country_data = aug_quotebank_brexit.loc[:,[\"nationality\",\"speaker\"]].drop_duplicates(subset=['speaker'])\n",
    "country_data = country_data.groupby(\"nationality\").size().reset_index(name=\"Speaker Count\")\n",
    "\n",
    "# Add number of quotations per country\n",
    "country_data = country_data.join(aug_quotebank_brexit.groupby(\"nationality\").size().reset_index(name=\"Quotation count\"),\n",
    "                                 on = \"nationality\")\n",
    "\n",
    "sns.barplot(data=country_data,x=\"country\",y=\"Speaker Count\",ax=axes[0,1])\n",
    "sns.barplot(data=country_data,x=\"country\",y=\"Quotation Count\",ax=axes[1,1])\n",
    "\"\"\"\n",
    "\n",
    "# Plot the number of \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Country'></a>\n",
    "\n",
    "## Analyze the way Brexit is perceived in European countries\n",
    "\n",
    "Recall that the goal is to analyze the way Brexit is perceived in each Europe country based on the sentiment carried by the quotation. Besides we would like to add the time dimension to this analysis, meaning that we would like to follow the evolution of the overall feelings towards Brexit. A view of the expected result is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Sector'></a>\n",
    "\n",
    "## Analyze the way Brexit is perceived in different sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYNTHETIC DATAS\n",
    "df_dic = {}\n",
    "df_dic[\"Sector\"] = [\"Politic\"]*3 + [\"Economy\"]*3 + [\"Science\"]*3 + [\"Art\"]*3\n",
    "df_dic[\"Sentiment\"] = [\"Positive\",\"Neutral\",\"Negative\"]*4\n",
    "df_dic[\"percentage\"] = [40,40,20,50,20,30] + [30,50,20]*2\n",
    "\n",
    "# df = pd.get_dummies(pd.DataFrame(df_dic))\n",
    "df = pd.DataFrame(df_dic)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x=\"Sector\", y=\"Sentiment\", color=\"Sentiment\",\n",
    "                 size='percentage')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2Dplot'></a>\n",
    "\n",
    "## Visualize speakers orientation trough a 2D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Recommandation'></a>\n",
    "\n",
    "## Recommandation tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Stocks'></a>\n",
    "\n",
    "## Correlation with stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION WITH STOCKS ACTIONS"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c7ebd9986d9dd66f5c55cec683961b660fa3bc87c5eb159f7033b074e7bd831"
  },
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
