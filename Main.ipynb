{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone 2\n",
    "\n",
    "Here we will describe the whole pipeline to get all the results we would like to include in the final story (on the final website). We will go through all the different steps and describe as detailed as possible the operations needed.\n",
    "\n",
    "## **[Preprocessing steps](#Preprocessing)**\n",
    "\n",
    "As usual the first step consist in several substeps that aims at cleaning and transforming the data. By clicking on the task link, you can access the respective pipeline.\n",
    "- *[Data exploration and Sanity check](#Sanity_check)* : Explore the dataset, check its consistency and get familiar with the different features/information provided into.\n",
    "    - Collaborators assigned to that task: ALL.\n",
    "- *[Data extraction](#extraction)* : Extract the datas of interest that will be further used to perform the tasks related to each idea.\n",
    "    - Collaborators assigned to that task: Arnaud.\n",
    "- *[Data augmentation](#augmentation)* : Perform a data augmentation to get more features about the quotations such as the quote field, the nationality of the speaker and so on... These new features will be further used to perform the tasks related to each idea.\n",
    "    - Collaborators assigned to that task: Jean & Gaelle. \n",
    "- *[Quotations and speakers clustering](#clustering)* : Cluster the quotations and the speakers according to the a quotation vector and the added features (data augmentation).\n",
    "    - Collaborators assigned to that task: Rafaele (RESTART FROM SCRATCH, PLEASE DO SOMETHING INTELLIGIBLE) + Look at website.\n",
    "\n",
    "## **[Generate the results for the final story](#Results)**\n",
    "\n",
    "*[Analysis of the way Brexit is perceived](#Brexit)* : The first idea consists in visualizing the the way Brexit is perceived accross different fields as well as accross the different countries in Europe.**JEAN COULD YOU COMPLETE WITH YOUR INITIAL DESCRIPTION ? (PUT SOMETHING APPEALING)**\n",
    "- Country map\n",
    "- Bubbles\n",
    "- Clustering evolution\n",
    "- Maybe recommandation system\n",
    "- Evolution of the number of quotations about Brexit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before diving into the code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have a **Data** Folder containing all the quotebank datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import useful librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2 \n",
    "import json\n",
    "import pandas as pd\n",
    "#from pybrainyquote import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Preprocessing'></a>\n",
    "\n",
    "# Preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Sanity_check'></a>\n",
    "\n",
    "## Data exploration and Sanity check\n",
    "\n",
    "Define which exploration and sanity checks we need to perform:\n",
    "\n",
    "- Distribution of the number of occurences\n",
    "- Remove duplicate quotations\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRIEVE DATA FROM DIFFERENT SOURCES\n",
    "\n",
    "\"\"\"\n",
    "    Initialize quotes dictionary\n",
    "    key = author\n",
    "    values = [quotations]\n",
    "\"\"\"\n",
    "quotes_dict = {}\n",
    "\n",
    "\"\"\"\n",
    "    Authors sparse informations that can be already retrieved\n",
    "    key = author\n",
    "    values = [info_as_string]\n",
    "\"\"\"\n",
    "authors_info = {} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOURCE: Brainyquote.com website\n",
    "brainyquotes = Quote.find_all('brexit')\n",
    "\n",
    "if brainyquotes is not None:\n",
    "    for q in branyquotes:\n",
    "        quotes_dict[q.author].append(q.content)\n",
    "        authors_info[q.author] = q.info\n",
    "else:\n",
    "    print(\"Error: retrieving data from Brainyquotes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORM THE DIFFERENT SANITY CHECKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='extraction'></a>\n",
    "\n",
    "## Data extraction\n",
    "\n",
    "As mentionned previously, we are planning to analyze the influence of Brexit on different branch as well as analyzing the evolution of feelings towards China. To be able to perform such tasks, we need first to extract the quotations that are talking from Brexit and the ones that are talking about China. To do so we will follow the following pipeline:\n",
    "\n",
    "1. Both for Brexit and China, define a neighborhood containing all the words that are respectively closely related to Brexit and China. This neighborhood will be a list of words or expressions that are commonly used to refer to Brexit or China. For instance, for China one could actually add to the vocabulary neighborhood the *\"the Middle Kingdom\"* expression that is often used to refer to China.\n",
    "2. Both for Brexit and China, select all the quotations for which, at least, one word/expression from the vocabulary neighborhood appears in it.\n",
    "3. Store the new two datasets in the following files: \n",
    "    - `Brexit_quotes.json.bz2`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE BREXIT VOCAB\n",
    "\n",
    "Brexit_vocab = [\"Brexit\",\"brexit\"]\n",
    "\n",
    "\"Brexit\" in \"Brexit is beautiful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO THE EXTRACTION HERE\n",
    "\n",
    "# PATHS_TO_FILE = [('Data/quotes-20%d.json.bz2' % i) for i in range(15,21)]\n",
    "PATHS_TO_FILE = ['Data/quotes-2020.json.bz2']\n",
    "PATH_TO_OUT = 'Brexit_datas/Brexit_quotes.json.bz2'\n",
    "\n",
    "with bz2.open(PATH_TO_OUT, 'wb') as d_file:\n",
    "    for quotebank_data in PATHS_TO_FILE:\n",
    "        with bz2.open(quotebank_data, 'rb') as s_file:\n",
    "            for instance in s_file:\n",
    "                instance = json.loads(instance) # loading a sample\n",
    "                quotation = instance['quotation'] # extracting the quotation\n",
    "                if any(substring in quotation for substring in Brexit_vocab):\n",
    "                    d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) # writing in the new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotebank_brexit_test = pd.read_json('Brexit_datas/Brexit_quotes.json.bz2',compression=\"bz2\",lines=True)\n",
    "quotebank_brexit_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='augmentation'></a>\n",
    "\n",
    "## Data augmentation\n",
    "\n",
    "When we will generate the results for the final story, we will need more information than the initial features we have. The further analysis will require to have access to other features such as the topic of the quotation, the sentiment that carries the quotation, some information about the author and so on. The main idea is to add new features to the existing dataset or only to the data of interest. To do so, we will follow the following pipeline for each quotation:\n",
    "\n",
    "1. **Add features related to the author** : The first type of features one can add are the ones related to the author. Accessing at its wikipedia page gives us a lot of different information: looking carrefully at wikidata item field let us select some useful features listed below:\n",
    "    - `occupation` tells you the author domain.\n",
    "    - `member of political party` tells you the party at which the author belongs to.\n",
    "    - `educated at` tells you where the author studied.\n",
    "    - `country of citizenship` tells you the nationality of the author.\n",
    "    \n",
    "    These fields may not exist for all authors (as not all the authors are politicians), but we can actually assign a NaN value when the field does not appear for one author.\n",
    "\n",
    "2. **Add computed features** : The second type of features we can add are the ones that are directly derived from the initial ones. We selected a bunch of them that will be useful for further analysis:\n",
    "    - TO BE OPTIONALLY COMPLETED\n",
    "3. **Add features issued from a sentiment analysis** : The last feature we would like to add is the sentiment carried on by the quotation. Initially we were thinking about a binary sentiment classification: 0 if the sentiment is negative, 1 if it is positive. We could further expand that by classifying the quotations into several categories such as *anger*, *sadness*, *factual* and so on...    \n",
    "Performing such a text classification task can actually be done using pretrained Deep Neural Networks. XLNet network ([GitHub page](https://github.com/zihangdai/xlnet/) & [Library containing XLNet](https://huggingface.co/transformers/model_doc/xlnet.html)) is close to the state of the art algorithm for classification. Therefore we plan to use it to determine the sentiment contained in each quotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO THE DATA AUGMENTATION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clustering'></a>\n",
    "\n",
    "## Quotations and speakers clustering\n",
    "\n",
    "The last preprocessing step consist in clustering the quotations as well as the speakers, this clustering will then be used to create a Recommandation Tool. The idea would be to first cluster the quotations and then the speakers such that two quotations/speakers that are in the same cluster are quotations/speakers that carries on similar things/ideas. Performing such a task can be done following this pipeline:\n",
    "1. The first step is to convert sentences into vectors to be able to further perform the clustering. This task can be achieved using the [SentenceTransformer](https://www.sbert.net/docs/usage/semantic_textual_similarity.html) deep neural network. The vector obtained from this operation cab be then concatenated with the other existing features (that would be converted to one hot vectors if necessary).\n",
    "2. \\[OPTIONAL STEP\\] The second step consists in reducing the dimension of the datas before applying the clustering algorithm. This task can be achieved using the [T-stochastic neighbors embeddings](#https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) algorithm or the [Locally Linear Embeddings](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding) algorithm. These two techniques (specially the first one) are efficient non-linear dimensionality reduction methods.\n",
    "3. The third step is specific to speaker clustering. Indeed the vectorization of quotes as well as the reduction of dimensionality is only applied to quotes. Thus we need to perform an **aggregation** to be able to attribute a vector to each speaker. For each speaker, this aggregation can simply be done by taking the mean of the vectors associated with each of their quotations. \n",
    "4. The last step consist in performing the clustering operation. This task can be achieved using [Gaussian Mixture Model](https://scikit-learn.org/stable/modules/mixture.html#mixture) algorithm or  [Spectral Clustering](#https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering) method.\n",
    "\n",
    "However a question arises : which amount of datas should we consider to perform such a clustering ? Cluster using the whole data set seems infeasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORM CLUSTERING HERE\n",
    "\n",
    "# upload those modules on jupyter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import pytorch as torch\n",
    "from tsne_torch import TorchTSNE as TSNE\n",
    "\n",
    "# Encode data\n",
    "\n",
    "encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode all data into a pytorch tensor NxD\n",
    "# N = number of sentences (samples)\n",
    "# D = dimension of a sentence vector\n",
    "data_tensor = None\n",
    "\n",
    "# suppose quotes is a dictionary with speakers as keys and their respective quotes as values \n",
    "\n",
    "for speaker, sentences in quotes.items():\n",
    "    # Step 1: encode sentences into a pytorch tensor NxD\n",
    "    # N = number of sentences (samples)\n",
    "    # D = dimension of a sentence vector\n",
    "    quotes_tensor = encoder.encode(sentences, convert_to_tensor=True)\n",
    "    \n",
    "    # concatenate tensors by rows\n",
    "    if data_tensor is None:\n",
    "        data_tensor = quotes_tensor\n",
    "    else:\n",
    "        torch.cat((data_tensor, quotes_tensor), 0)\n",
    "    \n",
    "    \n",
    "# Step 2: T-stochastic neighboor embedding\n",
    "final_dim = 20 # dim = NxD\n",
    "data_tensor_emb = TSNE(n_components=final_dim, perplexity=30, n_iter=1000).fit_transform(speaker_tensor) # dim = Nxfinal_dim\n",
    "\n",
    "# Step 3: contract tensor by mean along rows\n",
    "speaker_tensor = None\n",
    "\n",
    "with 0 as i:\n",
    "    for speaker, sentences in quotes.items():\n",
    "        speaker_vector = torch.mean(data_tensor_emb[i : len(sentences)], 1)\n",
    "        i += len(sentences)\n",
    "        \n",
    "        if speaker_tensor is None:\n",
    "            speaker_tensor = speaker_vector\n",
    "        else:\n",
    "            torch.cat((speaker_tensor, speaker_vector), 0)\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Clustering and actual training\n",
    "\n",
    "# TODO: vector length normalization?\n",
    "\n",
    "cluster_model = SpectralClustering()\n",
    "cluster_model.fit(speaker_tensor_emb)\n",
    "\n",
    "# TODO: visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Results'></a>\n",
    "\n",
    "# Generate the results for the final story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Brexit'></a>\n",
    "\n",
    "## Analysis of the way Brexit is perceived\n",
    "\n",
    "Recall that the goal is to analyze the way Brexit is perceived in each Europe country and in each branch (economy) based on the sentiment carried by the quotation. Besides we would like to add the time dimension to this analysis, meaning that we would like to follow the evolution of the overall feelings towards Brexit. A view of the expected result is given below:\n",
    "\n",
    "Sector Analysis | Country Analysis\n",
    "- | -\n",
    "![alt text](Images/brexit_bubbles.png \"Sector analysis\") | ![alt text](Images/brexit_expected_outcomes.png \"Country analysis\")\n",
    "\n",
    "### *Pipeline*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BREXIT ANALYSIS"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c7ebd9986d9dd66f5c55cec683961b660fa3bc87c5eb159f7033b074e7bd831"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ADA': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
