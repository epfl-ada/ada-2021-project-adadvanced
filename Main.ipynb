{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone 2\n",
    "\n",
    "Here we will describe the whole pipeline to get all the results we would like to include in the final story (on the final website). We will go through all the different steps and describe as detailed as possible the operations needed. \n",
    "\n",
    "For the final story we decided to focus on the influence of the Brexit. More precisely we would like to assess how the Brexit was perceived and how it evolves along the years. All the different visualizations we aim at providing in the final story are well detailed in this [Section](#Results).\n",
    "\n",
    "## **[Preprocessing steps](#Preprocessing)**\n",
    "\n",
    "As usual the first step consist in several substeps that aims at cleaning and transforming the data. By clicking on the task link, you can access the respective pipeline.\n",
    "- *[Data exploration and Sanity check](#Sanity_check)* : Explore the dataset, check its consistency and get familiar with the different features/information provided into.\n",
    "    - Collaborators assigned to that task: ALL.\n",
    "- *[Data extraction](#extraction)* : Extract the datas of interest that will be further used to perform the tasks related to each idea.\n",
    "    - Collaborators assigned to that task: Arnaud.\n",
    "- *[Data augmentation](#augmentation)* : Perform a data augmentation to get more features about the quotations such as the quote field, the nationality of the speaker and so on... These new features will be further used to perform the tasks related to each idea.\n",
    "    - Collaborators assigned to that task: Jean & Gaelle. \n",
    "- *[Quotations and speakers clustering](#clustering)* : Cluster the quotations and the speakers according to the a quotation vector and the added features (data augmentation). This clustering will be further mainly used to develop a recommandation tool.\n",
    "    - Collaborators assigned to that task: Raffaele.\n",
    "\n",
    "## **[Generate the results for the final story](#Results)**\n",
    "\n",
    "- [General Statitics](#Statistics) : \n",
    "- [Country map](#Country) : \n",
    "- [Sector map](#Sector) : \n",
    "- [Visualize speakers evolution](#2Dplot) :\n",
    "- [Recommandation Tool](#Recommandation) :\n",
    "- [Correlation with stocks](#Stocks) :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before diving into the code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have a **Data** Folder containing all the quotebank datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import useful librairies and define useful librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2 \n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "PATHS_TO_FILE = ['Data/quotes-20%d.json.bz2' % i for i in range(15,21)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Preprocessing'></a>\n",
    "\n",
    "# Preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Sanity_check'></a>\n",
    "\n",
    "## Data exploration and Sanity check\n",
    "\n",
    "We decided to perform the following snaity checks on the original datas: \n",
    "\n",
    "- We first check that each entry for each quotation is specified in the right format (e.g. `numOccurences` should be an integer).\n",
    "- We check that the `probas` sum to 1.\n",
    "- We check that the `numOccurences` is superior or equal to the length of the list containing the urls.$\n",
    "- The `date` is consistent with the dataset they are coming from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECK FUNCTIONS\n",
    "\n",
    "def check_type(instance,entry,dtype):\n",
    "    return type(instance[entry]) == dtype\n",
    "\n",
    "def check_probas(instance):\n",
    "    # TO BE DEFINED\n",
    "    return None\n",
    "\n",
    "def check_numOcc(instance):\n",
    "    # TO BE DEFINED\n",
    "    return None\n",
    "\n",
    "def check_date(instance,year):\n",
    "    # TO BE DEFINED\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the types for each entry\n",
    "TYPES = {\"quoteID\":str,\n",
    "         \"quotation\":str,\n",
    "         \"speaker\":str,\n",
    "         \"qids\":list,\n",
    "         \"date\":str,\n",
    "         \"numOccurrences\":int,\n",
    "         \"probas\":list,\n",
    "         \"urls\":list,\n",
    "         \"phase\":str}\n",
    "\n",
    "error_dic = {'Data/quotes-20%d.json.bz2' % i : [] for i in range(20,21)}\n",
    "\n",
    "# Loop over the different files that we will read\n",
    "for quotebank_data in PATHS_TO_FILE:\n",
    "    # Open the file we want to read\n",
    "    with bz2.open(quotebank_data, 'rb') as s_file:\n",
    "        # Loop over the samples\n",
    "        for instance in s_file:\n",
    "            # Loading a sample\n",
    "            instance = json.loads(instance)\n",
    "            #### CHECK THE TYPES ####\n",
    "            for key, value in TYPES.items():\n",
    "                if not check_type(instance,key,value):\n",
    "                    error_dic[quotebank_data].append(instance[\"quoteIDS\"] + \": \" + key + \" type problem\")\n",
    "\n",
    "print(error_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='extraction'></a>\n",
    "\n",
    "## Data extraction\n",
    "\n",
    "As mentionned previously, we are planning to analyze the influence of Brexit on different branch as well as analyzing the evolution of feelings towards China. To be able to perform such tasks, we need first to extract the quotations that are talking from Brexit and the ones that are talking about China. To do so we will follow the following pipeline:\n",
    "\n",
    "1. Both for Brexit and China, define a neighborhood containing all the words that are respectively closely related to Brexit and China. This neighborhood will be a list of words or expressions that are commonly used to refer to Brexit or China. For instance, for China one could actually add to the vocabulary neighborhood the *\"the Middle Kingdom\"* expression that is often used to refer to China.\n",
    "2. Both for Brexit and China, select all the quotations for which, at least, one word/expression from the vocabulary neighborhood appears in it.\n",
    "3. Store the new two datasets in the following files: \n",
    "    - `Brexit_quotes.json.bz2`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file\n",
    "PATHS_TO_FILE = ['Data/quotes-20%d.json.bz2' % i for i in range(15,21)]\n",
    "# Output file\n",
    "PATH_TO_OUT = 'Brexit_datas/Brexit_quotes.json.bz2'\n",
    "\n",
    "# Open the file where we will write\n",
    "with bz2.open(PATH_TO_OUT, 'wb') as d_file:\n",
    "    # Loop over the different files that we will read\n",
    "    for quotebank_data in PATHS_TO_FILE:\n",
    "        print(\"Reading \",quotebank_data,\" file...\")\n",
    "        # Open the file we want to read\n",
    "        with bz2.open(quotebank_data, 'rb') as s_file:\n",
    "            # Loop over the samples\n",
    "            for instance in s_file:\n",
    "                # Loading a sample\n",
    "                instance = json.loads(instance)\n",
    "                # Extracting the quotation\n",
    "                quotation = instance['quotation']\n",
    "                # Check if the quotation contains at least one word related to Brexit\n",
    "                if \"brexit\" in quotation.lower():\n",
    "                    # Writing in the new file\n",
    "                    d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotebank_brexit = pd.read_json('Brexit_datas/Brexit_quotes.json.bz2',compression=\"bz2\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotebank_brexit.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='augmentation'></a>\n",
    "\n",
    "## Data augmentation\n",
    "\n",
    "When we will generate the results for the final story, we will need more information than the initial features we have. The further analysis will require to have access to other features such as the topic of the quotation, the sentiment that carries the quotation, some information about the author and so on. The main idea is to add new features to the existing dataset or only to the data of interest. To do so, we will follow the following pipeline for each quotation:\n",
    "\n",
    "1. **Add features related to the author** : The first type of features one can add are the ones related to the author. Accessing at its wikipedia page gives us a lot of different information: looking carrefully at wikidata item field let us select some useful features listed below:\n",
    "    - `occupation` tells you the author domain.\n",
    "    - `member of political party` tells you the party at which the author belongs to.\n",
    "    - `educated at` tells you where the author studied.\n",
    "    - `country of citizenship` tells you the nationality of the author.\n",
    "    \n",
    "    These fields may not exist for all authors (as not all the authors are politicians), but we can actually assign a NaN value when the field does not appear for one author.\n",
    "\n",
    "2. **Add computed features** : The second type of features we can add are the ones that are directly derived from the initial ones. We selected a bunch of them that will be useful for further analysis:\n",
    "    - TO BE OPTIONALLY COMPLETED\n",
    "3. **Add features issued from a sentiment analysis** : The last feature we would like to add is the sentiment carried on by the quotation. Initially we were thinking about a binary sentiment classification: 0 if the sentiment is negative, 1 if it is positive. We could further expand that by classifying the quotations into several categories such as *anger*, *sadness*, *factual* and so on...    \n",
    "Performing such a text classification task can actually be done using pretrained Deep Neural Networks. XLNet network ([GitHub page](https://github.com/zihangdai/xlnet/) & [Library containing XLNet](https://huggingface.co/transformers/model_doc/xlnet.html)) is close to the state of the art algorithm for classification. Therefore we plan to use it to determine the sentiment contained in each quotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO THE DATA AUGMENTATION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load speaker_attributes.parquet file that contains attributes in terms of QIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attributes = pd.read_parquet('Data/speaker_attributes.parquet')\n",
    "df_attributes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are not interested in the aliases, lastrevid, US_congress_bio_ID, id, candidacy and type.\n",
    "\n",
    "keep_attributes = ['label', 'date_of_birth', 'nationality', 'gender', 'ethnic_group', 'occupation', 'party', 'academic_degree', 'religion']\n",
    "df_attributes = df_attributes[keep_attributes].set_index('label')\n",
    "df_attributes.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For speaker attributes, map the QIDs to meaningful labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionnary to use it as a lookup table \n",
    "\n",
    "df_map = pd.read_csv('Data/wikidata_labels_descriptions_quotebank.csv.bz2', compression='bz2', index_col='QID')\n",
    "map_dict = df_map.Label.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(QIDs):\n",
    "    \"\"\"The purpose of this function is to map all the QIDs to their labels, using wikidata_labels_descriptions_quotebank.csv\"\"\"\n",
    "    \n",
    "    if QIDs is None:\n",
    "        return pd.NA\n",
    "    else:\n",
    "        QIDs_mapped = []\n",
    "        for QID in QIDs:\n",
    "            try:\n",
    "                QIDs_mapped.append(map_dict[QID])\n",
    "            except KeyError:\n",
    "                continue\n",
    "        return QIDs_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_map = ['nationality', 'gender', 'ethnic_group', 'occupation', 'party', 'academic_degree', 'religion']\n",
    "\n",
    "# A MODIFIER: je sais pas pourquoi la ligne d'en bas ne fonctionne pas. La méthode alternative avec la boucle fonctionne mais c'est pas très propre.\n",
    "# df_attributes[columns_to_map] = df_attributes[columns_to_map].apply(mapping, axis=0)\n",
    "for column in columns_to_map:\n",
    "    df_attributes[column] = df_attributes[column].apply(mapping)\n",
    "    \n",
    "df_attributes.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add sentiment score to quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotebank_brexit = pd.read_json('Data/Brexit_quotes.json.bz2', compression='bz2', lines=True)\n",
    "quotebank_brexit.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_score(quote):\n",
    "    \"\"\"The purpose of this function is to use the sentiment analysis tool VADER to find the sentiment associated with a quote.\"\"\"\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sid.polarity_scores(quote)\n",
    "    \n",
    "    # The Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between\n",
    "    # -1(most extreme negative) and +1 (most extreme positive).\n",
    "    # positive sentiment : (compound score >= 0.05) \n",
    "    # neutral sentiment : (compound score > -0.05) and (compound score < 0.05) \n",
    "    # negative sentiment : (compound score <= -0.05)\n",
    "    # see https://predictivehacks.com/how-to-run-sentiment-analysis-in-python-using-vader/\n",
    "    # or https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/\n",
    "    \n",
    "    # decide sentiment as positive, negative and neutral\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        return \"Positive\"\n",
    " \n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        return \"Negative\" \n",
    " \n",
    "    else :\n",
    "        return \"Neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotebank_brexit['sentiment_score'] = quotebank_brexit['quotation'].apply(sent_score) \n",
    "quotebank_brexit.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge both dataframes to obtain final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_quotebank_brexit = pd.merge(quotebank_brexit, df_attributes, 'inner', left_on='speaker', right_index=True)\n",
    "augmented_quotebank_brexit.head(2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "augmented_quotebank_brexit.speaker.value_counts().to_frame().to_csv('out.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clustering'></a>\n",
    "\n",
    "## Quotations and speakers clustering\n",
    "\n",
    "The last preprocessing step consist in clustering the quotations as well as the speakers, this clustering will then be used to create a Recommandation Tool in the context of Brexit. The idea would be to first cluster the quotations and then the speakers such that two quotations/speakers that are in the same cluster are quotations/speakers carries on similar things/ideas. Performing such a task can be done following this pipeline:\n",
    "1. The first step is to convert sentences into vectors to be able to further perform the clustering. This task can be achieved using the [SentenceTransformer](https://www.sbert.net/docs/usage/semantic_textual_similarity.html) deep neural network. The vector obtained from this operation cab be then concatenated with the other existing features (that would be converted to one hot vectors if necessary).\n",
    "2. \\[OPTIONAL STEP\\] The second step consists in reducing the dimension of the datas before applying the clustering algorithm. This task can be achieved using the [T-stochastic neighbors embeddings](#https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) algorithm or the [Locally Linear Embeddings](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding) algorithm. These two techniques (specially the first one) are efficient non-linear dimensionality reduction methods.\n",
    "3. The third step is specific to speaker clustering. Indeed the vectorization of quotes as well as the reduction of dimensionality is only applied to quotes. Thus we need to perform an **aggregation** to be able to attribute a vector to each speaker. For each speaker, this aggregation can simply be done by taking the mean of the vectors associated with each of their quotations. \n",
    "4. The last step consist in performing the clustering operation. This task can be achieved using [Gaussian Mixture Model](https://scikit-learn.org/stable/modules/mixture.html#mixture) algorithm or  [Spectral Clustering](#https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORM CLUSTERING HERE\n",
    "\n",
    "# upload those modules on jupyter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import pytorch as torch\n",
    "from tsne_torch import TorchTSNE as TSNE\n",
    "\n",
    "# Encode data\n",
    "\n",
    "encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode all data into a pytorch tensor NxD\n",
    "# N = number of sentences (samples)\n",
    "# D = dimension of a sentence vector\n",
    "data_tensor = None\n",
    "\n",
    "# suppose quotes is a dictionary with speakers as keys and their respective quotes as values \n",
    "\n",
    "for speaker, sentences in quotes.items():\n",
    "    # Step 1: encode sentences into a pytorch tensor NxD\n",
    "    # N = number of sentences (samples)\n",
    "    # D = dimension of a sentence vector\n",
    "    quotes_tensor = encoder.encode(sentences, convert_to_tensor=True)\n",
    "    \n",
    "    # concatenate tensors by rows\n",
    "    if data_tensor is None:\n",
    "        data_tensor = quotes_tensor\n",
    "    else:\n",
    "        torch.cat((data_tensor, quotes_tensor), 0)\n",
    "    \n",
    "    \n",
    "# Step 2: T-stochastic neighboor embedding\n",
    "final_dim = 20 # dim = NxD\n",
    "data_tensor_emb = TSNE(n_components=final_dim, perplexity=30, n_iter=1000).fit_transform(speaker_tensor) # dim = Nxfinal_dim\n",
    "\n",
    "# Step 3: contract tensor by mean along rows\n",
    "speaker_tensor = None\n",
    "\n",
    "with 0 as i:\n",
    "    for speaker, sentences in quotes.items():\n",
    "        speaker_vector = torch.mean(data_tensor_emb[i : len(sentences)], 1)\n",
    "        i += len(sentences)\n",
    "        \n",
    "        if speaker_tensor is None:\n",
    "            speaker_tensor = speaker_vector\n",
    "        else:\n",
    "            torch.cat((speaker_tensor, speaker_vector), 0)\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Clustering and actual training\n",
    "\n",
    "# TODO: vector length normalization?\n",
    "\n",
    "cluster_model = SpectralClustering()\n",
    "cluster_model.fit(speaker_tensor_emb)\n",
    "\n",
    "# TODO: visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Results'></a>\n",
    "\n",
    "# Generate the results for the final story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Statistics'></a>\n",
    "\n",
    "## General Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2,figsize=[20,14])\n",
    "\n",
    "# Plot the number of quotations with respect to the date\n",
    "sns.histplot(data=augmented_quotebank_brexit,x=\"date\",ax = axes[0,0])\n",
    "\n",
    "# Plot the number of speakers and the number of quotations per country\n",
    "\n",
    "# Count speakers per country\n",
    "country_data = augmented_quotebank_brexit.loc[:,[\"country\",\"speaker\"]].drop_duplicates(subset=['speaker'])\n",
    "country_data = country_data.groupby(\"country\").size().reset_index(name=\"Speaker Count\")\n",
    "\n",
    "# Add number of quotations per country\n",
    "country_data = country_data.join(augmented_quotebank_brexit.groupby(\"country\").size().reset_index(name=\"Quotation count\"),\n",
    "                                 on = \"country\")\n",
    "\n",
    "sns.barplot(data=country_data,x=\"country\",y=\"Speaker Count\",ax=axes[0,1])\n",
    "sns.barplot(data=country_data,x=\"country\",y=\"Quotation Count\",ax=axes[1,1])\n",
    "\n",
    "# Plot the number of \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Country'></a>\n",
    "\n",
    "## Analyze the way Brexit is perceived in European countries\n",
    "\n",
    "Recall that the goal is to analyze the way Brexit is perceived in each Europe country based on the sentiment carried by the quotation. Besides we would like to add the time dimension to this analysis, meaning that we would like to follow the evolution of the overall feelings towards Brexit. A view of the expected result is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Sector'></a>\n",
    "\n",
    "## Analyze the way Brexit is perceived in different sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYNTHETIC DATAS\n",
    "df_dic = {}\n",
    "df_dic[\"Sector\"] = [\"Politic\",\"Politic\",\"Politic\",\"Economy\",\"Science\",\"Art\"]\n",
    "df_dic[\"Sentiment\"] = [\"Positive\",\"Neutral\",\"Negative\",\"Positive\",\"Positive\",\"Positive\"]\n",
    "df_dic[\"percentage\"] = [40,60,80,80,60,50]\n",
    "\n",
    "df = pd.DataFrame(df_dic)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x=\"Sector\", y=\"Sentiment\", color=\"Sentiment\",\n",
    "                 size='percentage')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2Dplot'></a>\n",
    "\n",
    "## Visualize speakers orientation trough a 2D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Recommandation'></a>\n",
    "\n",
    "## Recommandation tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Stocks'></a>\n",
    "\n",
    "## Correlation with stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c7ebd9986d9dd66f5c55cec683961b660fa3bc87c5eb159f7033b074e7bd831"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
