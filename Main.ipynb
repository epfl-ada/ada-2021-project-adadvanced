{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone 2\n",
    "\n",
    "Here we will describe the whole pipeline to get all the results we would like to include in the final story (on the final website). We will go through all the different steps and describe as detailed as possible the operations needed. \n",
    "\n",
    "For the final story we decided to focus on the influence of the Brexit. More precisely we would like to assess how the Brexit was perceived and how it evolves along the years. All the different visualizations we aim at providing in the final story are well detailed in this [Section](#Results).\n",
    "\n",
    "## **[Preprocessing steps](#Preprocessing)**\n",
    "\n",
    "As usual the first step consist in several substeps that aims at cleaning and transforming the data. By clicking on the task link, you can access the respective pipeline.\n",
    "- *[Data exploration and Sanity check](#Sanity_check)* : Explore the dataset, check its consistency and get familiar with the different features/information provided into.\n",
    "    - Collaborators assigned to that task: ALL.\n",
    "- *[Data extraction](#extraction)* : Extract the datas of interest that will be further used to perform the tasks related to each idea.\n",
    "    - Collaborators assigned to that task: Arnaud.\n",
    "- *[Data augmentation](#augmentation)* : Perform a data augmentation to get more features about the quotations such as the quote field, the nationality of the speaker and so on... These new features will be further used to perform the tasks related to each idea.\n",
    "    - Collaborators assigned to that task: Jean & Gaelle. \n",
    "- *[Data cleaning](#augmentation)* \n",
    "- *[Quotations and speakers clustering](#clustering)* : Cluster the quotations and the speakers according to the a quotation vector and the added features (data augmentation). This clustering will be further mainly used to develop a recommandation tool.\n",
    "    - Collaborators assigned to that task: Raffaele.\n",
    "\n",
    "## **[Generate the results for the final story](#Results)**\n",
    "\n",
    "- [General Statitics](#Statistics) : \n",
    "- [Country map](#Country) : \n",
    "- [Sector map](#Sector) : \n",
    "- [Visualize speakers evolution](#2Dplot) :\n",
    "- [Recommandation Tool](#Recommandation) :\n",
    "- [Correlation with stocks](#Stocks) :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before diving into the code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have a `Data` folder containing the following files: \n",
    "- The quotebank datasets for each year: `quotes-yyyy.json.bz2`\n",
    "- The speaker attributes folder `speaker-attributes.parquet` as well as the associated lookup table `wikidata_labels_descriptions_quotebank.csv.bz2`\n",
    "\n",
    "Make sure you have a `Brexit_datas` folder containing the following files available on this Google drive: \n",
    "- The quotebank dataset containing brexit quotations: \n",
    "- The quotebank dataset containing the brexit quotations with a sentiment analysis\n",
    "- The quotebank dataset containing the quotes translated into vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import useful librairies and define useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARD LIBRAIRIES\n",
    "from os.path import exists\n",
    "import bz2 \n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Dynamic graphs\n",
    "import plotly.express as px\n",
    "import dash\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "# Machine learning librairies\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from tsne_torch import TorchTSNE as TSNE\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Graph algorithms\n",
    "import networkx as nx\n",
    "\n",
    "# import string distances\n",
    "import stringdist\n",
    "\n",
    "# Load the lexicon for sentiment analysis\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data files\n",
    "PATHS_TO_FILE = ['Data/quotes-20%d.json.bz2' % i for i in range(15,21)]\n",
    "\n",
    "# Columns to analyte for one-hot vectorization task\n",
    "columns_to_map = ['nationality', 'gender', 'ethnic_group', 'occupation', 'party', 'academic_degree', 'religion']\n",
    "\n",
    "# type of the sentence_transformer\n",
    "sentence_transformer_type = 'all-MiniLM-L6-v2' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Preprocessing'></a>\n",
    "\n",
    "# Preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Sanity_check'></a>\n",
    "\n",
    "## Data exploration and Sanity check\n",
    "\n",
    "We decided to perform the following snaity checks on the original datas: \n",
    "\n",
    "- We first check that each entry for each quotation is specified in the right format (e.g. `numOccurences` should be an integer).\n",
    "- We check that the `probas` sum to 1.\n",
    "- We check that the `numOccurences` is superior or equal to the length of the list containing the urls.\n",
    "- The `date` is consistent with the dataset they are coming from\n",
    "- We check that if a `qids` exists then a `speaker` should be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECK FUNCTIONS\n",
    "\n",
    "def check_type(instance,entry,dtype):\n",
    "    return type(instance[entry]) == dtype\n",
    "\n",
    "def check_probas(instance):\n",
    "    if len(instance) > 0:\n",
    "        proba_sum = sum([float(potential[1]) for potential in instance[\"probas\"]])\n",
    "        if proba_sum < 0.98 or proba_sum > 1.02:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_numOcc(instance):\n",
    "    return (len(instance[\"urls\"]) <= instance[\"numOccurrences\"])\n",
    "\n",
    "def check_date(instance,year):\n",
    "    quotation_year = int(instance[\"date\"][:4])\n",
    "    return (quotation_year == year)\n",
    "\n",
    "def check_author_qids(instance):\n",
    "    if len(instance[\"qids\"]) > 0 and instance[\"speaker\"] is None:\n",
    "        return False\n",
    "    else: \n",
    "        return True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the types for each entry\n",
    "TYPES = {\"quoteID\":str,\n",
    "         \"quotation\":str,\n",
    "         \"speaker\":str,\n",
    "         \"qids\":list,\n",
    "         \"date\":str,\n",
    "         \"numOccurrences\":int,\n",
    "         \"probas\":list,\n",
    "         \"urls\":list,\n",
    "         \"phase\":str}\n",
    "\n",
    "error_file = \"Data/error_file.json.bz2\"\n",
    "\n",
    "\n",
    "if not exists(error_file):\n",
    "    with bz2.open(error_file, 'wb') as e_file:\n",
    "        # Loop over the different files that we will read\n",
    "        for quotebank_data in PATHS_TO_FILE:\n",
    "            year = int(quotebank_data[-13:-9])\n",
    "            print(\"Reading \",quotebank_data,\" file...\")\n",
    "            # Open the file we want to read\n",
    "            with bz2.open(quotebank_data, 'rb') as s_file:\n",
    "                # Loop over the samples\n",
    "                for instance in s_file:\n",
    "                    potential_error = \"\"\n",
    "                    # Loading a sample\n",
    "                    instance = json.loads(instance)\n",
    "                    #### CHECK THE TYPES ####\n",
    "                    for key, value in TYPES.items():\n",
    "                        if not check_type(instance,key,value):\n",
    "                            potential_error += \"| Type problem: \" + key + \" |\"\n",
    "                            # Continue because there exists a problem with the type that may affect the other checks\n",
    "                            continue\n",
    "                    #### CHECK THE PROBAS ####\n",
    "                    if not check_probas(instance):\n",
    "                        potential_error += \"| Probas problem |\"\n",
    "                    #### CHECK THE DATE ####\n",
    "                    if not check_date(instance,year):\n",
    "                        potential_error += \"| Date problem |\"\n",
    "                    #### CHECK THE NUMOCCURENCES ####\n",
    "                    if not check_numOcc(instance):\n",
    "                        potential_error += \"| NumOccurences problem |\"\n",
    "                    #### CHECK THE AUTHOR-QIDS ####\n",
    "                    if not check_author_qids(instance):\n",
    "                        potential_error += \"| Author-qids problem |\"\n",
    "                    # WRITE INTO THE FILE FOR POTENTIAL ERRORS #\n",
    "                    if len(potential_error) > 0:\n",
    "                        instance[\"error\"] = potential_error\n",
    "                        e_file.write((json.dumps(instance)+'\\n').encode('utf-8'))\n",
    "\n",
    "pd.read_json('Data/error_file.json.bz2',compression=\"bz2\",lines=True).shape                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='extraction'></a>\n",
    "\n",
    "## Data extraction\n",
    "\n",
    "As mentionned previously, we are planning to analyze the way Brexit is perceived and the way it influenced other things. To be able to perform such tasks, we need first to extract the quotations that are talking about Brexit. To do so we will follow the following pipeline:\n",
    "\n",
    "1. Define a neighborhood containing all the words that are respectively closely related to Brexit. This neighborhood will be a list of words or expressions that are commonly used to refer to Brexit.\n",
    "2. Select all the quotations for which, at least, one word/expression from the vocabulary neighborhood appears in it.\n",
    "3. Store the new two datasets in the `Brexit_quotes.json.bz2` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists('Brexit_datas/Brexit_quotes.json.bz2'):\n",
    "    # Input file\n",
    "    PATHS_TO_FILE = ['Data/quotes-20%d.json.bz2' % i for i in range(15,21)]\n",
    "    # Output file\n",
    "    PATH_TO_OUT = 'Brexit_datas/Brexit_quotes.json.bz2'\n",
    "\n",
    "    # Open the file where we will write\n",
    "    with bz2.open(PATH_TO_OUT, 'wb') as d_file:\n",
    "        # Loop over the different files that we will read\n",
    "        for quotebank_data in PATHS_TO_FILE:\n",
    "            print(\"Reading \",quotebank_data,\" file...\")\n",
    "            # Open the file we want to read\n",
    "            with bz2.open(quotebank_data, 'rb') as s_file:\n",
    "                # Loop over the samples\n",
    "                for instance in s_file:\n",
    "                    # Loading a sample\n",
    "                    instance = json.loads(instance)\n",
    "                    # Extracting the quotation\n",
    "                    quotation = instance['quotation']\n",
    "                    # Check if the quotation contains at least one word related to Brexit\n",
    "                    if \"brexit\" in quotation.lower():\n",
    "                        # Writing in the new file\n",
    "                        d_file.write((json.dumps(instance)+'\\n').encode('utf-8'))\n",
    "\n",
    "quotebank_brexit = pd.read_json('Brexit_datas/Brexit_quotes.json.bz2',compression=\"bz2\",lines=True)\n",
    "quotebank_brexit.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='augmentation'></a>\n",
    "\n",
    "## Data augmentation\n",
    "\n",
    "When we will generate the results for the final story, we will need more information than the initial features provided. The further analysis will require to have access to other features such as the sentiment carried by the quotation and additional information about the author. To do so, the following pipeline will be performed on each quotation:\n",
    "\n",
    "1. **[Adding features related to the author](#Features_Author)** :  Using the provided file `speaker_attributes.parquet` that was extracted from the Wikidata knowledge base, the following attributes are of interest for each speaker:\n",
    "    - `occupation`: describes the author's occupancy \n",
    "    - `party` identifies the political affiliation of the speaker.\n",
    "    - `academic_degree` gives information about the education of the author as well as their alma mater.\n",
    "    - `nationality` identifies the citizenship(s) of the author.\n",
    "    - `date_of_birth`: identifies the date of birth of the speaker.\n",
    "    - `gender`: identifies the gender of the speaker.\n",
    "    - `ethnic_group`: identifies the ethnic group of the speaker.\n",
    "    - `religion`: identifies the religion of the speaker. \n",
    "\n",
    "    The provided `speaker_attributes.parquet` file contains attributes in terms of QIDs, thereby being uninterpretable by humans. To map the QIDs to meaningful labels, we used the provide the file `wikidata_labels_descriptions_quotebank.csv.bz`.\n",
    "    \n",
    "    The aforementioned attributes may not be available for all authors. When it is the case, a NaN value is assigned.\n",
    "\n",
    "2. **[Adding features issued from a sentiment analysis](#Sentiment_Quote)** : The last feature of interest is the sentiment that is carried by the quotation. For the sake of simplicity, each quotation will be classified into three different categories: *Negative*, *Neutral* and *Positive*. \n",
    "Sentiment Analysis task can be performed using pretrained Deep Neural Networks. We decided to use **Vader** Neural network for its good performance. NLTK's Vader sentiment analysis tool uses a bag of words approach with some simple heuristics. More on it [here](https://github.com/cjhutto/vaderSentiment). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Features_Author'></a>\n",
    "***Loading the speaker_attributes.parquet file***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parquet that contains the information about speakers\n",
    "df_attributes = pd.read_parquet('Data/speaker_attributes.parquet')\n",
    "\n",
    "# we are not interested in the aliases, lastrevid, US_congress_bio_ID, id, candidacy and type.\n",
    "keep_attributes = ['id','label', 'date_of_birth', 'nationality', 'gender', 'ethnic_group', 'occupation', 'party', 'academic_degree', 'religion']\n",
    "# Set the index\n",
    "df_attributes = df_attributes[keep_attributes].set_index('id')\n",
    "# Sanity check for the qids\n",
    "print(\"Sanity check ok ? : \",df_attributes.index.is_unique)\n",
    "# Let's have a look\n",
    "df_attributes.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Features_Author'></a>\n",
    "***Mapping the QIDs to meaningful labels***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionnary to use it as a lookup table \n",
    "df_map = pd.read_csv('Data/wikidata_labels_descriptions_quotebank.csv.bz2', compression='bz2', index_col='QID')\n",
    "# Dictionnary where qids are keys and values are corresponding element\n",
    "map_dict = df_map.Label.to_dict()\n",
    "\n",
    "def mapping(QIDs):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to map all the QIDs to their labels, \n",
    "    using wikidata_labels_descriptions_quotebank.csv\n",
    "    \"\"\"\n",
    "    \n",
    "    if QIDs is None:\n",
    "        return np.nan\n",
    "    else:\n",
    "        QIDs_mapped = []\n",
    "        for QID in QIDs:\n",
    "            try:\n",
    "                # If a correspondance exists\n",
    "                QIDs_mapped.append(map_dict[QID])\n",
    "            except KeyError:\n",
    "                # If no correspondance exits\n",
    "                continue\n",
    "        # If nothing was extracted\n",
    "        if len(QIDs_mapped) == 0:\n",
    "            return np.nan\n",
    "        # Things extracted\n",
    "        else:\n",
    "            return QIDs_mapped\n",
    "\n",
    "\n",
    "\n",
    "# For each column perform the mapping to transform qids to real value\n",
    "for column in columns_to_map:\n",
    "    df_attributes[column] = df_attributes[column].apply(mapping)\n",
    "    \n",
    "df_attributes.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Sentiment_Quote'></a>\n",
    "***Adding sentiment score to each quote***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_score(quote):\n",
    "    \"\"\"The purpose of this function is to use the sentiment analysis tool VADER to find the sentiment associated with a quote.\"\"\"\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sid.polarity_scores(quote)\n",
    "    \n",
    "    # The Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between\n",
    "    # -1(most extreme negative) and +1 (most extreme positive).\n",
    "    # positive sentiment : (compound score >= 0.05) \n",
    "    # neutral sentiment : (compound score > -0.05) and (compound score < 0.05) \n",
    "    # negative sentiment : (compound score <= -0.05)\n",
    "    # see https://predictivehacks.com/how-to-run-sentiment-analysis-in-python-using-vader/\n",
    "    # or https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/\n",
    "    \n",
    "    # decide sentiment as positive, negative and neutral\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        return \"Positive\"\n",
    " \n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        return \"Negative\" \n",
    " \n",
    "    else :\n",
    "        return \"Neutral\"\n",
    "\n",
    "# backup quotebank dataframe with sentiment score if the corresponding file doesn't exists\n",
    "if not exists(\"Brexit_datas/quotebank_brexit_with_sentiment.json.bz2\"):\n",
    "    quotebank_brexit['sentiment_score'] = quotebank_brexit.quotation.apply(sent_score) \n",
    "    quotebank_brexit.to_json(\"Brexit_datas/quotebank_brexit_with_sentiment.json.bz2\")\n",
    "    \n",
    "else:\n",
    "    quotebank_brexit = pd.read_json(\"Brexit_datas/quotebank_brexit_with_sentiment.json.bz2\",compression=\"bz2\")\n",
    "\n",
    "quotebank_brexit.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleaning'></a>\n",
    "\n",
    "## Data merging and cleaning\n",
    "\n",
    "Depending on the different task we want to perform we will need to have the dataset in various forms, thus we will generate three types of dataset: \n",
    "- `quotebank_brexit`: original dataset [+sentiment score] where the quotations are cleaned.\n",
    "- `aug_quotebank_brexit`: dataset with augmented datas where both quotations and speakers are cleaned.\n",
    "- `oneh_quotebank_brexit`: copy of `aug_quotebank_brexit` where categorical values are one-hot encoded.\n",
    "\n",
    "Thus we first start by [cleaning the quotations](#cleaning_quotation) on the `quotebank_brexit` dataset and then we [clean the speakers](#cleaning_speaker) to be able to merge with augmented data and generate the `aug_quotebank_brexit`. After a [processing](#aug_preprocessing) of the `aug_quotebank_brexit` we will finally [one hot encode](#one_hot_encoding) to generate the `oneh_quotebank_brexit` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleaning_quotation'></a>\n",
    "\n",
    "### Cleaning the quotations\n",
    "\n",
    "We noticed that some quotations were very similar, actually too similar. They sometimes differ from the fact that one quotation is nested in another or sometimes they only differ by one character. Here is an example of such a quotation:\n",
    "- quoteID: **2018-01-26-042810** - *\"I look at Nigel Farage's example. It took 17 years, but Brexit came,\"*\n",
    "- quoteID: **2018-01-26-042811** - *\"I look at Nigel Farage's example. It took 17 years, but Brexit came. I don't plan to wait that long\"*\n",
    "\n",
    "We need to remove these kind of *duplicates*. To do so we followed this pipeline:\n",
    "- Converting quotations into vectors using [SentenceTransformer](https://www.sbert.net/docs/usage/semantic_textual_similarity.html) deep neural network.\n",
    "- Computing [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between each pair of quotations\n",
    "- Removing quotations that are too similar from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode quotation \n",
    "if not exists(\"Brexit_datas/vector_quotes.csv.gz\"):\n",
    "    encoder = SentenceTransformer(sentence_transformer_type)\n",
    "    # Encode quotations\n",
    "    quotes_encoded = encoder.encode(quotebank_brexit['quotation'].values, convert_to_numpy=True)\n",
    "    # Convert to df\n",
    "    quotes_df = pd.DataFrame(quotes_encoded, index = quotebank_brexit.index)\n",
    "    # Add speaker column\n",
    "    quotes_df[\"speaker\"] = quotebank_brexit[\"speaker\"].values\n",
    "    # Export into a compressed format\n",
    "    quotes_df.to_csv(\"Brexit_datas/vector_quotes.csv.gz\")\n",
    "    \n",
    "else:\n",
    "    # Read the file\n",
    "    quotes_df = pd.read_csv(\"Brexit_datas/vector_quotes.csv.gz\",index_col=0,compression=\"gzip\")\n",
    "\n",
    "quotes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the supported device\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu') # don't have GPU \n",
    "    return device\n",
    "\n",
    "# convert a df to tensor to be used in pytorch\n",
    "def df_to_tensor(df):\n",
    "    device = get_device()\n",
    "    return torch.from_numpy(df.values).float().to(device)\n",
    "\n",
    "# compare pairwise similarity\n",
    "def filter_similar(df):\n",
    "    # Get the embeddings computed before\n",
    "    embeddings = df_to_tensor(df.drop(columns='quoteID'))\n",
    "    # Compute cosine similarity\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "    # Convert to df\n",
    "    score = pd.DataFrame(cosine_scores.numpy())\n",
    "    index = set(score[score>0.95].stack().index.tolist())\n",
    "    index = [(a,b) for (a,b) in index if a != b]\n",
    "    # multiple tuples can have common element: need to merge them\n",
    "    graph = nx.Graph(index)\n",
    "    index = list(nx.connected_components(graph))\n",
    "    # map the indices with the Qid of the quote\n",
    "    index = [tuple(df.quoteID.iloc[ind] for ind in path) for path in index]\n",
    "    return index\n",
    "\n",
    "similar_count = quotes_df.assign(quoteID=quotebank_brexit.quoteID.values).groupby('speaker').apply(filter_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of two similar quotations\n",
    "print(\"Yvette Cooper:\", similar_count[\"Yvette Cooper\"][0])\n",
    "quotebank_brexit[(quotebank_brexit.quoteID=='2019-01-27-041304') | (quotebank_brexit.quoteID=='2019-01-27-029003')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices to drop\n",
    "def drop_duplicate_quotes(ids):\n",
    "    return [[quoteID for quoteID in path[1:]] for path in ids]\n",
    "        \n",
    "# generate the list of quoteIDs to be removed\n",
    "to_be_removed = similar_count.apply(drop_duplicate_quotes).values.sum()\n",
    "to_be_removed = list(itertools.chain.from_iterable(to_be_removed))\n",
    "\n",
    "quotebank_brexit = quotebank_brexit[~quotebank_brexit.quoteID.isin(to_be_removed)]\n",
    "\n",
    "print(\"Number of quotations to be removed: \",len(to_be_removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleaning_speaker'></a>\n",
    "\n",
    "### Cleaning the speakers\n",
    "\n",
    "In the `aug_quotebank_brexit` we will have a lot of information coming from the speaker such as the `nationality` or the `occupation`. However one can notice that sometimes the neural network didn't succeed in finding a speaker and fill `speaker` entry with `None` value. These missing values are difficult to handle as it would require to guess who told the quotation. One could think about training a classifier on the datas where the speaker is mentionned but it is actually a fastidious task that we are not able to manage. So unfortunaltely the last solution to get ride of these datas was chosen in the `aug_quotebank_brexit`.\n",
    "\n",
    "An other issue comes from the fact that for one speaker many qids were mentionned. We interpreted this multiple values as multiple wikipedia pages that may point to the same person but in different langagues. This could also come from the fact that there exists multiple wikipedia pages that points to different persons as homonyms may exist. So when many qids are mentionned we check that all the attributes are similar for all the qids, if not, then we are not able to determine which qid is the right one so unfortunately we discard the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_consistent_qids(QIDS_original):\n",
    "    QIDS = QIDS_original.copy()\n",
    "    if len(QIDS) == 0:\n",
    "        return pd.NA\n",
    "    elif len(QIDS) == 1:\n",
    "        return QIDS_original[0]\n",
    "    else:\n",
    "        while len(QIDS) > 1:\n",
    "            first_idx = QIDS.pop(-1)\n",
    "            try:\n",
    "                first = df_attributes.loc[first_idx].fillna(0)\n",
    "                second_idx = QIDS.pop(-1)\n",
    "                try:\n",
    "                    second = df_attributes.loc[second_idx].fillna(0)\n",
    "                except KeyError:\n",
    "                    QIDS.append(first_idx)\n",
    "                    continue\n",
    "            except KeyError:\n",
    "                continue\n",
    "            try: \n",
    "                if (first != second).sum() > 0:\n",
    "                    return pd.NA\n",
    "            except ValueError:\n",
    "                return pd.NA\n",
    "        return QIDS_original[0]\n",
    "\n",
    "if not exists(\"Brexit_datas/aug_quotebank.json.bz2\"):\n",
    "    # Remove nan values\n",
    "    aug_quotebank_brexit = quotebank_brexit[quotebank_brexit.speaker != \"None\"]\n",
    "\n",
    "    # Remove speakers with multiple different qids\n",
    "    aug_quotebank_brexit.qids = aug_quotebank_brexit.qids.apply(check_consistent_qids)\n",
    "    aug_quotebank_brexit = aug_quotebank_brexit[~aug_quotebank_brexit.qids.isna()]\n",
    "\n",
    "    # Merge the augmented quotebank brexit with df_attributes on qids\n",
    "    aug_quotebank_brexit = pd.merge(aug_quotebank_brexit, df_attributes, 'inner', left_on=\"qids\", right_index=True)\n",
    "\n",
    "    # Export to json to add check points\n",
    "    aug_quotebank_brexit.to_json(\"Brexit_datas/aug_quotebank.json.bz2\")\n",
    "else:\n",
    "    # Read json if it already exists\n",
    "    aug_quotebank_brexit = pd.read_json(\"Brexit_datas/aug_quotebank.json.bz2\",compression=\"bz2\")\n",
    "\n",
    "# Let's have a look\n",
    "print(\"New shape\",aug_quotebank_brexit.shape)\n",
    "aug_quotebank_brexit.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {}\n",
    "\n",
    "for col in columns_to_map:\n",
    "    col_serie = aug_quotebank_brexit[col].copy()\n",
    "    unique_values[col] = pd.unique(col_serie.apply(pd.Series).stack())\n",
    "    print(col,\" : number of different categories = \",len(unique_values[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aug_preprocessing'></a>\n",
    "\n",
    "### What about the categorical features added\n",
    "\n",
    "Now that we added new features, we had a look at their properties and We noticed that there were more than 800 different occupations, we would like to classify them in *supercategories*. Thee problem is that we do not have any labels on them and using ML techniques such as pre-trained neural networks would be over-killed. We rather followed a semi-manual approach described below: \n",
    "- Identify which words are the more frequent in the `occupation` names and add them a label. We will call them key words.\n",
    "- For each `occupation` match it with any key word labels if any.\n",
    "- Pick the remaining occupations and label them manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Manage frequent keywords***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame of the occupations\n",
    "occupation_df = pd.DataFrame(unique_values[\"occupation\"],columns=[\"occupation\"])\n",
    "\n",
    "key_words = []\n",
    "\n",
    "# Loop over the occupations\n",
    "for occupation in unique_values[\"occupation\"]:\n",
    "    # Split the occupation string and concatenate\n",
    "    key_words += occupation.split()\n",
    "\n",
    "# Convert to a Dataframe\n",
    "key_words_df = pd.DataFrame(key_words,columns=[\"occupation\"])\n",
    "# Put all strings to lower\n",
    "key_words_df.occupation = key_words_df.occupation.str.lower()\n",
    "# For each key word count the number of occurences and sort by descending\n",
    "key_words_df = key_words_df.groupby(\"occupation\").size().reset_index(name=\"Count\").sort_values(by=\"Count\",ascending=False)\n",
    "\n",
    "# If the classification has not been already done\n",
    "if not exists(\"Brexit_datas/occupation_class/occupation_agg.csv\"):\n",
    "    key_words_df.to_csv(\"Brexit_datas/occupation_class/occupation_agg.csv\")\n",
    "\n",
    "print(\"Look at the most frequent keywords\")\n",
    "print(key_words_df.head(3))\n",
    "\n",
    "answer = input(\"Is the classification of keywords done ?\")\n",
    "\n",
    "if (answer.lower() == \"yes\"):\n",
    "    # Get the classified keywords\n",
    "    key_words_classified = pd.read_csv(\"Brexit_datas/occupation_class/occupation_agg.csv\",index_col=0)\n",
    "    # Get ride of keywords that have not been classified\n",
    "    key_words_classified = key_words_classified.loc[~key_words_classified.Category.isna()]\n",
    "    # Manage the case when several categories have been entered\n",
    "    key_words_classified.Category = key_words_classified.Category.apply(lambda x: x.split(\"-\"))\n",
    "    # let's have a look at the table\n",
    "    print(\"Look at the output table\")\n",
    "    print(key_words_classified.head(3))\n",
    "else:\n",
    "    print(\"Then please classify the keywords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Match occupation and keyword labels***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if keywords are contained in an occupation\n",
    "def check_string_in(occupation):\n",
    "    # Initialize the final list of the supercategories\n",
    "    final_list = []\n",
    "    # Loop over the key_words_classified\n",
    "    for items in key_words_classified.occupation.iteritems():\n",
    "        # If the keyword is contained in the occupation\n",
    "        if items[1] in occupation.lower():\n",
    "            # Concat the supercategories with th existing list\n",
    "            final_list = final_list + key_words_classified.loc[items[0],\"Category\"]\n",
    "    # If no categories return NaN\n",
    "    if len(final_list) == 0:\n",
    "        return pd.NA\n",
    "    # Else return the list without duplicates\n",
    "    else:\n",
    "        return list(set(final_list))\n",
    "        \n",
    "# Apply the function\n",
    "occupation_df[\"Category\"] = occupation_df.occupation.apply(check_string_in)\n",
    "\n",
    "if not exists(\"Brexit_datas/occupation_class/unclassified_occupation.csv\"):\n",
    "    # Export the occupations that have not been classified\n",
    "    occupation_df[occupation_df.Category.isna()].to_csv(\"Brexit_datas/occupation_class/unclassified_occupation.csv\")\n",
    "\n",
    "print(\"Look at the remaining occupations\")\n",
    "print(occupation_df[occupation_df.Category.isna()].head(3))\n",
    "\n",
    "answer = input(\"Is the classification of remaining occupations done ?\")\n",
    "\n",
    "if (answer.lower() == \"yes\"):\n",
    "    # Get the remaining occupations classified\n",
    "    remain_occupations_classified = pd.read_csv(\"Brexit_datas/occupation_class/unclassified_occupation.csv\",index_col=0)\n",
    "    # Merge with the current data frame\n",
    "    occupation_final_df = pd.merge(occupation_df,remain_occupations_classified,how=\"left\",on=\"occupation\",suffixes=(\"\",\"_2\"))\n",
    "    # Split into a list\n",
    "    occupation_final_df.Category_2 = occupation_final_df.Category_2.apply(lambda x: x.split(\"-\") if type(x) == str else pd.NA)\n",
    "    # Merge into a single column\n",
    "    occupation_final_df.loc[~occupation_final_df.Category_2.isna(),\"Category\"] = occupation_final_df.loc[~occupation_final_df.Category_2.isna(),\"Category_2\"]\n",
    "    # Drop the artificial column\n",
    "    occupation_final_df.drop(columns=[\"Category_2\"],inplace=True)\n",
    "    # Drop na values that corresponds to unclassifiable jobs such as nazi hunter\n",
    "    occupation_final_df.dropna(axis=0,inplace=True)\n",
    "    # Let's have a look\n",
    "    print(\"Final data set for the classification of occupations:\")\n",
    "    print(occupation_final_df.head(5))\n",
    "    # Export to a json file\n",
    "    if not exists(\"Brexit_datas/occupation_class/classified_occupation.json\"):\n",
    "        occupation_final_df.to_json(\"Brexit_datas/occupation_class/classified_occupation.json\")\n",
    "else:\n",
    "    print(\"Then please classify the remaining occupations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Label remaining occupations***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupation_final_df = pd.read_json(\"Brexit_datas/occupation_class/classified_occupation.json\").set_index(\"occupation\")\n",
    "\n",
    "# Let's have a look at the supercategories\n",
    "print(list(pd.unique(occupation_final_df.Category.apply(pd.Series).stack())))\n",
    "\n",
    "# Let's replace this into the aug_quotebank dataset\n",
    "def replace_occupation(occupation):\n",
    "    if type(occupation) == list:\n",
    "        if len(occupation) > 0:\n",
    "            new_occupation = []\n",
    "            for job in occupation:\n",
    "                try:\n",
    "                    new_occupation += occupation_final_df.loc[job,\"Category\"]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            if len(new_occupation) > 0:\n",
    "                return list(set(new_occupation))\n",
    "            else:\n",
    "                return pd.NA\n",
    "                \n",
    "    else:\n",
    "        return pd.NA\n",
    "\n",
    "aug_quotebank_brexit.occupation = aug_quotebank_brexit.occupation.apply(replace_occupation)\n",
    "aug_quotebank_brexit.head(2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='one_hot_encoding'></a>\n",
    "\n",
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot vectorization of columns cotaining categorical values\n",
    "dummy_col = \"AAADummy column for the sake\"\n",
    "# Make a copy\n",
    "oneh_quotebank_brexit = aug_quotebank_brexit.copy()\n",
    "\n",
    "# Check that the element is a list that contains only one string\n",
    "def ensure_list(value):\n",
    "  if isinstance(value, list):\n",
    "    for i in range(len(value)):\n",
    "      value[i] = str(value[i])\n",
    "  elif not pd.isna(value):\n",
    "    value = [value]\n",
    "  return value\n",
    "\n",
    "# Loop over categorical columns\n",
    "for col in columns_to_map:\n",
    "  # Get the serie\n",
    "  col_serie = aug_quotebank_brexit[col].copy().apply(ensure_list)\n",
    "  # Change nan values to a list containing a dummy column\n",
    "  col_serie[col_serie.isna()] = col_serie[col_serie.isna()].apply(lambda x: [dummy_col])\n",
    "    \n",
    "  # One hot vectorize\n",
    "  categorical_df = pd.get_dummies(col_serie.apply(pd.Series).stack()).groupby(level=0).sum()\n",
    "    \n",
    "  # Drop the dummy column\n",
    "  categorical_df.drop(columns=[dummy_col],inplace=True)\n",
    "  # Refresh unique values\n",
    "  unique_values[col] = categorical_df.columns\n",
    "    \n",
    "  # Join with quotebank brexit\n",
    "  oneh_quotebank_brexit = oneh_quotebank_brexit.join(categorical_df,how=\"left\",rsuffix=col[:3])\n",
    "  print(\"One hot vectorizing : \",col,\n",
    "        \"| NaN values : \",categorical_df.isna().apply(lambda x: x*1).sum().sum(),\n",
    "        \"| Number of different categories : \",len(categorical_df.columns),\n",
    "        \"| Shape reduced ? \",categorical_df.shape,oneh_quotebank_brexit.shape)\n",
    "  # Drop the categorical column\n",
    "  oneh_quotebank_brexit.drop(columns=col,inplace=True)\n",
    "  # Check for NaN values\n",
    "  print(\"Any NA in the final dataframe: \",oneh_quotebank_brexit.isna().apply(lambda x: x*1).sum().sum())\n",
    "\n",
    "print(\"Shape of the final data frame\",oneh_quotebank_brexit.shape)\n",
    "print(\"Any NA in the final dataframe: \",oneh_quotebank_brexit.isna().apply(lambda x: x*1).sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clustering_task'></a>\n",
    "\n",
    "# Quotations and speakers clustering\n",
    "\n",
    "The last preprocessing step consist in clustering the quotations as well as the speakers, this clustering will then be used to create a Recommandation Tool in the context of Brexit. The idea would be to first cluster the quotations and then the speakers such that two quotations/speakers that are in the same cluster are quotations/speakers carries on similar things/ideas. Performing such a task can be done following this pipeline:\n",
    "1. The first step is to convert sentences into vectors to be able to further perform the clustering. This task can be achieved using the [SentenceTransformer](https://www.sbert.net/docs/usage/semantic_textual_similarity.html) deep neural network. The vector obtained from this operation cab be then concatenated with the other existing features (that would be converted to one hot vectors if necessary).\n",
    "2. The second step consists in reducing the dimension of the datas before applying the clustering algorithm. This task can be achieved using the [T-stochastic neighbors embeddings](#https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) algorithm or the [Locally Linear Embeddings](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding) algorithm. These two techniques (specially the first one) are efficient non-linear dimensionality reduction methods.\n",
    "3. The third step is specific to speaker clustering. Indeed the vectorization of quotes as well as the reduction of dimensionality is only applied to quotes. Thus we need to perform an **aggregation** to be able to attribute a vector to each speaker. For each speaker, this aggregation can simply be done by taking the mean of the vectors associated with each of their quotations. \n",
    "4. The last step consist in performing the clustering operation. This task can be achieved using [Gaussian Mixture Model](https://scikit-learn.org/stable/modules/mixture.html#mixture) algorithm or  [Spectral Clustering](#https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    User-defined parameters for the task\n",
    "\"\"\"\n",
    "sentiment_amplification = 1.5 # coefficient of amplification of sentiment, amplification is applied after normalization\n",
    "normalize_tensor = True # chose whether to normalize the sentence tensor before clustering\n",
    "nb_clusters = 8   # Number of clusters to be identified\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Prepare dataframe for the one-hot vectorization task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_to_int(value):\n",
    "  return int(value == \"Positive\") - int(value == \"Negative\")\n",
    "  \n",
    "columns_to_drop = [\"date\",\"quoteID\",\"qids\",\"phase\",\"probas\",\"urls\",\"date_of_birth\",\"label\"]\n",
    "\n",
    "# restrict dataframe to the one needed\n",
    "cluster_df = oneh_quotebank_brexit.drop(columns=columns_to_drop)\n",
    "\n",
    "print(\"Is there any na values ?\",cluster_df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='One-hot'></a>\n",
    "\n",
    "## Step 2: Compose the vectorized dataframe\n",
    "\n",
    "1. Convert sentiment score into signed integer format: \"Positive\" = 1, \"Negative\" = -1, \"Neutral\" = 0\n",
    "2. For each column concerning the speaker information, generate a dummy dataframe (see DataFrame.get_dummy)\n",
    "3. Concatenate along columns all obtained dataframe\n",
    "4. Average all rows matching the same speaker (which is set as index)\n",
    "5. Normalize dataset by row and amplify sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Compose a vectorized dataframe\n",
    "\"\"\"\n",
    "\n",
    "quotes_df = pd.read_csv(\"Brexit_datas/vector_quotes.csv.gz\",compression=\"gzip\").drop(columns=\"speaker\")\n",
    "\n",
    "# normalize quotations vector\n",
    "quotes_df = (quotes_df - quotes_df.mean(axis=0))/quotes_df.std(axis=0)\n",
    "\n",
    "# Merge the two data frames\n",
    "cluster_full_df = pd.concat([cluster_df.drop(columns=\"quotation\"), quotes_df.loc[cluster_df.index]],axis = 1).set_index(\"speaker\")\n",
    "\n",
    "# convert sentiment_score to int format\n",
    "cluster_full_df['sentiment_score'] = cluster_full_df['sentiment_score'].apply(sentiment_to_int)\n",
    "\n",
    "# normalize numOccurences df by row\n",
    "cluster_full_df.numOccurrences = (cluster_full_df.numOccurrences - cluster_full_df.numOccurrences.mean()) / cluster_full_df.numOccurrences.std()\n",
    "\n",
    "# average over the same speaker\n",
    "cluster_full_df = cluster_full_df.groupby(level=0).agg(np.mean)\n",
    "\n",
    "#amplify sentiment\n",
    "cluster_full_df['sentiment_score'] *= sentiment_amplification\n",
    "\n",
    "cluster_full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TSNE'></a>\n",
    "\n",
    "## Step 3: Convert to *pytorch* tensor and apply TSNE aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into pytorch tensor\n",
    "full_data_tensor = df_to_tensor(cluster_full_df)\n",
    "\n",
    "# Apply T-stochastic neighboor embedding\n",
    "tsne_dim = 2 # TSNE reduction final dimension, default is 2\n",
    "data_np_emb = TSNE(n_components=tsne_dim, perplexity=30, n_iter=1000).fit_transform(full_data_tensor) # dim = Nxfinal_dim\n",
    "\n",
    "data_np_reduc = data_tensor_emb.transpose()\n",
    "\n",
    "# Visualize without clustering\n",
    "plt.scatter(data_np_tensor[0], data_np_tensor[1])\n",
    "plt.title(\"TSNE bidimensional reduction of the speaker vectorization\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_np_reduc[0], data_np_reduc[1])\n",
    "plt.title(\"TSNE bidimensional reduction of the speaker vectorization\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Clustering'></a>\n",
    "\n",
    "## Step 4: Apply the clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Clustering\n",
    "\"\"\" \n",
    "\n",
    "# Apply Clustering\n",
    "clustering = SpectralClustering(nb_clusters).fit(data_tensor_emb)\n",
    "\n",
    "\"\"\"\n",
    "    Visualization\n",
    "\"\"\" \n",
    "fig, axis = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "results = data_tensor_emb.transpose()\n",
    "\n",
    "# Visualize without clustering\n",
    "axis[0].scatter(results[0], results[1])\n",
    "\n",
    "\n",
    "for label in range(nb_clusters):\n",
    "    # select data by clustering label\n",
    "    points = data_tensor_emb[clustering.labels_ == label]\n",
    "    points = points.transpose()\n",
    "    # plot data\n",
    "    axis[1].scatter(points[0], points[1])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Results'></a>\n",
    "\n",
    "# Generate the results for the final story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Statistics'></a>\n",
    "\n",
    "## General Statistics\n",
    "\n",
    "Now that we had preprocessed the datas let's have a look on different basic statistics to explore deeply the dataset. Let's first look at the distribution of the quotations accross time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(quotebank_brexit,x=\"date\")\n",
    "fig.update_layout(title=\"Number of quotations about Brexit accross time\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the top 20 countries that are providing the more quotations about Brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df = oneh_quotebank_brexit.loc[:,unique_values[\"nationality\"]].sum(axis=0).T.to_frame().reset_index()\n",
    "country_df = country_df.sort_values(by=0,ascending=False).iloc[:20]\n",
    "fig = px.bar(country_df,y=0,x=\"index\",log_y=True)\n",
    "fig.update_layout(title=\"Number of quotations about Brexit accross time\",\n",
    "                  xaxis_title=\"Country\",yaxis_title=\"Count\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the top 20 ethnic group that are providing the more quotations about Brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnic_df = oneh_quotebank_brexit.loc[:,unique_values[\"ethnic_group\"]].sum(axis=0).T.to_frame().reset_index()\n",
    "ethnic_df = ethnic_df.sort_values(by=0,ascending=False).iloc[:20]\n",
    "fig = px.bar(ethnic_df,y=0,x=\"index\",log_y=True)\n",
    "fig.update_layout(title=\"Number of quotations about Brexit accross time\",\n",
    "                  xaxis_title=\"Ethnic group\",yaxis_title=\"Count\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the top 20 countries that are providing the more quotations about Brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df = oneh_quotebank_brexit.loc[:,unique_values[\"occupation\"]].sum(axis=0).T.to_frame().reset_index()\n",
    "job_df = job_df.sort_values(by=0,ascending=False)\n",
    "fig = px.bar(job_df,y=0,x=\"index\",log_y=True)\n",
    "fig.update_layout(title=\"Number of quotations about Brexit accross time\",\n",
    "                  xaxis_title=\"Sector\",yaxis_title=\"Count\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the top 10 speakers that are providing the more quotations about Brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_quotebank_brexit.groupby(\"speaker\").size().reset_index(name=\"count\").loc[:,[\"speaker\",\"count\"]].sort_values(by=\"count\",ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Country'></a>\n",
    "\n",
    "## Analyze the way Brexit is perceived in European countries\n",
    "\n",
    "Recall that the goal is to analyze the way Brexit is perceived in each Europe country based on the sentiment carried by the quotation. Besides we would like to add the time dimension to this analysis, meaning that we would like to follow the evolution of the overall feelings towards Brexit. A view of the expected result is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df_by_year = select_by_year(2015,2020,\"nationality\").rename(columns={\"index\":\"country_init\"})\n",
    "\n",
    "# countries per countries\n",
    "df_countries = px.data.gapminder().query(\"year == 2007\")\n",
    "\n",
    "# merge country_df_by_year with previous dataset \n",
    "df = pd.merge(left=df_countries, right=country_df_by_year, how=\"outer\", left_on=\"country\",right_on=\"country_init\")\n",
    "\n",
    "left_nan_tensor = df.loc[df[\"pop\"].isna(),\"country_init\"].values\n",
    "right_nan_tensor = df.loc[df[\"count\"].isna(),\"country\"].values\n",
    "threshold = 0.55\n",
    "\n",
    "for left in left_nan_tensor:\n",
    "    for right in right_nan_tensor:\n",
    "        if stringdist.levenshtein_norm(left,right) < threshold:\n",
    "            print(\"{'\",left,\"':'\",right,\"'}\")\n",
    "\n",
    "map_dic = {'Hong Kong':'Hong Kong, China',\n",
    "           'Slovakia':'Slovak Republic',\n",
    "           'The Gambia':'Gambia',\n",
    "           'United States of America':'United States',\n",
    "           'Yemen':'Yemen, Rep.'}\n",
    "\n",
    "country_df_by_year = select_by_year(2015,2020,\"nationality\").rename(columns={\"index\":\"country\"})\n",
    "indexes = country_df_by_year.country.isin(map_dic.keys())\n",
    "country_df_by_year.loc[indexes,\"country\"] = country_df_by_year.loc[indexes,\"country\"].map(map_dic)\n",
    "\n",
    "df = pd.merge(left=df_countries, right=country_df_by_year, how=\"inner\", on=\"country\")\n",
    "\n",
    "df = df[[\"country\", \"continent\", \"iso_alpha\", \"iso_num\", \"Positive\", \"count\"]]\n",
    "\n",
    "\n",
    "fig = px.choropleth(df, locations=\"iso_alpha\", color=\"Positive\",\n",
    "                    color_continuous_scale=[\"red\", \"blue\", \"green\"],\n",
    "                    color_continuous_midpoint=50,\n",
    "                    hover_data=[\"count\",\"country\"],\n",
    "                    labels={'iso_alpha': 'country code', 'Positive':'In favor of Brexit [%]'},\n",
    "                    title=\"% of  favor of country of Brexit\")\n",
    "fig.update_layout(height=400, margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    dcc.Graph(id=\"country_map\"),\n",
    "    html.P(\"Year:\"),\n",
    "    dcc.RangeSlider(\n",
    "        id='range-slider',\n",
    "        min=2015, max=2020, step=1,\n",
    "        marks={2015: '2015', 2016:'2016',2017:'2017',2018:'2018',2019:'2019',2020: '2020'},\n",
    "        value=[2015,2020]\n",
    "    ),\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"country_map\", \"figure\"), \n",
    "    [Input(\"range-slider\", \"value\")])\n",
    "def update_bar_chart(slider_range):\n",
    "    low, high = slider_range\n",
    "    df = select_by_year(2015,2020,\"nationality\")\n",
    "    fig = px.choropleth(df, locations=\"iso_alpha\", color=\"Positive\",\n",
    "                        color_continuous_scale=[\"red\", \"white\", \"blue\"],\n",
    "                        color_continuous_midpoint=50,\n",
    "                        hover_data=[\"count\",\"country\"],\n",
    "                        labels={'iso_alpha': 'country code', 'Positive':'In favor of Brexit [%]'},\n",
    "                        title=\"% of  favor of country of Brexit\")\n",
    "    fig.update_layout(height=400, margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "    return fig\n",
    "\n",
    "app.run_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Sector'></a>\n",
    "\n",
    "## Analyze the way Brexit is perceived in different sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_by_year(low_year,up_year, col):\n",
    "    year_col = pd.DatetimeIndex(oneh_quotebank_brexit.date).year\n",
    "    cols = list(unique_values[col]) + [\"sentiment_score\"]\n",
    "    filter_df = oneh_quotebank_brexit.loc[(year_col >= low_year) & (year_col <= up_year),cols]\n",
    "    filter_df = filter_df.groupby(\"sentiment_score\").sum()\n",
    "    count = filter_df.sum(axis=0)\n",
    "    filter_df = (filter_df * 100/ filter_df.sum(axis=0)).T.reset_index()\n",
    "    filter_df[\"count\"] = count.values\n",
    "    return filter_df\n",
    "\n",
    "sector_df = select_by_year(2015,2020,\"occupation\")\n",
    "\n",
    "fig = px.bar(sector_df, x=\"index\",text=\"count\",y=sector_df.columns[-4:-1])\n",
    "fig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\n",
    "fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Dynamic visualization with Dash, going to the url output by the cell below should give you a plot like that with a range slider to select the year range:***\n",
    "\n",
    "![Dash render](Images\\dash_sector.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    dcc.Graph(id=\"scatter-plot\"),\n",
    "    html.P(\"Year:\"),\n",
    "    dcc.RangeSlider(\n",
    "        id='range-slider',\n",
    "        min=2015, max=2020, step=1,\n",
    "        marks={2015: '2015', 2016:'2016',2017:'2017',2018:'2018',2019:'2019',2020: '2020'},\n",
    "        value=[2015,2020]\n",
    "    ),\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"scatter-plot\", \"figure\"), \n",
    "    [Input(\"range-slider\", \"value\")])\n",
    "def update_bar_chart(slider_range):\n",
    "    low, high = slider_range\n",
    "    sector_df = select_by_year(low,high,\"occupation\")\n",
    "    fig = px.bar(sector_df, x=\"index\",text=\"count\",y=sector_df.columns[-4:-1])\n",
    "    return fig\n",
    "\n",
    "app.run_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2Dplot'></a>\n",
    "\n",
    "## Visualize speakers orientation trough a 2D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Recommandation'></a>\n",
    "\n",
    "## Recommandation tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Stocks'></a>\n",
    "\n",
    "## Correlation with stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "FTSE_companies = pd.read_excel(\"Brexit_datas\\FTSE_100_list.xlsx\")\n",
    "tickers_FTSE = list(FTSE_companies.Ticker)\n",
    "\n",
    "# Get the data for the FTSE companies\n",
    "stock_action_FTSE = yf.download(tickers_FTSE,'2015-01-01','2020-08-01')['Adj Close']\n",
    "\n",
    "stock_action_FTSE = stock_action_FTSE.dropna(axis=1).reset_index()\n",
    "\n",
    "fig = px.line(stock_action_FTSE,x=\"Date\",y=stock_action_FTSE.columns[1:10],log_y=True)\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c7ebd9986d9dd66f5c55cec683961b660fa3bc87c5eb159f7033b074e7bd831"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
