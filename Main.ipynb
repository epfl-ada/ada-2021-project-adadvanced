{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone 2\n",
    "\n",
    "Here we will describe the whole pipeline to get all the results we would like to include in the final story (on the final website). We will go through all the different steps and describe in detail the operations needed. \n",
    "\n",
    "For the final story we decided to focus on the influence of the Brexit. More precisely we would like to assess how the Brexit was perceived and how it evolved over the years. The visualizations we aim at providing in the final story are detailed in this [Section](#Results).\n",
    "\n",
    "## **[Preprocessing steps](#Preprocessing)**\n",
    "\n",
    "As usual the first step consists in several substeps that aims at cleaning and transforming the data. By clicking on the task link, you can access the respective pipeline.\n",
    "- *[Data exploration and Sanity check](#Sanity_check)* : Explore the dataset, check its consistency and get familiar with the different features/information provided.\n",
    "- *[Data extraction](#extraction)* : Extract the data of interest that will be used to perform the tasks related to each idea.\n",
    "- *[Data augmentation](#augmentation)* : Perform a data augmentation to get more features about the quotations such as the quote field, the nationality of the speaker and so on... These new features will be used to perform the tasks related to each idea.\n",
    "- *[Data cleaning and merging](#augmentation)* : Perform a final cleaning on the quotations as well as on the speakers and generate 3 main datasets that will be used for the analysis\n",
    "- *[Quotations and speakers clustering](#clustering)* : Cluster the quotations and the speakers according to the quotation vector and the added features in the data augmentation task. This clustering will be mainly used to develop a recommandation tool.\n",
    "\n",
    "## **[Generate the results for the final story](#Results)**\n",
    "\n",
    "- [General Statitics](#Statistics) : Explore the dataset, visualize some first graphs for each new features.  \n",
    "- [Country map](#Country) : Show how brexit is perceived depending on the country.\n",
    "- [Sector map](#Sector) : Show how brexit is perceived depending on the sector.\n",
    "- [Visualize speakers evolution](#2Dplot) : Visualize speakers into an embedding space that should reflect the similarities between speakers **[TO BE COMPLETED]**.\n",
    "- [Recommandation Tool](#Recommandation) : Tool that recommends similar speakers to the one searched by the user **[TO BE COMPLETED]**. \n",
    "- [Correlation with stocks](#Stocks) : Study if a correlation exists between remarkable Brexit peaks and the stock actions from companies of the FTSE100 **[TO BE COMPLETED]**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before diving into the code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run everything from scratch, make sure to have a `Data` folder containing the following files: \n",
    "- The quotebank datasets for each year: `quotes-yyyy.json.bz2`\n",
    "- The speaker attributes folder `speaker-attributes.parquet` as well as the associated lookup table `wikidata_labels_descriptions_quotebank.csv.bz2`\n",
    "\n",
    "To benefit from check points, download `Brexit_datas` from [Google drive](https://drive.google.com/drive/folders/12EgO7E97KcNrZtQhjUmkOp5iDF1V7ufR?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import useful librairies and define useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARD LIBRAIRIES\n",
    "from os.path import exists\n",
    "import bz2 \n",
    "import json\n",
    "import geojson\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from scipy import stats \n",
    "\n",
    "# Dynamic graphs\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import dash\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "# Machine learning librairies\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import SpectralClustering, DBSCAN, AgglomerativeClustering\n",
    "from sklearn import metrics\n",
    "#from sklearn.manifold import TSNE\n",
    "from tsne_torch import TorchTSNE as TSNE\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Graph algorithms\n",
    "import networkx as nx\n",
    "\n",
    "# import string distances\n",
    "import stringdist\n",
    "\n",
    "# Load the lexicon for sentiment analysis\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data files\n",
    "PATHS_TO_FILE = ['Data/quotes-20%d.json.bz2' % i for i in range(15,21)]\n",
    "\n",
    "# Columns to analyte for one-hot vectorization task\n",
    "columns_to_map = ['nationality', 'gender', 'occupation', 'party', 'academic_degree', 'religion']\n",
    "\n",
    "# time discretization by periods\n",
    "period_labels = [\"< 2018\", \"2018\", \"early 2019\", \"late 2019 - 2020\"]\n",
    "\n",
    "# type of the sentence_transformer\n",
    "sentence_transformer_type = 'all-MiniLM-L6-v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Preprocessing'></a>\n",
    "\n",
    "# Preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Sanity_check'></a>\n",
    "\n",
    "## Data exploration and Sanity check\n",
    "\n",
    "We decided to perform the following snaity checks on the original data: \n",
    "\n",
    "- We first check that each entry for each quotation is specified in the right format (e.g. `numOccurences` should be an integer).\n",
    "- We check that the `probas` sum to 1.\n",
    "- We check that the `numOccurences` is superior or equal to the length of the list containing the urls.\n",
    "- The `date` is consistent with the dataset they are coming from\n",
    "- We check that if a `qids` exists then a `speaker` should be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECK FUNCTIONS\n",
    "\n",
    "def check_type(instance,entry,dtype):\n",
    "    return type(instance[entry]) == dtype\n",
    "\n",
    "def check_probas(instance):\n",
    "    if len(instance) > 0:\n",
    "        proba_sum = sum([float(potential[1]) for potential in instance[\"probas\"]])\n",
    "        if proba_sum < 0.98 or proba_sum > 1.02:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_numOcc(instance):\n",
    "    return (len(instance[\"urls\"]) <= instance[\"numOccurrences\"])\n",
    "\n",
    "def check_date(instance,year):\n",
    "    quotation_year = int(instance[\"date\"][:4])\n",
    "    return (quotation_year == year)\n",
    "\n",
    "def check_author_qids(instance):\n",
    "    if len(instance[\"qids\"]) > 0 and instance[\"speaker\"] is None:\n",
    "        return False\n",
    "    else: \n",
    "        return True\n",
    "        \n",
    "# CONVERSION FUNCTIONS\n",
    "\n",
    "# determine the supported device\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu') # don't have GPU \n",
    "    return device\n",
    "\n",
    "# convert a df to tensor to be used in pytorch\n",
    "def df_to_tensor(df):\n",
    "    device = get_device()\n",
    "    return torch.from_numpy(df.values).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the types for each entry\n",
    "TYPES = {\"quoteID\":str,\n",
    "         \"quotation\":str,\n",
    "         \"speaker\":str,\n",
    "         \"qids\":list,\n",
    "         \"date\":str,\n",
    "         \"numOccurrences\":int,\n",
    "         \"probas\":list,\n",
    "         \"urls\":list,\n",
    "         \"phase\":str}\n",
    "\n",
    "error_file = \"Data/error_file.json.bz2\"\n",
    "\n",
    "\n",
    "if not exists(error_file):\n",
    "    with bz2.open(error_file, 'wb') as e_file:\n",
    "        # Loop over the different files that we will read\n",
    "        for quotebank_data in PATHS_TO_FILE:\n",
    "            year = int(quotebank_data[-13:-9])\n",
    "            print(\"Reading \",quotebank_data,\" file...\")\n",
    "            # Open the file we want to read\n",
    "            with bz2.open(quotebank_data, 'rb') as s_file:\n",
    "                # Loop over the samples\n",
    "                for instance in s_file:\n",
    "                    potential_error = \"\"\n",
    "                    # Loading a sample\n",
    "                    instance = json.loads(instance)\n",
    "                    #### CHECK THE TYPES ####\n",
    "                    for key, value in TYPES.items():\n",
    "                        if not check_type(instance,key,value):\n",
    "                            potential_error += \"| Type problem: \" + key + \" |\"\n",
    "                            # Continue because there exists a problem with the type that may affect the other checks\n",
    "                            continue\n",
    "                    #### CHECK THE PROBAS ####\n",
    "                    if not check_probas(instance):\n",
    "                        potential_error += \"| Probas problem |\"\n",
    "                    #### CHECK THE DATE ####\n",
    "                    if not check_date(instance,year):\n",
    "                        potential_error += \"| Date problem |\"\n",
    "                    #### CHECK THE NUMOCCURENCES ####\n",
    "                    if not check_numOcc(instance):\n",
    "                        potential_error += \"| NumOccurences problem |\"\n",
    "                    #### CHECK THE AUTHOR-QIDS ####\n",
    "                    if not check_author_qids(instance):\n",
    "                        potential_error += \"| Author-qids problem |\"\n",
    "                    # WRITE INTO THE FILE FOR POTENTIAL ERRORS #\n",
    "                    if len(potential_error) > 0:\n",
    "                        instance[\"error\"] = potential_error\n",
    "                        e_file.write((json.dumps(instance)+'\\n').encode('utf-8'))\n",
    "\n",
    "pd.read_json('Data/error_file.json.bz2',compression=\"bz2\",lines=True).shape                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='extraction'></a>\n",
    "\n",
    "## Data extraction\n",
    "\n",
    "As mentionned previously, we are planning to analyze the way Brexit is perceived. Thus, we need to extract first the quotations that discuss Brexit. To do so we will follow the following pipeline:\n",
    "\n",
    "1. Select all the quotations that contain the word Brexit.\n",
    "2. Store the new two dataset in the `Brexit_quotes.json.bz2` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists('Brexit_datas/Brexit_quotes.json.bz2'):\n",
    "    # Input file\n",
    "    PATHS_TO_FILE = ['Data/quotes-20%d.json.bz2' % i for i in range(15,21)]\n",
    "    # Output file\n",
    "    PATH_TO_OUT = 'Brexit_datas/Brexit_quotes.json.bz2'\n",
    "\n",
    "    # Open the file where we will write\n",
    "    with bz2.open(PATH_TO_OUT, 'wb') as d_file:\n",
    "        # Loop over the different files that we will read\n",
    "        for quotebank_data in PATHS_TO_FILE:\n",
    "            print(\"Reading \",quotebank_data,\" file...\")\n",
    "            # Open the file we want to read\n",
    "            with bz2.open(quotebank_data, 'rb') as s_file:\n",
    "                # Loop over the samples\n",
    "                for instance in s_file:\n",
    "                    # Loading a sample\n",
    "                    instance = json.loads(instance)\n",
    "                    # Extracting the quotation\n",
    "                    quotation = instance['quotation']\n",
    "                    # Check if the quotation contains at least one word related to Brexit\n",
    "                    if \"brexit\" in quotation.lower():\n",
    "                        # Writing in the new file\n",
    "                        d_file.write((json.dumps(instance)+'\\n').encode('utf-8'))\n",
    "\n",
    "quotebank_brexit = pd.read_json('Brexit_datas/Brexit_quotes.json.bz2',compression=\"bz2\",lines=True)\n",
    "quotebank_brexit.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='augmentation'></a>\n",
    "\n",
    "## Data augmentation\n",
    "\n",
    "When we will generate the results for the final story, we will need more information than the initial features provided. The further analysis will require to have access to other features such as the sentiment carried by the quotation and additional information about the author. To do so, the following pipeline will be performed on each quotation:\n",
    "\n",
    "1. **[Adding features related to the author](#Features_Author)** :  Using the provided file `speaker_attributes.parquet` that was extracted from the Wikidata knowledge base, the following attributes are of interest for each speaker:\n",
    "    - `occupation`: describes the author's occupancy \n",
    "    - `party` identifies the political affiliation of the speaker.\n",
    "    - `academic_degree` gives information about the education of the author as well as their alma mater.\n",
    "    - `nationality` identifies the citizenship(s) of the author.\n",
    "    - `date_of_birth`: identifies the date of birth of the speaker.\n",
    "    - `gender`: identifies the gender of the speaker.\n",
    "    - `ethnic_group`: identifies the ethnic group of the speaker.\n",
    "    - `religion`: identifies the religion of the speaker. \n",
    "\n",
    "    The provided `speaker_attributes.parquet` file contains attributes in terms of QIDs, thereby being uninterpretable by humans. To map the QIDs to meaningful labels, we used the provide the file `wikidata_labels_descriptions_quotebank.csv.bz`.\n",
    "    \n",
    "    The aforementioned attributes may not be available for all authors. When it is the case, a NaN value is assigned.\n",
    "\n",
    "2. **[Adding features issued from a sentiment analysis](#Sentiment_Quote)** : The last feature of interest is the sentiment that is carried by the quotation. For the sake of simplicity, each quotation will be classified into three different categories: *Negative*, *Neutral* and *Positive*. \n",
    "Sentiment Analysis task can be performed using pretrained Deep Neural Networks. We decided to use **Vader** Neural network for its good performance. NLTK's Vader sentiment analysis tool uses a bag of words approach with some simple heuristics. More on it [here](https://github.com/cjhutto/vaderSentiment). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Features_Author'></a>\n",
    "***Loading the speaker_attributes.parquet file***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parquet that contains the information about speakers\n",
    "df_attributes = pd.read_parquet('Data/speaker_attributes.parquet')\n",
    "\n",
    "# we are not interested in the aliases, lastrevid, US_congress_bio_ID, id, candidacy and type.\n",
    "keep_attributes = ['id','label', 'date_of_birth', 'nationality', 'gender', 'ethnic_group', 'occupation', 'party', 'academic_degree', 'religion']\n",
    "# Set the index\n",
    "df_attributes = df_attributes[keep_attributes].set_index('id')\n",
    "# Sanity check for the qids\n",
    "print(\"Sanity check ok ? : \",df_attributes.index.is_unique)\n",
    "# Let's have a look\n",
    "df_attributes.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Features_Author'></a>\n",
    "***Mapping the QIDs to meaningful labels***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionnary to use it as a lookup table \n",
    "df_map = pd.read_csv('Data/wikidata_labels_descriptions_quotebank.csv.bz2', compression='bz2', index_col='QID')\n",
    "# Dictionnary where qids are keys and values are corresponding element\n",
    "map_dict = df_map.Label.to_dict()\n",
    "\n",
    "def mapping(QIDs):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to map all the QIDs to their labels, \n",
    "    using wikidata_labels_descriptions_quotebank.csv\n",
    "    \"\"\"\n",
    "    \n",
    "    if QIDs is None:\n",
    "        return np.nan\n",
    "    else:\n",
    "        QIDs_mapped = []\n",
    "        for QID in QIDs:\n",
    "            try:\n",
    "                # If a correspondance exists\n",
    "                QIDs_mapped.append(map_dict[QID])\n",
    "            except KeyError:\n",
    "                # If no correspondance exits\n",
    "                continue\n",
    "        # If nothing was extracted\n",
    "        if len(QIDs_mapped) == 0:\n",
    "            return np.nan\n",
    "        # Things extracted\n",
    "        else:\n",
    "            return QIDs_mapped\n",
    "\n",
    "\n",
    "\n",
    "# For each column perform the mapping to transform qids to real value\n",
    "for column in columns_to_map:\n",
    "    df_attributes[column] = df_attributes[column].apply(mapping)\n",
    "    \n",
    "df_attributes.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Sentiment_Quote'></a>\n",
    "***Adding sentiment score to each quote***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_score(quote):\n",
    "    \"\"\"The purpose of this function is to use the sentiment analysis tool VADER to find the sentiment associated with a quote.\"\"\"\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sid.polarity_scores(quote)\n",
    "    \n",
    "    # The Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between\n",
    "    # -1(most extreme negative) and +1 (most extreme positive).\n",
    "    # positive sentiment : (compound score >= 0.05) \n",
    "    # neutral sentiment : (compound score > -0.05) and (compound score < 0.05) \n",
    "    # negative sentiment : (compound score <= -0.05)\n",
    "    # see https://predictivehacks.com/how-to-run-sentiment-analysis-in-python-using-vader/\n",
    "    # or https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/\n",
    "    \n",
    "    # decide sentiment as positive, negative and neutral\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        return \"Positive\"\n",
    " \n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        return \"Negative\" \n",
    " \n",
    "    else :\n",
    "        return \"Neutral\"\n",
    "\n",
    "# backup quotebank dataframe with sentiment score if the corresponding file doesn't exists\n",
    "if not exists(\"Brexit_datas/quotebank_brexit_with_sentiment.json.bz2\"):\n",
    "    quotebank_brexit['sentiment_score'] = quotebank_brexit.quotation.apply(sent_score) \n",
    "    quotebank_brexit.to_json(\"Brexit_datas/quotebank_brexit_with_sentiment.json.bz2\")\n",
    "    \n",
    "else:\n",
    "    quotebank_brexit = pd.read_json(\"Brexit_datas/quotebank_brexit_with_sentiment.json.bz2\",compression=\"bz2\")\n",
    "\n",
    "quotebank_brexit.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleaning'></a>\n",
    "\n",
    "## Data merging and cleaning\n",
    "\n",
    "Depending on the different tasks we want to perform we will need to have the dataset in various forms, thus we will generate three types of dataset: \n",
    "- `quotebank_brexit`: original dataset [+sentiment score] where dublicated quotations are removed.\n",
    "- `aug_quotebank_brexit`: dataset with augmented data where both quotations and speakers are cleaned.\n",
    "- `oneh_quotebank_brexit`: copy of `aug_quotebank_brexit` where categorical values are one-hot encoded.\n",
    "\n",
    "Thus we first start by [cleaning the quotations](#cleaning_quotation) in the `quotebank_brexit` dataset and then we [clean the speakers](#cleaning_speaker) to be able to merge with augmented data and generate the `aug_quotebank_brexit`. After a [processing](#aug_preprocessing) of the `aug_quotebank_brexit` we will finally [one hot encode](#one_hot_encoding) to generate the `oneh_quotebank_brexit` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleaning_quotation'></a>\n",
    "\n",
    "### Cleaning the quotations\n",
    "\n",
    "We noticed that some quotations were very similar, actually too similar. They sometimes differ from the fact that one quotation is nested in another or sometimes they only differ by one character. Here is an example of such a quotation:\n",
    "- quoteID: **2018-01-26-042810** - *\"I look at Nigel Farage's example. It took 17 years, but Brexit came,\"*\n",
    "- quoteID: **2018-01-26-042811** - *\"I look at Nigel Farage's example. It took 17 years, but Brexit came. I don't plan to wait that long\"*\n",
    "\n",
    "We need to remove these kind of *duplicates*. To do so we followed this pipeline:\n",
    "- Converting quotations into vectors using [SentenceTransformer](https://www.sbert.net/docs/usage/semantic_textual_similarity.html) deep neural network.\n",
    "- Computing [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between each pair of quotations\n",
    "- Removing quotations that are too similar from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode quotation \n",
    "if not exists(\"Brexit_datas/vector_quotes.csv.gz\"):\n",
    "    encoder = SentenceTransformer(sentence_transformer_type)\n",
    "    # Encode quotations\n",
    "    quotes_encoded = encoder.encode(quotebank_brexit['quotation'].values, convert_to_numpy=True, show_progress_bar=True)\n",
    "    # Convert to df\n",
    "    quotes_df = pd.DataFrame(quotes_encoded, index = quotebank_brexit.index)\n",
    "    # Add significant columns\n",
    "    quotes_df[\"speaker\"] = quotebank_brexit[\"speaker\"].values\n",
    "    # Export into a compressed format\n",
    "    quotes_df.to_csv(\"Brexit_datas/vector_quotes.csv.gz\")\n",
    "    \n",
    "else:\n",
    "    # Read the file\n",
    "    quotes_df = pd.read_csv(\"Brexit_datas/vector_quotes.csv.gz\",index_col=0,compression=\"gzip\")\n",
    "\n",
    "quotes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# compare pairwise similarity\n",
    "def filter_similar(df):\n",
    "    # Get the embeddings computed before\n",
    "    embeddings = df_to_tensor(df.drop(columns='quoteID'))\n",
    "    # Compute cosine similarity\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "    # Convert to df\n",
    "    score = pd.DataFrame(cosine_scores.numpy())\n",
    "    index = set(score[score>0.95].stack().index.tolist())\n",
    "    index = [(a,b) for (a,b) in index if a != b]\n",
    "    # multiple tuples can have common element: need to merge them\n",
    "    graph = nx.Graph(index)\n",
    "    index = list(nx.connected_components(graph))\n",
    "    # map the indices with the Qid of the quote\n",
    "    index = [tuple(df.quoteID.iloc[ind] for ind in path) for path in index]\n",
    "    return index\n",
    "\n",
    "similar_count = quotes_df.assign(quoteID=quotebank_brexit.quoteID.values).groupby('speaker').apply(filter_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of two similar quotations\n",
    "print(\"Yvette Cooper:\", similar_count[\"Yvette Cooper\"][0])\n",
    "quotebank_brexit[(quotebank_brexit.quoteID=='2019-01-27-041304') | (quotebank_brexit.quoteID=='2019-01-27-029003')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices to drop\n",
    "def drop_duplicate_quotes(ids):\n",
    "    return [[quoteID for quoteID in path[1:]] for path in ids]\n",
    "        \n",
    "# generate the list of quoteIDs to be removed\n",
    "to_be_removed = similar_count.apply(drop_duplicate_quotes).values.sum()\n",
    "to_be_removed = list(itertools.chain.from_iterable(to_be_removed))\n",
    "\n",
    "quotebank_brexit = quotebank_brexit[~quotebank_brexit.quoteID.isin(to_be_removed)]\n",
    "\n",
    "print(\"Number of quotations to be removed: \",len(to_be_removed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_quote_bank = \"Brexit_datas/quotebank_brexit_cleaned.json.bz2\"\n",
    "\n",
    "if not exists(file_quote_bank):\n",
    "    quotebank_brexit.to_json(file_quote_bank)\n",
    "else:\n",
    "    quotebank_brexit = pd.read_json(file_quote_bank,compression=\"bz2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleaning_speaker'></a>\n",
    "\n",
    "### Cleaning the speakers\n",
    "\n",
    "The `aug_quotebank_brexit` provides information about the speaker such as the `nationality`, `occupation`. However one can notice that sometimes the neural network doesn't succeed in finding a speaker and therefore fills `speaker` entry with `None` value. These missing values are difficult to handle as it would require to guess who said the quotation. One could think about training a classifier on the data where the speaker is mentionned but it is actually a fastidious task that we are not able to manage. Unfortunaltely, we decided to remove them from the dataset.\n",
    "\n",
    "An other issue comes from the fact that for one speaker different Qids exist. However, these Qids correspond to the Wikipedia pages of the same person but in different langagues. This could also come from the fact that there exist multiple wikipedia pages that point to different persons who are homonyms. When many qids exist we check if all the attributes are similar for all the qids. If not, then we are not able to determine which qid is the correct one so unfortunately we discard the row from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_consistent_qids(QIDS_original):\n",
    "    QIDS = QIDS_original.copy()\n",
    "    if len(QIDS) == 0:\n",
    "        return pd.NA\n",
    "    elif len(QIDS) == 1:\n",
    "        return QIDS_original[0]\n",
    "    else:\n",
    "        while len(QIDS) > 1:\n",
    "            first_idx = QIDS.pop(-1)\n",
    "            try:\n",
    "                first = df_attributes.loc[first_idx].fillna(0)\n",
    "                second_idx = QIDS.pop(-1)\n",
    "                try:\n",
    "                    second = df_attributes.loc[second_idx].fillna(0)\n",
    "                except KeyError:\n",
    "                    QIDS.append(first_idx)\n",
    "                    continue\n",
    "            except KeyError:\n",
    "                continue\n",
    "            try: \n",
    "                if (first != second).sum() > 0:\n",
    "                    return pd.NA\n",
    "            except ValueError:\n",
    "                return pd.NA\n",
    "        return QIDS_original[0]\n",
    "\n",
    "if not exists(\"Brexit_datas/aug_quotebank.json.bz2\"):\n",
    "    # Remove nan values\n",
    "    aug_quotebank_brexit = quotebank_brexit[quotebank_brexit.speaker != \"None\"]\n",
    "\n",
    "    # Remove speakers with multiple different qids\n",
    "    aug_quotebank_brexit.qids = aug_quotebank_brexit.qids.apply(check_consistent_qids)\n",
    "    aug_quotebank_brexit = aug_quotebank_brexit[~aug_quotebank_brexit.qids.isna()]\n",
    "\n",
    "    # Merge the augmented quotebank brexit with df_attributes on qids\n",
    "    aug_quotebank_brexit = pd.merge(aug_quotebank_brexit, df_attributes, 'inner', left_on=\"qids\", right_index=True)\n",
    "\n",
    "    # Export to json to add check points\n",
    "    aug_quotebank_brexit.to_json(\"Brexit_datas/aug_quotebank.json.bz2\")\n",
    "else:\n",
    "    # Read json if it already exists\n",
    "    aug_quotebank_brexit = pd.read_json(\"Brexit_datas/aug_quotebank.json.bz2\",compression=\"bz2\")\n",
    "\n",
    "# Let's have a look\n",
    "print(\"New shape:\",aug_quotebank_brexit.shape)\n",
    "aug_quotebank_brexit.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute age feature for each speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age(birth_date,current_year=datetime.now().year):\n",
    "    \n",
    "    # Check it is a list \n",
    "    if isinstance(birth_date,list) and len(birth_date) > 0:\n",
    "        # Only get the first 4 digit (year)\n",
    "        birth_year = int(birth_date[0][1:5])\n",
    "        # Return the age\n",
    "        return current_year - birth_year\n",
    "    else:\n",
    "        return pd.NA\n",
    "\n",
    "# Add the age column\n",
    "aug_quotebank_brexit[\"Age\"] = aug_quotebank_brexit.date_of_birth.apply(get_age)\n",
    "\n",
    "aug_quotebank_brexit.loc[:,[\"Age\",\"date_of_birth\",\"speaker\"]].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get unique values of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {}\n",
    "\n",
    "# For each categotical value\n",
    "for col in columns_to_map:\n",
    "    # Get the serie\n",
    "    col_serie = aug_quotebank_brexit[col].copy()\n",
    "    # Get unique values\n",
    "    unique_values[col] = pd.unique(col_serie.apply(pd.Series).stack())\n",
    "    print(col,\" : number of different categories = \",len(unique_values[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aug_preprocessing'></a>\n",
    "\n",
    "### What about the categorical features added\n",
    "\n",
    "### Occupation feature\n",
    "\n",
    "Now that we added new features, we had a look at their values. We noticed that there are more than 800 different occupations. It would be interesting to classify them into *categories*. The problem is that we do not have any label on them and using ML techniques such as pre-trained neural networks would be an over-kill. We rather followed a semi-manual approach that is described below: \n",
    "- Identify which words are the more frequent in the `occupation` names and associate them with a label. We will call them key words.\n",
    "- For each `occupation` match it with any key word labels when applicable.\n",
    "- Label the remaining occupations manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Manage frequent keywords***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame of the occupations\n",
    "occupation_df = pd.DataFrame(unique_values[\"occupation\"],columns=[\"occupation\"])\n",
    "\n",
    "key_words = []\n",
    "\n",
    "# Loop over the occupations\n",
    "for occupation in unique_values[\"occupation\"]:\n",
    "    # Split the occupation string and concatenate\n",
    "    key_words += occupation.split()\n",
    "\n",
    "# Convert to a Dataframe\n",
    "key_words_df = pd.DataFrame(key_words,columns=[\"occupation\"])\n",
    "# Put all strings to lower\n",
    "key_words_df.occupation = key_words_df.occupation.str.lower()\n",
    "# For each key word count the number of occurences and sort by descending\n",
    "key_words_df = key_words_df.groupby(\"occupation\").size().reset_index(name=\"Count\").sort_values(by=\"Count\",ascending=False)\n",
    "\n",
    "# If the classification has not been already done\n",
    "if not exists(\"Brexit_datas/occupation_class/occupation_agg.csv\"):\n",
    "    key_words_df.to_csv(\"Brexit_datas/occupation_class/occupation_agg.csv\")\n",
    "\n",
    "print(\"Look at the most frequent keywords\")\n",
    "print(key_words_df.head(3))\n",
    "\n",
    "answer = input(\"Is the classification of keywords done ?\")\n",
    "\n",
    "if (answer.lower() == \"yes\"):\n",
    "    # Get the classified keywords\n",
    "    key_words_classified = pd.read_csv(\"Brexit_datas/occupation_class/occupation_agg.csv\",index_col=0)\n",
    "    # Get ride of keywords that have not been classified\n",
    "    key_words_classified = key_words_classified.loc[~key_words_classified.Category.isna()]\n",
    "    # Manage the case when several categories have been entered\n",
    "    key_words_classified.Category = key_words_classified.Category.apply(lambda x: x.split(\"-\"))\n",
    "    # let's have a look at the table\n",
    "    print(\"Look at the output table\")\n",
    "    print(key_words_classified.head(3))\n",
    "else:\n",
    "    print(\"Then please classify the keywords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Match occupation and keyword labels***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if keywords are contained in an occupation\n",
    "def check_string_in(occupation):\n",
    "    # Initialize the final list of the supercategories\n",
    "    final_list = []\n",
    "    # Loop over the key_words_classified\n",
    "    for items in key_words_classified.occupation.iteritems():\n",
    "        # If the keyword is contained in the occupation\n",
    "        if items[1] in occupation.lower():\n",
    "            # Concat the supercategories with th existing list\n",
    "            final_list = final_list + key_words_classified.loc[items[0],\"Category\"]\n",
    "    # If no categories return NaN\n",
    "    if len(final_list) == 0:\n",
    "        return pd.NA\n",
    "    # Else return the list without duplicates\n",
    "    else:\n",
    "        return list(set(final_list))\n",
    "        \n",
    "# Apply the function\n",
    "occupation_df[\"Category\"] = occupation_df.occupation.apply(check_string_in)\n",
    "\n",
    "if not exists(\"Brexit_datas/occupation_class/unclassified_occupation.csv\"):\n",
    "    # Export the occupations that have not been classified\n",
    "    occupation_df[occupation_df.Category.isna()].to_csv(\"Brexit_datas/occupation_class/unclassified_occupation.csv\")\n",
    "\n",
    "print(\"Look at the remaining occupations\")\n",
    "print(occupation_df[occupation_df.Category.isna()].head(3))\n",
    "\n",
    "answer = input(\"Is the classification of remaining occupations done ?\")\n",
    "\n",
    "if (answer.lower() == \"yes\"):\n",
    "    # Get the remaining occupations classified\n",
    "    remain_occupations_classified = pd.read_csv(\"Brexit_datas/occupation_class/unclassified_occupation.csv\",index_col=0)\n",
    "    # Merge with the current data frame\n",
    "    occupation_final_df = pd.merge(occupation_df,remain_occupations_classified,how=\"left\",on=\"occupation\",suffixes=(\"\",\"_2\"))\n",
    "    # Split into a list\n",
    "    occupation_final_df.Category_2 = occupation_final_df.Category_2.apply(lambda x: x.split(\"-\") if type(x) == str else pd.NA)\n",
    "    # Merge into a single column\n",
    "    occupation_final_df.loc[~occupation_final_df.Category_2.isna(),\"Category\"] = occupation_final_df.loc[~occupation_final_df.Category_2.isna(),\"Category_2\"]\n",
    "    # Drop the artificial column\n",
    "    occupation_final_df.drop(columns=[\"Category_2\"],inplace=True)\n",
    "    # Drop na values that corresponds to unclassifiable jobs such as nazi hunter\n",
    "    occupation_final_df.dropna(axis=0,inplace=True)\n",
    "    # Let's have a look\n",
    "    print(\"Final data set for the classification of occupations:\")\n",
    "    print(occupation_final_df.head(5))\n",
    "    # Export to a json file\n",
    "    if not exists(\"Brexit_datas/occupation_class/classified_occupation.json\"):\n",
    "        occupation_final_df.to_json(\"Brexit_datas/occupation_class/classified_occupation.json\")\n",
    "else:\n",
    "    print(\"Then please classify the remaining occupations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Label remaining occupations***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupation_final_df = pd.read_json(\"Brexit_datas/occupation_class/classified_occupation.json\").set_index(\"occupation\")\n",
    "\n",
    "# Let's have a look at the supercategories\n",
    "print(list(pd.unique(occupation_final_df.Category.apply(pd.Series).stack())))\n",
    "\n",
    "# Let's replace this into the aug_quotebank dataset\n",
    "def replace_occupation(occupation):\n",
    "    if type(occupation) == list:\n",
    "        if len(occupation) > 0:\n",
    "            new_occupation = []\n",
    "            for job in occupation:\n",
    "                try:\n",
    "                    new_occupation += occupation_final_df.loc[job,\"Category\"]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            if len(new_occupation) > 0:\n",
    "                return list(set(new_occupation))\n",
    "            else:\n",
    "                return pd.NA\n",
    "                \n",
    "    else:\n",
    "        return pd.NA\n",
    "\n",
    "aug_quotebank_brexit.occupation = aug_quotebank_brexit.occupation.apply(replace_occupation)\n",
    "aug_quotebank_brexit.head(2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_levensthein(country,proposal=5):\n",
    "\n",
    "    # Given the initial country, should we deviate from this initial one ?\n",
    "    message = country + \" Any deviation ?\"\n",
    "    deviation = input(message)\n",
    "\n",
    "    # If a deviation was specified then replace\n",
    "    if len(deviation) > 0:\n",
    "        country = deviation\n",
    "    \n",
    "    # Compute the levenshtein distance with the normalized country\n",
    "    value = []\n",
    "    for existing in list(current_countries.Country):\n",
    "        leven_distance = stringdist.levenshtein_norm(country,existing)\n",
    "        value.append(leven_distance)\n",
    "        \n",
    "    value = np.array(value)\n",
    "    # Sort the countries according to their closeness\n",
    "    closer = current_countries.Country.values[np.argsort(value)]\n",
    "    # Get the \"proposal\" closest countries\n",
    "    closer = closer[:proposal]\n",
    "\n",
    "    # Display potential countries\n",
    "    message = country + \" potential candidates \\n\" + \" --- \".join(list(closer))\n",
    "\n",
    "    fine = True\n",
    "    while fine:\n",
    "        try:\n",
    "            # Give the index of the potential country that will replace the initial country\n",
    "            idx_to_keep = int(input(message))\n",
    "            fine = False\n",
    "        except ValueError:\n",
    "            print(\"Please specify an integer\")\n",
    "            fine = True\n",
    "\n",
    "    # If negative index then discard\n",
    "    if idx_to_keep < 0:\n",
    "        return pd.NA\n",
    "    # Else return the closer one\n",
    "    else:\n",
    "        return closer[idx_to_keep]\n",
    "\n",
    "\n",
    "if not exists(\"Brexit_datas/country_class/country_final_mapping.csv\"):\n",
    "\n",
    "    # Get the list of existing countries \n",
    "    current_countries = pd.read_excel(\"Brexit_datas/country_class/countries.xlsx\")\n",
    "\n",
    "    # Remove capital letters and special characters\n",
    "    current_countries.Country = current_countries.Country.str.lower()\n",
    "    current_countries.Country = current_countries.Country.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "\n",
    "    # Country values that we currently have\n",
    "    countries_to_map = pd.DataFrame(unique_values[\"nationality\"],columns=[\"Country\"])\n",
    "    # Remove capital letters and special characters\n",
    "    countries_to_map.Country = countries_to_map.Country.str.lower()\n",
    "    countries_to_map.Country = countries_to_map.Country.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    # Drop eventual duplicates\n",
    "    countries_to_map.drop_duplicates(subset=[\"Country\"],inplace=True)\n",
    "\n",
    "    # Let's perform a first merge\n",
    "    countries_to_map = pd.merge(current_countries,countries_to_map,left_on=\"Country\",right_on=\"Country\",how=\"right\")\n",
    "\n",
    "    # New column new countries\n",
    "    countries_to_map.loc[~countries_to_map.ISO.isna(),[\"real_country\"]] = countries_to_map[~countries_to_map.ISO.isna()].Country\n",
    "    # Fill the remaining countries manually\n",
    "    countries_to_map[countries_to_map.ISO.isna()].real_country= countries_to_map[countries_to_map.ISO.isna()].Country.apply(compare_levensthein)\n",
    "    # Remove countries that didn't find a correspondance\n",
    "    countries_to_map = countries_to_map[~countries_to_map.real_country.isna()].drop(columns=\"ISO\")\n",
    "    # Export the result\n",
    "    countries_to_map.to_csv(\"Brexit_datas/country_class/country_final_mapping.csv\",index_col=0)\n",
    "\n",
    "else:\n",
    "    # Read the already created csv\n",
    "    countries_to_map = pd.read_csv(\"Brexit_datas/country_class/country_final_mapping.csv\",index_col=0)\n",
    "\n",
    "# Set the index \n",
    "countries_to_map.set_index(\"Country\",inplace=True)\n",
    "countries_to_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_category(initial,lookup_table):\n",
    "\n",
    "    # Initialize the potential new_values\n",
    "    new_values = []\n",
    "\n",
    "    # Get the original values\n",
    "    original = list(lookup_table.index.values)\n",
    "    # Check that the element is a list\n",
    "    if isinstance(initial,list):\n",
    "        # For each word in the list\n",
    "        for old_original in initial:\n",
    "            # Check if there exists a corresponding word\n",
    "            if old_original.lower() in original:\n",
    "                # Then get the new value\n",
    "                new_values.append(lookup_table.loc[old_original.lower(),lookup_table.columns[0]])\n",
    "        if len(new_values) == 0:\n",
    "            return pd.NA\n",
    "        else:\n",
    "            return new_values\n",
    "    else:\n",
    "        return pd.NA\n",
    "\n",
    "# Replace initial country by normalized countries\n",
    "aug_quotebank_brexit.loc[:,\"nationality\"] = aug_quotebank_brexit.nationality.apply(match_category,lookup_table=countries_to_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding to remove categories with low number of occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_maps = {}\n",
    "\n",
    "# define threshold for each target column\n",
    "threshold_maps[\"nationality\"] = 80\n",
    "threshold_maps[\"party\"] = 50\n",
    "threshold_maps[\"academic_degree\"] = 20\n",
    "threshold_maps[\"religion\"] = 50\n",
    "threshold_maps[\"gender\"] = 50\n",
    "\n",
    "# function filtering \n",
    "def intersection_test(xlist, unique_set):\n",
    "    if not isinstance(xlist, list):\n",
    "        return pd.NA\n",
    "    if len(xlist) == 0:\n",
    "        return pd.NA\n",
    "    new_list = list(set(xlist).intersection(unique_set))\n",
    "    if len(new_list) > 0:\n",
    "        return new_list\n",
    "    else:\n",
    "        return pd.NA\n",
    "\n",
    "\n",
    "for col, threshold in threshold_maps.items():\n",
    "    print(\"Column\", col, \", Keep only values with occurrence >\", threshold)\n",
    "    # get unique quantities inside list objects and their count\n",
    "    unique_count = aug_quotebank_brexit[col].apply(pd.Series).stack().to_frame().rename(columns={0:\"Value\"})\n",
    "    unique_count = unique_count.groupby(\"Value\").size().reset_index(name=\"Count\").sort_values(by=\"Count\",ascending=False)\n",
    "    # define intersection for the specific unique values\n",
    "    intersection_unique = lambda xlist: intersection_test(xlist, unique_count[unique_count[\"Count\"] > threshold].Value.values)\n",
    "    # apply filtering\n",
    "    aug_quotebank_brexit[col] = aug_quotebank_brexit[col].apply(intersection_unique)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Academic degree gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to gather academic degree of people into a higher categories such as PhD, professor, master bachelor and so on. As the number of different remaining categories for the academic degree is quite low, it can be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File where we will match higher categories\n",
    "file_academic = \"Brexit_datas/academic_degree/original_academic.csv\"\n",
    "\n",
    "if not exists(file_academic):\n",
    "    # Get the unique values for academic\n",
    "    Academic_filter = aug_quotebank_brexit[\"academic_degree\"].apply(pd.Series).stack().to_frame().rename(columns={0:\"Value\"})\n",
    "    # Group by value, count and export\n",
    "    Academic_filter.groupby(\"Value\").count().to_csv(file_academic)\n",
    "\n",
    "# Please fill the file \n",
    "answer = input(\"Did you gather academic degrees categories into higher categories ?\")\n",
    "\n",
    "if answer.lower() == \"yes\":\n",
    "    # Read the updated matching file \n",
    "    Academic_filter = pd.read_csv(file_academic).set_index(\"Value\")\n",
    "    # Remove capital letters\n",
    "    Academic_filter.index = Academic_filter.index.str.lower()\n",
    "    # match category\n",
    "    aug_quotebank_brexit.loc[:,\"academic_degree\"] = aug_quotebank_brexit.academic_degree.apply(match_category,lookup_table=Academic_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We want to remove useless information: for instance \n",
    "# someone with bachelor and Phd should be replaced by Phd only\n",
    "def acadedmic_order(degrees,order):\n",
    "    \n",
    "    # Initialize max and argmax\n",
    "    max = -1\n",
    "    new = None\n",
    "\n",
    "    if isinstance(degrees,list):\n",
    "        # Loop over its potential degrees\n",
    "        for degree in degrees:\n",
    "            # Replace if higher degree \n",
    "            if max < order[degree]:\n",
    "                max = order[degree]\n",
    "                new = degree\n",
    "        if new is None:\n",
    "            print(\"Empty list\")\n",
    "            return pd.NA\n",
    "        return [new]\n",
    "    return pd.NA\n",
    "\n",
    "# Define the order between degrees \n",
    "Ordered_list = {\"Professor\":4,\"Phd\":3,\"Master\":2,\"Bachelor\":1,\"Other\":0}\n",
    "\n",
    "# Apply that man :)\n",
    "aug_quotebank_brexit.loc[:,\"academic_degree\"] = aug_quotebank_brexit.academic_degree.apply(acadedmic_order,order=Ordered_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Religion gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to actually do the same thing for the religion feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_religion = \"Brexit_datas/religion/original_religion.csv\"\n",
    "\n",
    "if not exists(file_religion):\n",
    "    Religion_filter = aug_quotebank_brexit[\"religion\"].apply(pd.Series).stack().to_frame().rename(columns={0:\"Value\"})\n",
    "    Religion_filter.groupby(\"Value\").count().to_csv(file_religion)\n",
    "\n",
    "answer = input(\"Did you gather religion categories into higher categories ?\")\n",
    "\n",
    "if answer.lower() == \"yes\":\n",
    "    Religion_filter = pd.read_csv(file_religion).set_index(\"Value\")\n",
    "    Religion_filter.index = Religion_filter.index.str.lower()\n",
    "    Religion_filter.head()\n",
    "    aug_quotebank_brexit.loc[:,\"religion\"] = aug_quotebank_brexit.religion.apply(match_category,lookup_table=Religion_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate period for each quotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_aug = \"Brexit_datas/aug_quotebank_brexit_true.json.bz2\"\n",
    "\n",
    "if not exists(file_aug):\n",
    "    aug_quotebank_brexit.to_json(file_aug)\n",
    "else:\n",
    "    aug_quotebank_brexit = pd.read_json(file_aug,compression=\"bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define period cut (6 = 1 year, 12 = 6 months)\n",
    "periods_nb = 12\n",
    "\n",
    "# Visualize with an histogram on the date column\n",
    "fig = px.histogram(aug_quotebank_brexit,x=\"date\", nbins=12)#, color_discrete_sequence = px.colors.cyclical.Phase)\n",
    "fig.update_layout(title=\"Number of quotations about Brexit accross time\")\n",
    "fig.show()\n",
    "\n",
    "# Get period cut\n",
    "period_list = list(range(periods_nb - 1))\n",
    "period_cut = pd.date_range('2015-01-01', freq='6M', periods=periods_nb)\n",
    "\n",
    "aug_quotebank_brexit[\"period\"] = pd.cut(aug_quotebank_brexit.date.astype(np.int64)//10**9,\n",
    "                   bins=period_cut.astype(np.int64)//10**9,\n",
    "                   labels=period_list)\n",
    "\n",
    "aug_quotebank_brexit.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually select and eventually merge periods in order to have almost a equal distribution between periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually merge periods (WARNING: it depends on periods )\n",
    "\n",
    "def reset_periods(x):\n",
    "    if x <= 5:\n",
    "        return period_labels[0]\n",
    "    elif x in [6, 7]:\n",
    "        return period_labels[1]\n",
    "    elif x == 8:\n",
    "        return period_labels[2]\n",
    "    else:\n",
    "        return period_labels[3]\n",
    "    \n",
    "if (type(aug_quotebank_brexit[\"period\"].dtypes) is pd.core.dtypes.dtypes.CategoricalDtype):\n",
    "    # print counts\n",
    "    print(\"Count occurrences for each periods, they must correspond to the histogram above.\")\n",
    "    for i in period_list:\n",
    "        print(\"Size of period\", i, \":\", len(aug_quotebank_brexit.loc[aug_quotebank_brexit.period == i]))\n",
    "\n",
    "    # apply period simplification\n",
    "    aug_quotebank_brexit[\"period\"] = aug_quotebank_brexit[\"period\"].apply(reset_periods)\n",
    "\n",
    "\n",
    "print(\"Check the occurrences after the period recalibration\")\n",
    "for i in period_labels:\n",
    "    print(\"Size of period\", i, \":\", len(aug_quotebank_brexit.loc[aug_quotebank_brexit.period == i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='one_hot_encoding'></a>\n",
    "\n",
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {}\n",
    "# One hot vectorization of columns cotaining categorical values\n",
    "dummy_col = \"AAADummy column for the sake\"\n",
    "# Make a copy\n",
    "oneh_quotebank_brexit = aug_quotebank_brexit.copy()\n",
    "\n",
    "# Check that the element is a list that contains only one string\n",
    "def ensure_list(value):\n",
    "  if isinstance(value, list):\n",
    "    for i in range(len(value)):\n",
    "      value[i] = str(value[i])\n",
    "  elif not pd.isna(value):\n",
    "    value = [value]\n",
    "  return value\n",
    "\n",
    "# Loop over categorical columns\n",
    "for col in columns_to_map:\n",
    "  # Get the serie\n",
    "  col_serie = aug_quotebank_brexit[col].copy().apply(ensure_list)\n",
    "  # Change nan values to a list containing a dummy column\n",
    "  col_serie[col_serie.isna()] = col_serie[col_serie.isna()].apply(lambda x: [dummy_col])\n",
    "    \n",
    "  # One hot vectorize\n",
    "  categorical_df = pd.get_dummies(col_serie.apply(pd.Series).stack()).groupby(level=0).sum()\n",
    "    \n",
    "  # Drop the dummy column\n",
    "  categorical_df.drop(columns=[dummy_col],inplace=True)\n",
    "  # Refresh unique values\n",
    "  unique_values[col] = categorical_df.columns\n",
    "    \n",
    "  # Join with quotebank brexit\n",
    "  oneh_quotebank_brexit = oneh_quotebank_brexit.join(categorical_df,how=\"left\",rsuffix=col[:3])\n",
    "  print(\"One hot vectorizing : \",col,\n",
    "        \"| NaN values : \",categorical_df.isna().apply(lambda x: x*1).sum().sum(),\n",
    "        \"| Number of different categories : \",len(categorical_df.columns),\n",
    "        \"| Shape reduced ? \",categorical_df.shape,oneh_quotebank_brexit.shape)\n",
    "  # Drop the categorical column\n",
    "  oneh_quotebank_brexit.drop(columns=col,inplace=True)\n",
    "  # Check for NaN values\n",
    "  print(\"Any NA in the final dataframe: \",oneh_quotebank_brexit.isna().apply(lambda x: x*1).sum().sum())\n",
    "\n",
    "print(\"Shape of the final data frame\",oneh_quotebank_brexit.shape)\n",
    "print(\"Any NA in the final dataframe: \",oneh_quotebank_brexit.isna().apply(lambda x: x*1).sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is better to remove it at the end of the work\n",
    "\n",
    "file_onehot = \"Brexit_datas/one_hot_quotebank_brexit.json.bz2\"\n",
    "\n",
    "if not exists(file_onehot):\n",
    "    oneh_quotebank_brexit.to_json(file_onehot)\n",
    "else:\n",
    "    oneh_quotebank_brexit = pd.read_json(file_onehot,compression=\"bz2\")\n",
    "    \n",
    "oneh_quotebank_brexit.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clustering_task'></a>\n",
    "\n",
    "# Quotations and speakers clustering\n",
    "\n",
    "The last preprocessing step consists of clustering the quotations as well as the speakers. This clustering will later be used to create a Recommandation Tool in the context of Brexit. Quotations and speakers that carry similar attributes/ideas will belong to the same cluster. Performing such task can be performed using the following pipeline:\n",
    "1. The first step is to convert sentences into vectors. This task can be achieved using the [SentenceTransformer](https://www.sbert.net/docs/usage/semantic_textual_similarity.html) deep neural network. The vector obtained from this operation cab be then concatenated with the other existing features (that would be converted to one hot vectors if necessary) (ALREADY DONE).\n",
    "2. The second step consists in reducing the dimension of the data before applying the clustering algorithm. This task can be achieved using the [Locally Linear Embeddings](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding) algorithm. This algorithm is considered to be an efficient non-linear dimensionality reduction method.\n",
    "3. The third step is specific to speaker clustering. Indeed, the vectorization of quotes as well as the reduction of dimensionality are only applied to quotes. Thus, we need to perform an **aggregation** to be able to attribute a vector to each speaker. For each speaker, this aggregation can simply be done by taking the mean of the vectors associated with each of their quotations. \n",
    "4. The last step consists in performing the clustering operation. This task can be achieved using [Spectral Clustering](#https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering) method.\n",
    "\n",
    "### Sentiment amplification\n",
    "\n",
    "In order to calibrate the importance of the *sentiment score* in the quotes vector space, a coefficient of amplification `sentiment_amplification` is applied to the integer coded as `sentiment_score`.\n",
    "\n",
    "This process aims at increasing the distance among the vectors grouping vectors with the same sentiment score. A priori, this boosts the labelling process.\n",
    "\n",
    "### Locally linear embedding\n",
    "\n",
    "This algorithm aims at preserving the neighbouring points. The process is described as follows: \n",
    "- For each point, its nearest neighbors are determined. \n",
    "- Then it tries to project the new point in the embedded space such that its neighbors are preserved\n",
    "This spectral dimensionality reduction technique is non-linear, fast and reliable enough to handle big and complex dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    User-defined parameters for the task\n",
    "\"\"\"\n",
    "# coefficient of amplification of sentiment\n",
    "# auto = give sentiment the same weight as the other quantities combined together\n",
    "sentiment_amplification = 'auto'\n",
    "oneh_amplification = 1.0\n",
    "\n",
    "# sentence-tranformer improved encoder\n",
    "sentence_tranformer_improved = 'multi-qa-mpnet-base-dot-v1'\n",
    "#sentence_tranformer_improved = 'msmarco-MiniLM-L6-cos-v5'\n",
    "\n",
    "# define clustering costumized model\n",
    "\n",
    "class EmbeddedSpectralClustering(SpectralClustering):\n",
    "    def __init__(self, n_clust, embedder = LocallyLinearEmbedding(n_components=100, n_neighbors=75, max_iter=1000, method='standard', eigen_solver='arpack', n_jobs = 4)):\n",
    "        super().__init__(n_clust, n_jobs=4, eigen_solver='lobpcg', affinity='nearest_neighbors')\n",
    "        self.embedder = embedder\n",
    "        \n",
    "    def fit(self, samples):\n",
    "        emb_samples = self.embedder.fit_transform(df_to_tensor(samples))\n",
    "        print(\"Embedder reconstruction error: \", self.embedder.reconstruction_error_)\n",
    "        return super().fit(emb_samples)\n",
    "    \n",
    "#clustering_model = lambda n: EmbeddedSpectralClustering(n, TSNE(n_components=3, perplexity=30, n_iter=1000, verbose=True))\n",
    "#clustering_model = EmbeddedSpectralClustering \n",
    "#clustering_model = AgglomerativeClustering \n",
    "clustering_model = lambda n: SpectralClustering(n, n_jobs=4, eigen_solver='lobpcg', affinity='nearest_neighbors', assign_labels='discretize')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataframe for clustering (keep only involved columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_to_int(value):\n",
    "  return int(value == \"Positive\") - int(value == \"Negative\")\n",
    "\n",
    "def standardize(df):\n",
    "    return (df - df.mean(axis=0)) / df.std(axis=0)\n",
    "\n",
    "  \n",
    "columns_to_drop = [\"Age\", \"date\", \"quoteID\",\"qids\",\"phase\",\"probas\",\"urls\",\"date_of_birth\",\"label\"]\n",
    "\n",
    "# restrict dataframe to the one needed\n",
    "oneh_cluster_df = oneh_quotebank_brexit.drop(columns=columns_to_drop)\n",
    "\n",
    "# amplify one-hot parameters\n",
    "oneh_cluster_df[oneh_cluster_df.select_dtypes(include=['number']).columns] *= oneh_amplification\n",
    "\n",
    "#print(\"Is there any na values ?\",oneh_cluster_df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='One-hot'></a>\n",
    "\n",
    "## Compose the vectorized dataframe\n",
    "\n",
    "1. Convert sentiment score into signed integer format: \"Positive\" = 1, \"Negative\" = -1, \"Neutral\" = 0\n",
    "2. For each column concerning the speaker information, generate a dummy dataframe (see DataFrame.get_dummy)\n",
    "3. Concatenate along columns all obtained dataframe\n",
    "4. Average all rows matching the same speaker (which is set as index)\n",
    "5. Normalize dataset by row and amplify sentiment_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the filtered dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode quotation if necessary\n",
    "aug_vector_quotes_file = \"Brexit_datas/aug_vector_quotes_%s.csv.gz\" % sentence_tranformer_improved\n",
    "\n",
    "if not exists(aug_vector_quotes_file):\n",
    "    encoder = SentenceTransformer(sentence_tranformer_improved)\n",
    "    # Encode quotations\n",
    "    quotes_encoded = encoder.encode(aug_quotebank_brexit['quotation'].values, convert_to_numpy=True, show_progress_bar=True)\n",
    "    # Convert to df\n",
    "    quotes_df = pd.DataFrame(quotes_encoded, index = aug_quotebank_brexit.index)\n",
    "    # Add significant columns\n",
    "    quotes_df[\"speaker\"] = aug_quotebank_brexit[\"speaker\"].values\n",
    "    # Export into a compressed format\n",
    "    quotes_df.to_csv(aug_vector_quotes_file)\n",
    "    \n",
    "else:\n",
    "    # Read the file\n",
    "    quotes_df = pd.read_csv(aug_vector_quotes_file,index_col=0,compression=\"gzip\")\n",
    "\n",
    "\n",
    "quotes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compose a vectorized dataframe\n",
    "\n",
    "Merge `oneh_cluster_df` with `quotes_df` and take the average over all quotations belonging to the same *speaker* and *period*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Compose a vectorized dataframe\n",
    "\"\"\"\n",
    "\n",
    "# normalize quotations vector\n",
    "if 'speaker' in quotes_df.columns:\n",
    "    quotes_df.drop(columns=['speaker'], inplace=True) # speaker is not needed, already contained in oneh_cluster_df\n",
    "quotes_df = standardize(quotes_df)\n",
    "\n",
    "# Merge the two data frames\n",
    "cluster_full_df = pd.concat([oneh_cluster_df.drop(columns=\"quotation\"), quotes_df.loc[oneh_cluster_df.index]],axis = 1)\n",
    "#cluster_full_df = pd.concat([oneh_cluster_df.loc[:,['speaker', 'period']], quotes_df],axis = 1)\n",
    "\n",
    "# convert sentiment_score to int format\n",
    "cluster_full_df['sentiment_score'] = cluster_full_df['sentiment_score'].apply(sentiment_to_int)\n",
    "\n",
    "# average over the same speaker and period\n",
    "# treat each period differently\n",
    "cluster_full_df = cluster_full_df.groupby(['speaker', 'period']).agg(np.mean)\n",
    "cluster_full_df.set_index('numOccurrences', append=True, drop=True, inplace=True)\n",
    "\n",
    "# normalize entire dataset\n",
    "cluster_full_df = standardize(cluster_full_df)\n",
    "\n",
    "# amplify sentiment\n",
    "# determine sentiment amplification if automatic\n",
    "#if sentiment_amplification == 'auto':\n",
    "#    sentiment_amplification = cluster_full_df.drop(columns = ['sentiment_score']).sum(axis=1).mean(axis=0)\n",
    "#    print(\"Automatically determined sentiment score amplification:\", sentiment_amplification)\n",
    "    \n",
    "#cluster_full_df['sentiment_score'] *= sentiment_amplification\n",
    "\n",
    "#print(\"Is there any na values ?\",cluster_full_df.isna().sum().sum())\n",
    "cluster_full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TSNE'></a>\n",
    "\n",
    "## First analysis aggregation per time period\n",
    "   - Group by period\n",
    "   - Apply Locally Linear Embedding aggregation and visualize\n",
    "   - Detect outliers with a DBSCAN clustering and filter them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply T-stochastic neighboor embedding\n",
    "# NOT USED: one-hot vectorization gave undefined results\n",
    "#data_np_emb = TSNE(n_components=2, perplexity=30, n_iter=1000, verbose=True).fit_transform(full_data_tensor) # dim = Nxfinal_dim\n",
    "\n",
    "fig,axes = plt.subplots(len(period_labels)//2,2, figsize=[15,8])\n",
    "\n",
    "# copy the dataframe and create outlier column\n",
    "cluster_filtered_df = pd.DataFrame(columns=cluster_full_df.columns)\n",
    "\n",
    "for ax, period in zip(axes.flatten()[0:len(period_labels)], period_labels[::-1]):\n",
    "    \n",
    "    # restrict by period\n",
    "    period_cluster_df = cluster_full_df.xs(period, level=1)\n",
    "    \n",
    "    # embed data\n",
    "    print(\"Size of the data which has been embedded: \", period_cluster_df.shape, \", period: \", period)\n",
    "    embedder = LocallyLinearEmbedding(n_components=2, n_neighbors=35, max_iter=500, method='standard', eigen_solver='arpack')\n",
    "    #embedder = TSNE(n_components=2, perplexity=10, n_iter=1000, verbose=True)\n",
    "    data_np_emb = embedder.fit_transform(period_cluster_df).transpose()\n",
    "    print(\"Embedded done, estimated reconstruction error: \", embedder.reconstruction_error_)\n",
    "    \n",
    "    # standardize X,Y distribution for embedded data\n",
    "    std = standardize(data_np_emb.transpose())\n",
    "    \n",
    "    # apply DBSCAN clustering and detect outliers\n",
    "    print(\"\\nClustering 2D sample\")\n",
    "    dbscanner = DBSCAN(eps = 1.5, min_samples=10).fit(std)\n",
    "    print(\"Outliers found: \", len(std[dbscanner.labels_ == -1]))\n",
    "    \n",
    "    # plot \n",
    "    ax.set_title('Period: %s' % period)\n",
    "    #ax.tick_params(left=False,bottom=False,labelleft=False,labelbottom=False) \n",
    "    \n",
    "    for label in set(dbscanner.labels_):\n",
    "        points = std[dbscanner.labels_ == label].transpose()\n",
    "        label = \"Label %d\" % label if label != -1 else \"Outliers\"\n",
    "        ax.scatter(points[0], points[1], label=label)\n",
    "    \n",
    "    ax.legend()\n",
    "        \n",
    "    # remove outliers\n",
    "    period_cluster_df.drop((period_cluster_df[dbscanner.labels_ == -1]).index, inplace=True)\n",
    "    \n",
    "    # correct index and append to new set \n",
    "    period_cluster_df['period'] = period\n",
    "    period_cluster_df.set_index('period', drop=True, append=True, inplace=True)\n",
    "\n",
    "    cluster_filtered_df = pd.concat([period_cluster_df, cluster_filtered_df], verify_integrity=True, copy=False)\n",
    "   \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Clustering'></a>\n",
    "\n",
    "## Clustering classification\n",
    "    - Embed data into a three dimensional space\n",
    "    - Checking silhouette score, find optimal number of clusters\n",
    "    - Finally, apply clustering with optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def threedim_embedding(period_df, perplexity, iters):\n",
    "    out_tensor = LocallyLinearEmbedding(n_components=3, n_neighbors=25, max_iter=1000, method='standard', eigen_solver='arpack').fit_transform(period_df).transpose()\n",
    "    #out_tensor = TSNE(n_components=3, perplexity=perplexity, n_iter=iters, verbose=True).fit_transform(df_to_tensor(period_df)).transpose()\n",
    "    \n",
    "    period_df['x'] = out_tensor[0]\n",
    "    period_df['y'] = out_tensor[1]\n",
    "    period_df['z'] = out_tensor[2]\n",
    "    \n",
    "    return period_df.loc[:,['x', 'y', 'z']]\n",
    "\n",
    "tsne_df = cluster_filtered_df.groupby(level=2).apply(threedim_embedding, 40, 1500)\n",
    "tsne_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters to be identified \n",
    "nb_clusters = range(6,30)\n",
    "\n",
    "cluster_scores = pd.DataFrame()\n",
    "\n",
    "# TODO: find a better estimator for clustering score and cross validate\n",
    "\n",
    "for period in period_labels:\n",
    "    \n",
    "    period_df = tsne_df.xs(period, level=2)\n",
    "    \n",
    "    # Cross validate clustering and check silhouette score\n",
    "    #cv_scores = {}\n",
    "    #scores = pd.DataFrame(columns = ['cluster_nb', 'silhouette', 'calinski_harabasz', 'period'])\n",
    "    \n",
    "    print(\"\\nComputing for period: \", period)\n",
    "    for n_clust in nb_clusters:\n",
    "        \n",
    "        # define model\n",
    "        model = clustering_model(n_clust)\n",
    "        \n",
    "        #print(\"Clustering a\", period_df.shape, \" shaped data. With \", n_clust, \" clusters\")\n",
    "        #cluster_scores = cross_val_score(model, period_df, y=None, cv=10, scoring='rand_score')\n",
    "        \n",
    "        #cv_scores[n_clust] = cluster_scores.mean()\n",
    "        model = model.fit(period_df)\n",
    "        \n",
    "        scores = pd.DataFrame({'cluster_nb' : n_clust, \\\n",
    "                               'silhouette' : metrics.silhouette_score(period_df, model.labels_, metric='euclidean'), \\\n",
    "                               'calinski_harabasz' : metrics.calinski_harabasz_score(period_df, model.labels_), \\\n",
    "                               'period' : period}, index=[0])\n",
    "                              \n",
    "        cluster_scores = cluster_scores.append(scores, ignore_index=True)\n",
    "        \n",
    "# Plot silhouette score\n",
    "fig = px.line(cluster_scores, x=\"cluster_nb\", y=\"silhouette\", range_y=[0,0.35], title='Silhouette score for different number of clusters', color=\"period\")\n",
    "fig[\"layout\"].pop(\"updatemenus\")\n",
    "fig.show()\n",
    "\n",
    "# Plot Calinski Harabasz\n",
    "fig_other = px.line(cluster_scores, x=\"cluster_nb\", y=\"calinski_harabasz\", log_y=True, title='Calinski Harabasz score for different number of clusters', color=\"period\")\n",
    "fig_other[\"layout\"].pop(\"updatemenus\")\n",
    "fig_other.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_data(period_df):\n",
    "    \n",
    "    # get configuration maximising clustering score\n",
    "    scores = cluster_scores.loc[cluster_scores['period'] == period_df.name]\n",
    "    optimum = scores[scores.calinski_harabasz == scores.calinski_harabasz.max()].reset_index()\n",
    "    \n",
    "    print(\"Got a maximum cross validation score for period\", period_df.name, \":\", optimum.calinski_harabasz[0], \", with optimum:\", optimum.cluster_nb[0])\n",
    "    \n",
    "    #model = clustering_model(optimum.cluster_nb[0]).fit(period_df)\n",
    "    model = clustering_model(8).fit(period_df)\n",
    "    period_df['Cluster'] = model.labels_\n",
    "    \n",
    "    return period_df\n",
    "    \n",
    "clustered_df = tsne_df.groupby(level=2, sort=False).apply(cluster_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize embedded optimum clustering results\n",
    "    - Embed data into two dimensions using Locally Linear Embedding\n",
    "    - Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_embed_2d(period_df):\n",
    "    embedded = LocallyLinearEmbedding(n_components=2, n_neighbors=35, max_iter=1000, method='standard', eigen_solver='arpack').fit_transform(period_df.drop(columns=['Cluster'])).transpose()\n",
    "    \n",
    "    period_df['x'] = embedded[0]\n",
    "    period_df['y'] = embedded[1]\n",
    "    \n",
    "    period_df.drop(columns=['z'], inplace=True)\n",
    "    \n",
    "    return period_df\n",
    "\n",
    "    \n",
    "# embed in 2D\n",
    "clustered_2d_df = clustered_df.groupby(level=2, sort=False, as_index=False).apply(final_embed_2d)\n",
    "clustered_2d_df = clustered_2d_df.groupby(level=2, sort=False, as_index=False).apply(lambda df: df.sort_values(by=['Cluster']))\n",
    "clustered_2d_df.Cluster = clustered_2d_df.Cluster.astype(str)\n",
    "\n",
    "# TODO: give an identity to each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig = px.scatter(clustered_2d_df.reset_index(), x=\"x\", y=\"y\", \\\n",
    "                 animation_frame=\"period\", \\\n",
    "                 size = \"numOccurrences\", \\\n",
    "                 size_max=50, color=\"Cluster\", \\\n",
    "                 hover_name=\"speaker\", \\\n",
    "                 width=800, height=800, opacity=0.9, \\\n",
    "                 title = 'Clustering of speakers vectorized properties shown for each period of Brexit.')\n",
    "\n",
    "fig[\"layout\"].pop(\"updatemenus\")\n",
    "fig.update_xaxes(visible=False)\n",
    "fig.update_yaxes(visible=False)\n",
    "\n",
    "fig.show()\n",
    "if not exists(\"Plotly_html/clustering_brexit.html\"):\n",
    "    fig.write_html(\"Plotly_html/clustering_brexit.html\",config={\"responsive\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Results'></a>\n",
    "\n",
    "# Generate the results for the final story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Statistics'></a>\n",
    "\n",
    "## General Statistics\n",
    "\n",
    "Now that we had preprocessed the datas let's have a look at different basic statistics to explore deeply the dataset. Let's first look at the distribution of the quotations accross time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic statistics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some numbers \n",
    "nb_quotations = len(quotebank_brexit)\n",
    "nb_speakers = len(pd.unique(oneh_quotebank_brexit.speaker))\n",
    "nb_countries = len(unique_values[\"nationality\"])\n",
    "print(\"Number of quotations :\",nb_quotations)\n",
    "print(\"Number of speakers :\",nb_speakers)\n",
    "print(\"Number of countries :\",nb_countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution of the number of quotations about Brexit accross time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# windows size to compute the mooving average \n",
    "windows_avg_quotations = 9\n",
    "\n",
    "# generate a data range to complete missing values\n",
    "date_range = pd.date_range(start=quotebank_brexit.date.dt.date.min(),end=quotebank_brexit.date.dt.date.max())\n",
    "quotes_count = pd.DataFrame(date_range,columns=[\"date\"])\n",
    "# Count the number of quotes per date on the original dataset\n",
    "count_quotebank = quotebank_brexit.date.dt.date.to_frame().groupby(\"date\").size().reset_index(name=\"Count\")\n",
    "count_quotebank[\"date\"] = pd.to_datetime(count_quotebank[\"date\"])\n",
    "# Merge the data range with the original data set\n",
    "quotes_count = pd.merge(quotes_count,count_quotebank,on=\"date\",how=\"left\")\n",
    "# date that do not have count then set it to 0 (no quotation recorded)\n",
    "quotes_count.loc[quotes_count[\"Count\"].isna(),\"Count\"] = 0\n",
    "# Compute the mooving average \n",
    "quotes_count[\"moo_avg\"] = quotes_count[\"Count\"].rolling(windows_avg_quotations,center=True).mean()\n",
    "# Only keep quotations after 2016, almost no quotations speaking about brexit before 2016\n",
    "quotes_count = quotes_count[quotes_count.date.dt.year >= 2016]\n",
    "\n",
    "# Initialize the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot the histogram of number of quotations\n",
    "fig.add_trace(go.Histogram(x=quotebank_brexit[quotebank_brexit.date.dt.year >= 2016].date.dt.date.values,xbins=dict(\n",
    "                      start=str(quotes_count.date.dt.date.min()),\n",
    "                      end=str(quotes_count.date.dt.date.max()),\n",
    "                      size='D'),autobinx=False, opacity=0.3, marker=dict(color=\"blue\"),name=\"Number of quotations\"))\n",
    "\n",
    "# Plot the moving average \n",
    "fig.add_trace(go.Scatter(x=quotes_count.date,y=quotes_count[\"moo_avg\"],\n",
    "              mode=\"lines\",marker=dict(color=\"blue\"),name=str(windows_avg_quotations) +\"-days averaged\"))\n",
    "\n",
    "# Set the font and other styles \n",
    "fig.update_layout(paper_bgcolor=\"white\",plot_bgcolor=\"white\",\n",
    "                    legend=dict(\n",
    "                    font={\"family\":\"Helvetica\"},\n",
    "                    orientation=\"h\",\n",
    "                    yanchor=\"bottom\",\n",
    "                    y=1.1,\n",
    "                    xanchor=\"left\",\n",
    "                    x=0),\n",
    "                   xaxis=dict(rangeslider=dict(\n",
    "                    visible=True\n",
    "                    )))\n",
    "\n",
    "fig.update_layout(title = {'text': \"Number of quotations about Brexit\",\n",
    "                 'y':0.95,\n",
    "                 'x':0.5,\n",
    "                 'xanchor': 'center',\n",
    "                 'yanchor': 'top',\"font\":{\"family\":\"Helvetica\",\"size\":18}})\n",
    "\n",
    "# Label axes \n",
    "fig.update_yaxes(title=dict(text=\"Number of quotations\",font={\"family\":\"Helvetica\"}))\n",
    "fig.update_xaxes(title=dict(text=\"Date\",font={\"family\":\"Helvetica\"}))\n",
    "\n",
    "# Show that \n",
    "fig.show()\n",
    "\n",
    "if not exists(\"Plotly_html/quotations_brexit.html\"):\n",
    "    fig.write_html(\"Plotly_html/quotations_brexit.html\",config={\"responsive\":True})\n",
    "\n",
    "quotes_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the top 20 countries that are providing the most quotations about Brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df = oneh_quotebank_brexit.loc[:,unique_values[\"nationality\"]].sum(axis=0).T.to_frame().reset_index()\n",
    "country_df = country_df.sort_values(by=0,ascending=False)\n",
    "fig = px.bar(country_df,y=0,x=\"index\",log_y=True)\n",
    "fig.update_layout(title=\"Number of quotations about Brexit accross time\",\n",
    "                  xaxis_title=\"Country\",yaxis_title=\"Count\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the top sectors that are providing the most quotations on Brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df = oneh_quotebank_brexit.loc[:,unique_values[\"occupation\"]].sum(axis=0).T.to_frame().reset_index()\n",
    "job_df = job_df.sort_values(by=0,ascending=False)\n",
    "fig = px.bar(job_df,y=0,x=\"index\",log_y=True)\n",
    "fig.update_layout(title=\"Number of quotations about Brexit accross time\",\n",
    "                  xaxis_title=\"Sector\",yaxis_title=\"Count\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the top 10 speakers that are providing the most quotations on Brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_quotebank_brexit.groupby(\"speaker\").size().reset_index(name=\"count\").loc[:,[\"speaker\",\"count\"]].sort_values(by=\"count\",ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will be used in further studies \n",
    "# This function returns a subset of features about quotations\n",
    "# And this function filters also the dataset according to a year range\n",
    "\n",
    "def select_by_year(low_year,up_year, col=None):\n",
    "    # Convert date into year\n",
    "    year_col = pd.DatetimeIndex(oneh_quotebank_brexit.date).year\n",
    "    if col is None:\n",
    "        cols = [\"sentiment_score\"]\n",
    "    else:\n",
    "        cols = list(unique_values[col]) + [\"sentiment_score\"] # Features selected\n",
    "    # Filter according to the date and the features\n",
    "    filter_df = oneh_quotebank_brexit.loc[(year_col >= low_year) & (year_col <= up_year),cols]\n",
    "    # Group by sentiment score and compute the sum\n",
    "    filter_df = filter_df.groupby(\"sentiment_score\").sum()\n",
    "    # count the number of quotation per feature\n",
    "    count = filter_df.sum(axis=0)\n",
    "    # Normalize between [0 100] and transpose \n",
    "    filter_df = (filter_df * 100/ filter_df.sum(axis=0)).T.reset_index()\n",
    "    # Add a count column\n",
    "    filter_df[\"count\"] = count.values\n",
    "    return filter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeling about brexit in the united kingdom\n",
    "\n",
    "As an introduction, we will first have a look on the evolution of the feeling about Brexit in the more involved country: the United Kingdom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the quotations from United kingdom\n",
    "UK_df = oneh_quotebank_brexit[oneh_quotebank_brexit[\"united kingdom\"] == 1]\n",
    "# Years to plot (removed 2015 because not enough data)\n",
    "years = list(range(2016,2021))\n",
    "\n",
    "# Negative - Neutral - Positive colors \n",
    "color_discrete_map = ['#FF5B5B',\"#FFF992\",'#7BD787']\n",
    "\n",
    "# Initialize the figure \n",
    "fig = make_subplots(rows=1, cols=len(years), specs=[[{'type':'domain'}]*len(years)])\n",
    "\n",
    "# Loop over the years \n",
    "for i,pie_year in enumerate(years):\n",
    "    # Get a year index\n",
    "    year_col = pd.DatetimeIndex(UK_df.date).year\n",
    "    # Get the right year \n",
    "    filter_df = UK_df.loc[year_col == pie_year,[\"sentiment_score\"]]\n",
    "    # Group by sentiment and count for each \n",
    "    filter_df = filter_df.groupby(\"sentiment_score\").size().reset_index(name=\"Count\")\n",
    "    # Generate the pie_chart\n",
    "    pie_trace = go.Pie(labels=filter_df.sentiment_score,\n",
    "                       values=filter_df.Count,name=str(pie_year),\n",
    "                       title=str(pie_year))\n",
    "    # Add a trace to the figure \n",
    "    fig.add_trace(pie_trace,1,i+1)\n",
    "\n",
    "# Make a hole, set colors \n",
    "fig.update_traces(hole=.4, hoverinfo=\"label+percent+name\",marker = dict(colors=color_discrete_map),textinfo='none')\n",
    "\n",
    "# Tune the figure \n",
    "fig.update_layout(title = {'text': \"Semtiment towards Brexit grouped by year [United kingdom]\",\n",
    "                'y':0.1,\n",
    "                'x':0.5,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'bottom',\"font\":{\"family\":\"Helvetica\",\"size\":18}},\n",
    "                legend={\"font\":{\"family\":\"Helvetica\",\"size\":16},\n",
    "                \"orientation\":\"h\",\n",
    "                \"yanchor\":\"top\",\n",
    "                \"y\":1.2,\n",
    "                \"xanchor\":\"center\",\n",
    "                \"x\":0.5})\n",
    "\n",
    "fig.update_layout(height=400,width=1200)\n",
    "\n",
    "# fig.show(config={\"responsive\":True})\n",
    "\n",
    "# Export the figure \n",
    "if not exists(\"Plotly_html/pie_chart.html\"):\n",
    "    fig.write_html(\"Plotly_html/pie_chart.html\",config={\"responsive\":True})\n",
    "\n",
    "UK_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is there a real difference between 2016 and 2020 ?\n",
    "\n",
    "We would like to figure out if the feeling about Brexit in the United Kingdom was really different between 2016 and 2020. To achieve this, we will compare the sentiment distributions of 2016 and 2020 using a two-sample kolmogorov smirnov test. The null hypothesis of such a test is: the distributions are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute your p-values here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Country'></a>\n",
    "\n",
    "## Analyze the way Brexit is perceived in European countries\n",
    "\n",
    "Recall that the goal is to analyze the way Brexit is perceived by each Europe country based on the sentiment carried by the quotation. Besides, we would like to add the time dimension to this analysis, meaning that we would like to follow the evolution of the overall feelings towards Brexit. To achieve this we will compute for each country the overall feeling about Brexit up to a year (2016 to 2020), we will then merge this into a single dataframe. This final dataframe will then be used to create an animation to follow the brexit sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all the information needed for a map plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the geojson file that will be required to define the map \n",
    "with open(\"Brexit_datas\\country_class\\countries.geojson\") as file:\n",
    "    country_gj = geojson.load(file)\n",
    "\n",
    "# Get the countries and their ISO2 id\n",
    "current_countries = pd.read_excel(\"Brexit_datas\\country_class\\countries.xlsx\")\n",
    "\n",
    "# Remove capital letters and special characters\n",
    "current_countries.Country = current_countries.Country.str.lower()\n",
    "current_countries.Country = current_countries.Country.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "\n",
    "# Map ISO2 id to ISO3 id (we need ISO3 instead of ISO2 because ISO3 is specified in the geojson dataset)\n",
    "ISO_mapping = pd.read_csv(\"Brexit_datas\\country_class\\ISO2_ISO3.csv\")\n",
    "current_countries = pd.merge(ISO_mapping,current_countries,left_on=\"ISO2\",right_on=\"ISO\",how=\"right\").drop(columns=[\"ISO\",\"ISO2\"])\n",
    "\n",
    "# Remove countries for which ISO3 was not found \n",
    "current_countries = current_countries[~current_countries.ISO3.isna()]\n",
    "\n",
    "# Export the first dataset needed \n",
    "if not exists(\"Brexit_datas/country_class/countries_to_ISO3.csv\"):\n",
    "    current_countries.to_csv(\"Brexit_datas/country_class/countries_to_ISO3.csv\",index=False)\n",
    "\n",
    "# Extract only relevant features \n",
    "country_analysis_df = oneh_quotebank_brexit[list(unique_values[\"nationality\"]) + [\"sentiment_score\",\"date\"]]\n",
    "country_analysis_df[\"date\"] = country_analysis_df.date.dt.year\n",
    "\n",
    "# Define a function that will be used in further studies \n",
    "# This function returns a subset of features about quotations\n",
    "# And this function filters also the dataset according to a year range\n",
    "def select_by_year_country(low, high, dash_df):\n",
    "    filter_df = dash_df[(dash_df.date >= low) & (dash_df.date <= high)]\n",
    "    filter_df.drop(columns=\"date\", inplace=True)\n",
    "    filter_df = filter_df.groupby(\"sentiment_score\").sum()\n",
    "    count = filter_df.sum(axis=0)\n",
    "    filter_df = (filter_df * 100 / filter_df.sum(axis=0)).T.reset_index().rename(columns={\"index\":\"country\"})\n",
    "    filter_df[\"count\"] = count.values\n",
    "    filter_df = filter_df[filter_df[\"count\"] > 40]\n",
    "    filter_df[\"Sentiment\"] = -1*filter_df.Negative + filter_df.Positive\n",
    "    filter_df = pd.merge(filter_df,current_countries,left_on=\"country\",right_on=\"Country\",how=\"left\").drop(columns=[\"country\"])\n",
    "    return filter_df\n",
    "\n",
    "country_analysis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the evolution into a single dataframe\n",
    "full_country_df = select_by_year_country(2016,2016,country_analysis_df)\n",
    "full_country_df[\"year\"] = 2016\n",
    "for year in range(2017,2021):\n",
    "    new_year = select_by_year_country(2016,year,country_analysis_df)\n",
    "    new_year[\"year\"] = year\n",
    "    full_country_df = pd.concat([full_country_df,new_year],axis=0)\n",
    "\n",
    "# Plot the figure with the analysis \n",
    "fig = px.choropleth_mapbox(full_country_df, geojson=country_gj, locations='ISO3', color='Sentiment',\n",
    "                            animation_frame=\"year\",\n",
    "                            featureidkey=\"properties.ISO_A3\",\n",
    "                            range_color=[-10,35],\n",
    "                            color_continuous_scale=\"RdYlGn\",\n",
    "                            mapbox_style=\"carto-positron\",\n",
    "                            zoom=2.5, center = {\"lat\": 48.856613, \"lon\": 2.352222},\n",
    "                            opacity=0.6,hover_data=[\"Country\",\"count\"])\n",
    "\n",
    "# Tune the map \n",
    "fig.update_layout(title={'text': \"Evolution of the feeling about Brexit per country [2016-2020]\",\n",
    "                             'y':0.02,\n",
    "                             'x':0.5,\n",
    "                             'xanchor': 'center',\n",
    "                             'yanchor': 'bottom'},legend_font_family=\"Helvetica\",legend_xanchor=\"center\",\n",
    "                             legend_yanchor=\"bottom\",\n",
    "                             title_font_family=\"Helvetica\",title_font_size=20,height=950)\n",
    "\n",
    "fig['layout']['updatemenus'][0][\"y\"] = 1.32\n",
    "fig['layout']['sliders'][0][\"y\"] = 1.32\n",
    "\n",
    "if not exists(\"Plotly_html/map_anim.html\"):\n",
    "    fig.write_html(\"Plotly_html/map_anim.html\")\n",
    "\n",
    "full_country_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Sector'></a>\n",
    "\n",
    "## Analyze the way Brexit is perceived in different sectors\n",
    "\n",
    "Then, we would like to perform a similar analysis, but based on the different sectors (economy, health, art ...). Let see if there are different trends between the different sectors. Again as there is more than 20 sectors we cannot afford to plot all the distributions in a single graph and we would need some filters to compare some given sector. Besides we would to add the time dimension again. \n",
    "\n",
    "To do so we need to use dynamic graphs with filters that allows the user to modify the year. This can be achieved using dash plotly application. This application, as well as the associated stuff needed, will be embeded into a **Heroku** server that will actually host the application. We will need to create two things:\n",
    "- The dataset that will be embedded in the heroku app. This dataset will be called each time the user wants to modify something.\n",
    "- The script that will drive the application (callbacks, layout and so on ...)\n",
    "\n",
    "We will then finally send these two files onto a heroku repository and then the application will run and be available online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the dataset that will be embeded into the heroku application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the right subset of features \n",
    "sector_df = oneh_quotebank_brexit[list(unique_values[\"occupation\"]) + [\"sentiment_score\",\"date\"]]\n",
    "sector_df[\"date\"] = sector_df.date.dt.year\n",
    "\n",
    "if not exists(\"Dash_datas/dash_data_occupation.csv.gz\"):\n",
    "    sector_df.to_csv(\"Dash_datas/dash_data_occupation.csv.gz\",index=False)\n",
    "\n",
    "sector_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python script embedded into the heroku app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the app\n",
    "app = dash.Dash(__name__)\n",
    "server = app.server\n",
    "\n",
    "# Colors that will be displayed on the pie charts \n",
    "color_discrete_map = ['#FF5B5B',\"#FFF992\",'#7BD787',\"#000000\"]\n",
    "\n",
    "# Define a function that will be used in further studies \n",
    "# This function returns a subset of features about quotations\n",
    "# And this function filters also the dataset according to a year range\n",
    "def select_by_year_occupation(low,high,dash_df):\n",
    "    filter_df = dash_df[(dash_df.date >= low) & (dash_df.date <= high)]\n",
    "    filter_df.drop(columns=\"date\",inplace=True)\n",
    "    filter_df = filter_df.groupby(\"sentiment_score\").sum()\n",
    "    count = filter_df.sum(axis=0)\n",
    "    filter_df = filter_df.T.reset_index().rename(columns={\"index\":\"occupation\"})\n",
    "    filter_df[\"count\"] = count.values\n",
    "    return filter_df\n",
    "\n",
    "# Get the datas \n",
    "dash_data = pd.read_csv(\"Dash_datas/dash_data_occupation.csv.gz\",compression=\"infer\")\n",
    "\n",
    "# Get the list of the occupations\n",
    "occupations = list(dash_data.columns[:-2])\n",
    "\n",
    "# Define the marks for the slider \n",
    "marks_slider = {year:{\"label\" : str(year),\n",
    "               \"style\" : {\"color\":\"black\",\"font-family\":\"Helvetica\",\"font-size\": 18}} \n",
    "               for year in range(dash_data.date.min()+1,dash_data.date.max()+1)}\n",
    "\n",
    "# Define the options for the dropdown\n",
    "options = [{\"label\": occup, \"value\": occup,\n",
    "            \"style\" : {\"color\":\"black\",\"font-family\":\"Helvetica\",\"font-size\": 18}} \n",
    "            for occup in list(occupations)]\n",
    "\n",
    "# Define the layout of the application\n",
    "app.layout = html.Div([\n",
    "        html.Div([html.P(\"Year\",style={\"display\": \"flex\",\n",
    "                                \"font-family\":\"Helvetica\",\n",
    "                                \"font-size\": 18,\n",
    "                                \"align-items\": \"center\",\n",
    "                                \"justify-content\": \"center\"}),\n",
    "                            dcc.RangeSlider(\n",
    "                                id='range-slider',\n",
    "                                min=dash_data.date.min()+1, max=dash_data.date.max(), step=1,\n",
    "                                marks=marks_slider,\n",
    "                                value=[dash_data.date.min()+1,dash_data.date.max()])],\n",
    "                            style = {\"width\":\"60%\",\"margin-bottom\":25,\"margin-left\":\"20%\",\"width\":\"60%\",\"margin-right\":\"20%\"}),\n",
    "        html.Div([dcc.Dropdown(id='drop-down',multi=True,\n",
    "                               style={\"color\":\"black\",\"font-family\":\"Helvetica\",\"font-size\": 18},\n",
    "                               options=options,value=[\"Art\",\"Health\",\"Economy\"])],\n",
    "                 style = {\"width\":\"60%\",\"margin-left\":\"20%\",\"width\":\"60%\",\"margin-right\":\"20%\"}),\n",
    "        html.Div([dcc.Graph(id=\"pie-chart\",responsive=True)])\n",
    "    ])\n",
    "\n",
    "\n",
    "# function that will update pies and drop down values \n",
    "@app.callback(Output(\"pie-chart\", \"figure\"),\n",
    "              [Input(\"range-slider\",\"value\"),Input(\"drop-down\",\"value\")])\n",
    "def update_pies(slider_range,values_selected):\n",
    "    # get the year range wanted \n",
    "    low, high = slider_range\n",
    "    # Filter according to the year \n",
    "    sector_analysis = select_by_year_occupation(low,high,dash_data)\n",
    "    # Filter by the sectors that are specified in the dropdown list\n",
    "    sector_analysis = sector_analysis[sector_analysis[\"occupation\"].isin(values_selected)]\n",
    "    # labels for graph\n",
    "    labels = list(sector_analysis.columns[-4:-1])\n",
    "    # Initialize the figure \n",
    "    fig = make_subplots(rows=1,cols=len(sector_analysis),specs=[[{\"type\":\"domain\"}]*len(sector_analysis)])\n",
    "    start = 1\n",
    "    # Sort by count \n",
    "    sector_analysis.sort_values(by=\"count\",ascending=False,inplace=True)\n",
    "    # Loop over the sector analysis rows \n",
    "    for index, row in sector_analysis.iterrows():\n",
    "        # If count lower than 40 then not enough data to well represent the sector\n",
    "        if row[\"count\"] < 40:\n",
    "            trace = go.Pie(labels=labels + [\"Not enough data\"],\n",
    "                           values=[0,0,0,1],\n",
    "                           name=row[\"occupation\"],\n",
    "                           title=row[\"occupation\"])\n",
    "        else:\n",
    "            trace = go.Pie(labels=labels,\n",
    "                            values=row[labels].values,\n",
    "                            name=row[\"occupation\"],\n",
    "                            title=row[\"occupation\"])\n",
    "        # Add the trace \n",
    "        fig.add_trace(trace,row =1,col=start)\n",
    "        start += 1\n",
    "    # Tune the figure \n",
    "    fig.update_traces(hole=.4, hoverinfo=\"label+percent+name\",marker = dict(colors=color_discrete_map),textinfo='none')\n",
    "    fig.update_layout(title = {'text': \"Semtiment towards Brexit by sector\",\n",
    "                        'y':0.05,\n",
    "                        'x':0.5,\n",
    "                        'xanchor': 'center',\n",
    "                        'yanchor': 'bottom',\"font\":{\"family\":\"Helvetica\",\"size\":20}},\n",
    "                        legend={\"font\":{\"family\":\"Helvetica\",\"size\":16},\n",
    "                        \"orientation\":\"h\",\n",
    "                        \"yanchor\":\"top\",\n",
    "                        \"y\":1.2,\n",
    "                        \"xanchor\":\"center\",\n",
    "                        \"x\":0.5})\n",
    "    return fig\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is there a real difference between sectors ?\n",
    "\n",
    "We would like to figure out if the feeling about Brexit is really different from one sector to another, idest is there a statistically significant difference between two sectors. We selected a subset of pairs of sectors for which we would like to test that. Again, to achieve this, we will compare the sentiment distributions between the two sectors in the pair using a two-sample kolmogorov smirnov test. The null hypothesis of such a test is: the distributions are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the p-values here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Age'></a>\n",
    "\n",
    "# Age analysis\n",
    "\n",
    "It is common to hear that Brexit is mostly due to elderly people and that young people are mostly against Brexit. Let try to see if such a feeling is confirmed by our datas. We will study the feeling about Brexit for each age category. We will here again use a dynamic graph, hence we will again follow the same procedure as decribed below.\n",
    "\n",
    "To do so we need to use dynamic graphs, this can be achieved using dash plotly application. This application, as well as the associated stuff needed, will be embeded into a **Heroku** server that will actually host the application. We will need to create two things:\n",
    "- The dataset that will be embedded in the heroku app. This dataset will be called each time the user wants to modify something.\n",
    "- The script that will drive the application (callbacks, layout and so on ...)\n",
    "\n",
    "We will then finally send these two files onto a heroku repository and then the application will run and be available online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the dataset that will be embeded into the heroku application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant features \n",
    "age_df = oneh_quotebank_brexit.loc[:,[\"speaker\",\"Age\",\"sentiment_score\"]]\n",
    "# Compute an overall sentiment score \n",
    "age_df[\"score\"] = age_df[\"sentiment_score\"].apply(lambda sent: 1*(sent == \"Positive\") - 1*(sent == \"Negative\"))\n",
    "# Drop useless columns \n",
    "age_df.drop(columns=[\"sentiment_score\"],inplace=True)\n",
    "# Take the mean for each speaker\n",
    "age_df = age_df.groupby(\"speaker\").mean()\n",
    "# Remove any speakers for which we do not have age information\n",
    "age_df = age_df[~age_df[\"Age\"].isna()]\n",
    "\n",
    "# Convert continuous sentiment score into categorical values \n",
    "def reverse_sentiment(score, threshold = 0.05):\n",
    "    if score > threshold:\n",
    "        return \"Positive\"\n",
    "    elif score < -threshold:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# convert to categorical\n",
    "age_df[\"sentiment\"] = age_df[\"score\"].apply(reverse_sentiment)\n",
    "# Categorize by ages - size bin = 10\n",
    "age_df[\"Age\"] = pd.cut(age_df.Age,bins=list(range(20,101,10)))\n",
    "# One hot vectorize that \n",
    "age_df = pd.get_dummies(age_df.reset_index().drop(columns=[\"speaker\",\"score\"]).set_index(\"Age\")).reset_index()\n",
    "# Group by age category and sum \n",
    "age_df = age_df.groupby(\"Age\").sum()\n",
    "age_df.columns = [\"Negative\",\"Neutral\",\"Positive\"]\n",
    "\n",
    "# Export the datas \n",
    "if not exists(\"Dash_datas/dash_data_age.csv\"):\n",
    "    age_df.to_csv(\"Dash_datas/dash_data_age.csv\")\n",
    "\n",
    "age_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python script embedded into the heroku app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the app\n",
    "app = dash.Dash(__name__)\n",
    "server = app.server\n",
    "\n",
    "# Colors to be used in the bar chart\n",
    "color_discrete_map = ['#FF5B5B',\"#FFF992\",'#7BD787']\n",
    "\n",
    "# Import the datas\n",
    "age_df = pd.read_csv(\"Dash_datas/dash_data_age.csv\",index_col=\"Age\")\n",
    "age_df_percentage = (age_df*100) / age_df.sum(axis=1).values.reshape(-1,1)\n",
    "\n",
    "# X values for the bar chart\n",
    "x_values = list(age_df.index.astype(str))\n",
    "x_values = [\"Age \" + age for age in x_values]\n",
    "\n",
    "# Options available \n",
    "graph_type = [\"Percentage\",\"Absolute\"]\n",
    "\n",
    "options = [{\"label\": gtype, \"value\": gtype,\n",
    "            \"style\" : {\"color\":\"black\",\"font-family\":\"Helvetica\",\"font-size\": 18}} \n",
    "            for gtype in list(graph_type)]\n",
    "\n",
    "# Define the layout of the application\n",
    "app.layout = html.Div([\n",
    "                        html.Div([dcc.RadioItems(id='drop-down',\n",
    "                                       options=options,\n",
    "                                       labelStyle={'display': 'inline-block',\n",
    "                                                \"color\":\"black\",\n",
    "                                                \"font-family\":\"Helvetica\",\n",
    "                                                \"font-size\": 18},\n",
    "                                       value=\"Percentage\")],\n",
    "                                    style={\"margin-left\":\"43%\",\"width\":\"20%\",\"margin-right\":\"37%\"}),\n",
    "                        dcc.Graph(id=\"bar-chart\",responsive=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Update the bar chart\n",
    "@app.callback(Output(\"bar-chart\", \"figure\"),\n",
    "              Input(\"drop-down\",\"value\"))\n",
    "def update_bars(value_selected):\n",
    "    # Percentage or absolute \n",
    "    if value_selected == \"Percentage\":\n",
    "        df = age_df_percentage\n",
    "    else:\n",
    "        df = age_df\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # for each type of sentiment \n",
    "    for i,col in enumerate(df.columns):\n",
    "        trace = go.Bar(name = col, x=x_values, y = df[col].values, \n",
    "                       marker=dict(color=color_discrete_map[i]))\n",
    "        fig.add_trace(trace)\n",
    "    \n",
    "    # Tune the figure\n",
    "    fig.update_layout(paper_bgcolor=\"white\",plot_bgcolor=\"white\",\n",
    "                      title = {'text': \"Perception of Brexit by the different age groups\",\n",
    "                      'y':0.03,\n",
    "                      'x':0.5,\n",
    "                      'xanchor': 'center',\n",
    "                      'yanchor': 'bottom',\"font\":{\"family\":\"Helvetica\",\"size\":22}},\n",
    "                      legend={\"font\":{\"family\":\"Helvetica\",\"size\":16},\n",
    "                      \"orientation\":\"h\",\n",
    "                      \"yanchor\":\"top\",\n",
    "                      \"y\":1.2,\n",
    "                      \"xanchor\":\"center\",\n",
    "                      \"x\":0.5})\n",
    "    \n",
    "    # Do not show y axis if it is percentage \n",
    "    if value_selected == \"Percentage\":\n",
    "        fig.update_yaxes(title=dict(text=\"Percentage\",font={\"family\":\"Helvetica\"}))\n",
    "    else:\n",
    "        fig.update_yaxes(title=dict(text=\"Number of speakers\",font={\"family\":\"Helvetica\"}))\n",
    "    \n",
    "    return fig\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is there a real difference between age categories ?\n",
    "\n",
    "Let see if the rumor previously mentionned is true or not, idest let check if there exists a statistically significant difference between two age category. We selected a subset of pairs of age categories for which we would like to test that. Again, to achieve this, we will compare the sentiment distributions between the two age categories in the pair using a two-sample kolmogorov smirnov test. The null hypothesis of such a test is: the distributions are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute p-values here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Gender'></a>\n",
    "\n",
    "## Gender Analysis\n",
    "\n",
    "In the same spirit it was done before, let see if the feeling about Brexit is a function of the gender. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select gender columns and compute for each of them some values \n",
    "gender_df = select_by_year(2016,2020,\"gender\").rename(columns={\"index\":\"gender\"})\n",
    "# List of distinct genders\n",
    "x_values = list(gender_df.gender)\n",
    "\n",
    "# Colors for the bar chart \n",
    "color_discrete_map = ['#FF5B5B',\"#FFF992\",'#7BD787']\n",
    "cols = [\"Negative\",\"Neutral\",\"Positive\"]\n",
    "\n",
    "# Initialize the figure \n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the histograms for each gender \n",
    "for i,col in enumerate(cols):\n",
    "    trace = go.Bar(name = col, x=x_values, y = gender_df[col].values, \n",
    "                    marker=dict(color=color_discrete_map[i]))\n",
    "    fig.add_trace(trace)\n",
    "\n",
    "# Tune the figure layout \n",
    "fig.update_layout(paper_bgcolor=\"white\",plot_bgcolor=\"white\",\n",
    "                  title = {'text': \"Difference in perception of Brexit by people of different gender\",\n",
    "                  'y':0.03,\n",
    "                  'x':0.5,\n",
    "                  'xanchor': 'center',\n",
    "                  'yanchor': 'bottom',\"font\":{\"family\":\"Helvetica\",\"size\":20}},\n",
    "                  legend={\"font\":{\"family\":\"Helvetica\",\"size\":16},\n",
    "                  \"orientation\":\"h\",\n",
    "                  \"yanchor\":\"top\",\n",
    "                  \"y\":1.2,\n",
    "                  \"xanchor\":\"center\",\n",
    "                  \"x\":0.5})\n",
    "\n",
    "# fig.show()\n",
    "\n",
    "if not exists(\"Plotly_html/gender_bar.html\"):\n",
    "    fig.write_html(\"Plotly_html/gender_bar.html\")\n",
    "\n",
    "gender_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Stocks'></a>\n",
    "\n",
    "## Correlation with stocks\n",
    "\n",
    "We would like to have more insights on how the Brexit influenced the Economy. To do so we will look at the top 100 british stock auctions also known at FTSE100 and try to see if they are correlated to the Brexit. To assess the correlation between Brexit and stock auction we followed the procedure described below: \n",
    "- We could actually measure the turmoil due to Brexit by looking at the evolution of the number of quotation accross time. A huge number of new quotations in a short time means that there is a a high turmoil due to Brexit. Thus it seems natural to estimate the Brexit turmoil through the derivative of the number quotations with respect to the time. Instead of taking the derivative of the original curve, we would rather take the derivative of the cumulative sum of the number of quotations with respect to time, as a decrease in the number of quotations would not have a real sense.\n",
    "- In the same spirit, regarding stock auctions, we are not really interested in the absolute value of them. We would rather prefer taking the derivative that tells you if the stock auction rises or falls. We then want to see if these rises or falls are correlated with Brexit turmoil. Besides, to simplify the things, we will take the absolute value of the derivative as Brexit may cause negative as well as positive effects.\n",
    "- Finally we will them compute the pearson coefficient correlation coefficient that will give us an insights on how Brexit is correlated with stock auction.\n",
    "\n",
    "Before computing any derivative, it may be important to recall that we are performing some signal processing on the original time series (in a nutshell mooving average) to get ride of an eventual noise. At the end we will provide an interactive dashboard that will allow the user to see the correlation between Brexit and some stock auction. We will only keep the ones with a sufficient pearson correlation coefficient and sufficient low p-value. We will here again use a dynamic graph to generate such a dashboard, hence we will again follow the same procedure as decribed below:\n",
    "\n",
    "To do so we need to use dynamic graphs, This can be achieved using dash plotly application. This application, as well as the associated stuff needed, will be embeded into a **Heroku** server that will actually host the application. We will need to create two things:\n",
    "- The dataset that will be embedded in the heroku app. This dataset will be called each time the user wants to modify something.\n",
    "- The script that will drive the application (callbacks, layout and so on ...)\n",
    "\n",
    "We will then finally send these two files onto a heroku repository and then the application will run and be available online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the stock auction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Get information about FTSE companies \n",
    "FTSE_companies = pd.read_excel(\"Brexit_datas/stock_actions/FTSE_100_list.xlsx\")\n",
    "tickers_FTSE = list(FTSE_companies.Ticker)\n",
    "FTSE_companies.set_index(\"Ticker\",inplace=True)\n",
    "\n",
    "# Get the data for the FTSE companies\n",
    "stock_action_FTSE = yf.download(tickers_FTSE,'2015-01-01','2020-08-01',show_errors=False)['Adj Close']\n",
    "\n",
    "# Clean corrupted columns\n",
    "stock_action_FTSE = stock_action_FTSE.dropna(axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the dataset that will be embeded into the heroku application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the windows over which we will compute the mooving average\n",
    "windows_avg_quotations = 9\n",
    "windows_avg_stock = 9\n",
    "\n",
    "# Generate a date index to remove sparsity from the datas\n",
    "date_range = pd.DataFrame(pd.date_range(start=stock_action_FTSE.Date.dt.date.min(),end=stock_action_FTSE.Date.dt.date.max()))\n",
    "date_range.columns = [\"Date\"]\n",
    "# Merge the data range\n",
    "stock_action_avg = pd.merge(stock_action_FTSE,date_range,on=\"Date\",how=\"right\")\n",
    "cols_without_date = list(stock_action_avg.columns)\n",
    "cols_without_date.remove(\"Date\")\n",
    "# Perform a linear interpolation for missing values \n",
    "stock_action_avg.loc[:,cols_without_date] = stock_action_avg.loc[:,cols_without_date].interpolate(axis=0,method=\"linear\",limit_direction=\"forward\")\n",
    "# Mooving average operation\n",
    "stock_action_avg.loc[:,cols_without_date] = stock_action_avg.loc[:,cols_without_date].rolling(windows_avg_stock,center=True).mean()\n",
    "stock_action_diff = stock_action_avg.copy()\n",
    "# Compute the derivative\n",
    "stock_action_diff.loc[:,cols_without_date] = stock_action_diff.loc[:,cols_without_date].diff(axis=0)\n",
    "# Get ride of the head and tail values\n",
    "stock_action_diff = stock_action_diff[~stock_action_diff.iloc[:,1].isna()]\n",
    "# Take the absolute value of the derivative\n",
    "stock_action_diff.loc[:,cols_without_date] = stock_action_diff.loc[:,cols_without_date].abs()\n",
    "# Set the Date as an index \n",
    "stock_action_diff.set_index(\"Date\",inplace=True)\n",
    "stock_action_avg.set_index(\"Date\",inplace=True)\n",
    "\n",
    "# Generate a date index to remove sparsity from the datas\n",
    "date_range = pd.date_range(start=quotebank_brexit.date.dt.date.min(),end=quotebank_brexit.date.dt.date.max())\n",
    "quotes_count = pd.DataFrame(date_range,columns=[\"date\"])\n",
    "# Count the number of quotations per date \n",
    "count_quotebank = quotebank_brexit.date.dt.date.to_frame().groupby(\"date\").size().reset_index(name=\"Count\")\n",
    "count_quotebank[\"date\"] = pd.to_datetime(count_quotebank[\"date\"])\n",
    "# Merge with the data range\n",
    "quotes_count = pd.merge(quotes_count,count_quotebank,on=\"date\",how=\"left\")\n",
    "# Fill date without quotations\n",
    "quotes_count.loc[quotes_count[\"Count\"].isna(),\"Count\"] = 0\n",
    "# Compute the cumulative sum\n",
    "quotes_count[\"Count\"] = quotes_count[\"Count\"].cumsum()\n",
    "# Compute the mooving average \n",
    "quotes_count[\"moo_avg\"] = quotes_count[\"Count\"].rolling(windows_avg_quotations,center=True).mean()\n",
    "# Let keep only quotes after 2016 (not enough data before)\n",
    "quotes_count = quotes_count[quotes_count.date.dt.year >= 2016]\n",
    "# Compute the differential\n",
    "quotes_count[\"differential\"] = quotes_count.moo_avg.diff()\n",
    "# Get ride of the head and tail values\n",
    "quotes_count = quotes_count[~quotes_count[\"differential\"].isna()]\n",
    "quotes_count.set_index(\"date\",inplace=True)\n",
    "\n",
    "# Reindex \n",
    "stock_action_diff = stock_action_diff.loc[quotes_count.index]\n",
    "stock_action_avg = stock_action_avg.loc[quotes_count.index]\n",
    "\n",
    "# Let compute the pearson correlation coefficient\n",
    "pearson_results = []\n",
    "# Significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# For each stock auction\n",
    "for col in cols_without_date:\n",
    "    # Compute pearson correlation coeff and associated p-value\n",
    "    pearson, p_value = stats.pearsonr(quotes_count.differential,stock_action_diff.loc[:,col])\n",
    "    pearson_results.append([pearson, p_value, col])\n",
    "\n",
    "# Convert to a DataFrame\n",
    "pearson_results = pd.DataFrame(pearson_results,columns = [\"pearson_value\",\"p-value\",\"action\"])\n",
    "# Keep stock auctions for which the correlation is significant \n",
    "pearson_results = pearson_results[pearson_results[\"p-value\"] < alpha]\n",
    "# Compute absolute value of pearson coeff\n",
    "pearson_results[\"pearson_value_abs\"] = pearson_results[\"pearson_value\"].abs()\n",
    "pearson_results.set_index(\"action\",inplace=True)\n",
    "# Keep only stock_auction for which the absolute value of r is superior to 0.15\n",
    "pearson_results = pearson_results[pearson_results.pearson_value.abs() > 0.15]\n",
    "pearson_results = pd.merge(pearson_results,FTSE_companies[[\"Company Name\"]],left_index=True,right_index=True,how=\"left\")\n",
    "pearson_results = pearson_results.sort_values(by=\"pearson_value_abs\",ascending=False)\n",
    "\n",
    "# Export results \n",
    "if not exists(\"Dash_datas/dash_datas_stock_pearson.csv\"):\n",
    "    pearson_results.to_csv(\"Dash_datas/dash_datas_stock_pearson.csv\")\n",
    "\n",
    "print(pearson_results.head())\n",
    "\n",
    "action_retained = list(pearson_results.index)\n",
    "\n",
    "# Keep only some stock auctions \n",
    "stock_action_diff = stock_action_diff.loc[:,action_retained]\n",
    "dash_datas_action = pd.merge(quotes_count.drop(columns=[\"Count\",\"moo_avg\"]),stock_action_diff,left_index=True,right_index=True)\n",
    "dash_datas_action.rename(columns={\"differential\":\"quotes_differential\"},inplace=True)\n",
    "# Export results \n",
    "if not exists(\"Dash_datas/dash_datas_stock_auction.csv.gz\"):\n",
    "    dash_datas_action.to_csv(\"Dash_datas/dash_datas_stock_auction.csv.gz\")\n",
    "\n",
    "dash_datas_action.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python script embedded into the heroku app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output\n",
    "import pandas as pd\n",
    "import statsmodels\n",
    "\n",
    "# Initialize the app\n",
    "app = dash.Dash(__name__)\n",
    "server = app.server\n",
    "\n",
    "# Import the datas \n",
    "dash_datas_action = pd.read_csv(\"Dash_datas/dash_datas_stock_auction.csv.gz\",compression=\"infer\")\n",
    "dash_datas_action.set_index(\"date\",inplace=True)\n",
    "dash_pearson = pd.read_csv(\"Dash_datas/dash_datas_stock_pearson.csv\")\n",
    "dash_pearson.set_index(\"action\",inplace=True)\n",
    "\n",
    "# Set the options \n",
    "options = [{\"label\": action, \"value\": action,\n",
    "            \"style\" : {\"color\":\"black\",\"font-family\":\"Helvetica\",\"font-size\": 18}} \n",
    "            for action in list(dash_pearson.index)]\n",
    "\n",
    "# Define the layout of the application\n",
    "app.layout = html.Div([\n",
    "                        dcc.Dropdown(id='drop-down',\n",
    "                                    style={\"color\":\"black\",\n",
    "                                           \"font-family\":\"Helvetica\",\n",
    "                                           \"margin-left\":\"15%\",\n",
    "                                           \"width\":\"70%\",\n",
    "                                           \"margin-right\":\"15%\",\n",
    "                                           \"font-size\": 18},\n",
    "                                    options=options,\n",
    "                                    searchable=False,\n",
    "                                    value=dash_pearson.index.values[0]),\n",
    "                        dcc.Graph(id=\"dashboard\",responsive=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Update the dashboard \n",
    "@app.callback(Output(\"dashboard\", \"figure\"),\n",
    "              Input(\"drop-down\",\"value\"))\n",
    "def update_action_dashboard(action):\n",
    "      # Style the dashboard \n",
    "      specs=[[{}, {\"rowspan\":2}],[{}, None]]\n",
    "      # Initialize the figure \n",
    "      fig = make_subplots(rows=2,cols=2,shared_xaxes=True,vertical_spacing=0.25,specs=specs,column_widths=[0.6, 0.4],\n",
    "                        subplot_titles=(\"\",\"Pearson coefficient r = \" + str(round(dash_pearson.loc[action,\"pearson_value\"],3)),\"\"))\n",
    "      # Derivative plot\n",
    "      fig.add_trace(go.Scatter(x=dash_datas_action.index,y=dash_datas_action.quotes_differential,\n",
    "                              mode=\"lines\",name=\"Derivative [quotations]\"),row=1,col=1)\n",
    "      fig.add_trace(go.Scatter(x=dash_datas_action.index,y=dash_datas_action.loc[:,action],\n",
    "                              mode=\"lines\",name=\"Derivate's absolute value [\" + action + \"]\"),row=2,col=1)\n",
    "      # Scatter plots \n",
    "      regline_fig = px.scatter(dash_datas_action, x=\"quotes_differential\", y=action, trendline=\"ols\",\n",
    "                              trendline_color_override=\"black\",color_discrete_sequence=[\"#ff8812\"])\n",
    "      \n",
    "      # Transfer trace to the figure \n",
    "      for trace in regline_fig[\"data\"]:\n",
    "            fig.add_trace(trace,row=1,col=2)\n",
    "\n",
    "      # Tune the figure \n",
    "      fig.update_layout(legend=dict(\n",
    "                        font={\"family\":\"Helvetica\"},\n",
    "                        orientation=\"h\",\n",
    "                        yanchor=\"top\",\n",
    "                        y=1.2,\n",
    "                        xanchor=\"left\",\n",
    "                        x=0),\n",
    "                        )\n",
    "      fig.update_layout(title = {'text': \"Correlation between brexit agitation and '\" + dash_pearson.loc[action,\"Company Name\"].lower() + \"' auction\",\n",
    "                        'y':0.02,\n",
    "                        'x':0.5,\n",
    "                        'xanchor': 'center',\n",
    "                        'yanchor': 'bottom',\"font\":{\"family\":\"Helvetica\",\"size\":22}})\n",
    "      fig.update_annotations(font_family='Helvetica')\n",
    "      fig['layout']['xaxis']['rangeslider']['visible']=True\n",
    "      fig['layout']['xaxis3']['title']='Date'\n",
    "      fig['layout']['xaxis3']['title']['font']['family'] = 'Helvetica'\n",
    "      fig['layout']['xaxis2']['title']='Derivative [quotations]'\n",
    "      fig['layout']['yaxis2']['title']='Derivative [' + action + ']'\n",
    "      fig['layout']['yaxis2']['side']= 'right'\n",
    "      fig['layout']['yaxis2']['title']['font']['family'] = 'Helvetica'\n",
    "      fig['layout']['xaxis2']['title']['font']['family'] = 'Helvetica'\n",
    "      fig['layout']['titlefont']['family'] = 'Helvetica'\n",
    "\n",
    "      fig.update_layout(height=1200)\n",
    "      return fig\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c7ebd9986d9dd66f5c55cec683961b660fa3bc87c5eb159f7033b074e7bd831"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
