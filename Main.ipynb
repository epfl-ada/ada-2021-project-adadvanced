{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone 2\n",
    "\n",
    "Here we will describe the whole pipeline to get all the results we would like to include in the final story (on the final website). We will go through all the different steps and describe as detailed as possible the operations needed. \n",
    "\n",
    "For the final story we decided to focus on the influence of the Brexit. More precisely we would like to assess how the Brexit was perceived and how it evolves along the years. All the different visualizations we aim at providing in the final story are well detailed in this [Section](#Results).\n",
    "\n",
    "## **[Preprocessing steps](#Preprocessing)**\n",
    "\n",
    "As usual the first step consist in several substeps that aims at cleaning and transforming the data. By clicking on the task link, you can access the respective pipeline.\n",
    "- *[Data exploration and Sanity check](#Sanity_check)* : Explore the dataset, check its consistency and get familiar with the different features/information provided into.\n",
    "    - Collaborators assigned to that task: ALL.\n",
    "- *[Data extraction](#extraction)* : Extract the datas of interest that will be further used to perform the tasks related to each idea.\n",
    "    - Collaborators assigned to that task: Arnaud.\n",
    "- *[Data augmentation](#augmentation)* : Perform a data augmentation to get more features about the quotations such as the quote field, the nationality of the speaker and so on... These new features will be further used to perform the tasks related to each idea.\n",
    "    - Collaborators assigned to that task: Jean & Gaelle. \n",
    "- *[Quotations and speakers clustering](#clustering)* : Cluster the quotations and the speakers according to the a quotation vector and the added features (data augmentation). This clustering will be further mainly used to develop a recommandation tool.\n",
    "    - Collaborators assigned to that task: Raffaele.\n",
    "\n",
    "## **[Generate the results for the final story](#Results)**\n",
    "\n",
    "- [General Statitics](#Statistics) : \n",
    "- [Country map](#Country) : \n",
    "- [Sector map](#Sector) : \n",
    "- [Visualize speakers evolution](#2Dplot) :\n",
    "- [Recommandation Tool](#Recommandation) :\n",
    "- [Correlation with stocks](#Stocks) :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before diving into the code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have a **Data** Folder containing all the quotebank datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import useful librairies and define useful librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2 \n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "PATHS_TO_FILE = ['Data/quotes-20%d.json.bz2' % i for i in range(20,21)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Preprocessing'></a>\n",
    "\n",
    "# Preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Sanity_check'></a>\n",
    "\n",
    "## Data exploration and Sanity check\n",
    "\n",
    "We decided to perform some sanity check in the datas: \n",
    "\n",
    "- We first check that each entry for each quotation is specified in the right format (e.g. `numOccurences` should be an integer).\n",
    "- We check that the `probas` sum to 1.\n",
    "- We check that the `numOccurences` is superior or equal to the length of the list containing the urls.$\n",
    "- The `date` is consistent with the dataset they are coming from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECK FUNCTIONS\n",
    "\n",
    "def check_type(instance,entry,dtype):\n",
    "    return type(instance[entry]) == dtype\n",
    "\n",
    "def check_probas(instance):\n",
    "    # TO BE DEFINED\n",
    "    return None\n",
    "\n",
    "def check_numOcc(instance):\n",
    "    # TO BE DEFINED\n",
    "    return None\n",
    "\n",
    "def check_date(instance,year):\n",
    "    # TO BE DEFINED\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the types for each entry\n",
    "TYPES = {\"quoteID\":str,\n",
    "         \"quotation\":str,\n",
    "         \"speaker\":str,\n",
    "         \"qids\":list,\n",
    "         \"date\":str,\n",
    "         \"numOccurrences\":int,\n",
    "         \"probas\":list,\n",
    "         \"urls\":list,\n",
    "         \"phase\":str}\n",
    "\n",
    "error_dic = {'Data/quotes-20%d.json.bz2' % i : [] for i in range(20,21)}\n",
    "\n",
    "# Loop over the different files that we will read\n",
    "for quotebank_data in PATHS_TO_FILE:\n",
    "    # Open the file we want to read\n",
    "    with bz2.open(quotebank_data, 'rb') as s_file:\n",
    "        # Loop over the samples\n",
    "        for instance in s_file:\n",
    "            # Loading a sample\n",
    "            instance = json.loads(instance)\n",
    "            #### CHECK THE TYPES ####\n",
    "            for key, value in TYPES.items():\n",
    "                if not check_type(instance,key,value):\n",
    "                    error_dic[quotebank_data].append(instance[\"quoteIDS\"] + \": \" + key + \" type problem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='extraction'></a>\n",
    "\n",
    "## Data extraction\n",
    "\n",
    "As mentionned previously, we are planning to analyze the influence of Brexit on different branch as well as analyzing the evolution of feelings towards China. To be able to perform such tasks, we need first to extract the quotations that are talking from Brexit and the ones that are talking about China. To do so we will follow the following pipeline:\n",
    "\n",
    "1. Both for Brexit and China, define a neighborhood containing all the words that are respectively closely related to Brexit and China. This neighborhood will be a list of words or expressions that are commonly used to refer to Brexit or China. For instance, for China one could actually add to the vocabulary neighborhood the *\"the Middle Kingdom\"* expression that is often used to refer to China.\n",
    "2. Both for Brexit and China, select all the quotations for which, at least, one word/expression from the vocabulary neighborhood appears in it.\n",
    "3. Store the new two datasets in the following files: \n",
    "    - `Brexit_quotes.json.bz2`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE BREXIT VOCAB\n",
    "\n",
    "Brexit_vocab = [\"Brexit\",\"brexit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file\n",
    "PATHS_TO_FILE = ['Data/quotes-20%d.json.bz2' % i for i in range(15,21)]\n",
    "# Output file\n",
    "PATH_TO_OUT = 'Brexit_datas/Brexit_quotes.json.bz2'\n",
    "\n",
    "# Open the file where we will write\n",
    "with bz2.open(PATH_TO_OUT, 'wb') as d_file:\n",
    "    # Loop over the different files that we will read\n",
    "    for quotebank_data in PATHS_TO_FILE:\n",
    "        # Open the file we want to read\n",
    "        with bz2.open(quotebank_data, 'rb') as s_file:\n",
    "            # Loop over the samples\n",
    "            for instance in s_file:\n",
    "                # Loading a sample\n",
    "                instance = json.loads(instance)\n",
    "                # Extracting the quotation\n",
    "                quotation = instance['quotation']\n",
    "                # Check if the quotation contains at least one word related to Brexit\n",
    "                if any(substring in quotation for substring in Brexit_vocab):\n",
    "                    # Writing in the new file\n",
    "                    d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotebank_brexit_test = pd.read_json('Brexit_datas/Brexit_quotes.json.bz2',compression=\"bz2\",lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='augmentation'></a>\n",
    "\n",
    "## Data augmentation\n",
    "\n",
    "When we will generate the results for the final story, we will need more information than the initial features we have. The further analysis will require to have access to other features such as the topic of the quotation, the sentiment that carries the quotation, some information about the author and so on. The main idea is to add new features to the existing dataset or only to the data of interest. To do so, we will follow the following pipeline for each quotation:\n",
    "\n",
    "1. **Add features related to the author** : The first type of features one can add are the ones related to the author. Accessing at its wikipedia page gives us a lot of different information: looking carrefully at wikidata item field let us select some useful features listed below:\n",
    "    - `occupation` tells you the author domain.\n",
    "    - `member of political party` tells you the party at which the author belongs to.\n",
    "    - `educated at` tells you where the author studied.\n",
    "    - `country of citizenship` tells you the nationality of the author.\n",
    "    \n",
    "    These fields may not exist for all authors (as not all the authors are politicians), but we can actually assign a NaN value when the field does not appear for one author.\n",
    "\n",
    "2. **Add computed features** : The second type of features we can add are the ones that are directly derived from the initial ones. We selected a bunch of them that will be useful for further analysis:\n",
    "    - TO BE OPTIONALLY COMPLETED\n",
    "3. **Add features issued from a sentiment analysis** : The last feature we would like to add is the sentiment carried on by the quotation. Initially we were thinking about a binary sentiment classification: 0 if the sentiment is negative, 1 if it is positive. We could further expand that by classifying the quotations into several categories such as *anger*, *sadness*, *factual* and so on...    \n",
    "Performing such a text classification task can actually be done using pretrained Deep Neural Networks. XLNet network ([GitHub page](https://github.com/zihangdai/xlnet/) & [Library containing XLNet](https://huggingface.co/transformers/model_doc/xlnet.html)) is close to the state of the art algorithm for classification. Therefore we plan to use it to determine the sentiment contained in each quotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO THE DATA AUGMENTATION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load speaker_attributes.parquet file that contains attributes in terms of QIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attributes = pd.read_parquet('Data/speaker_attributes.parquet')\n",
    "df_attributes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are not interested in the aliases, lastrevid, US_congress_bio_ID, id, candidacy and type.\n",
    "\n",
    "keep_attributes = ['label', 'date_of_birth', 'nationality', 'gender', 'ethnic_group', 'occupation', 'party', 'academic_degree', 'religion']\n",
    "df_attributes = df_attributes[keep_attributes].set_index('label')\n",
    "df_attributes.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For speaker attributes, map the QIDs to meaningful labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionnary to use it as a lookup table \n",
    "\n",
    "df_map = pd.read_csv('Data/wikidata_labels_descriptions_quotebank.csv.bz2', compression='bz2', index_col='QID')\n",
    "map_dict = df_map.Label.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(QIDs):\n",
    "    \"\"\"The purpose of this function is to map all the QIDs to their labels, using wikidata_labels_descriptions_quotebank.csv\"\"\"\n",
    "    \n",
    "    if QIDs is None:\n",
    "        return pd.NA\n",
    "    else:\n",
    "        QIDs_mapped = []\n",
    "        for QID in QIDs:\n",
    "            try:\n",
    "                QIDs_mapped.append(map_dict[QID])\n",
    "            except KeyError:\n",
    "                continue\n",
    "        return QIDs_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_map = ['nationality', 'gender', 'ethnic_group', 'occupation', 'party', 'academic_degree', 'religion']\n",
    "\n",
    "# A MODIFIER: je sais pas pourquoi la ligne d'en bas ne fonctionne pas. La méthode alternative avec la boucle fonctionne mais c'est pas très propre.\n",
    "# df_attributes[columns_to_map] = df_attributes[columns_to_map].apply(mapping, axis=0)\n",
    "for column in columns_to_map:\n",
    "    df_attributes[column] = df_attributes[column].apply(mapping)\n",
    "    \n",
    "df_attributes.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add sentiment score to quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotebank_brexit = pd.read_json('Data/Brexit_quotes.json.bz2', compression='bz2', lines=True)\n",
    "quotebank_brexit.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_score(quote):\n",
    "    \"\"\"The purpose of this function is to use the sentiment analysis tool VADER to find the sentiment associated with a quote.\"\"\"\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sid.polarity_scores(quote)\n",
    "    \n",
    "    # The Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between\n",
    "    # -1(most extreme negative) and +1 (most extreme positive).\n",
    "    # positive sentiment : (compound score >= 0.05) \n",
    "    # neutral sentiment : (compound score > -0.05) and (compound score < 0.05) \n",
    "    # negative sentiment : (compound score <= -0.05)\n",
    "    # see https://predictivehacks.com/how-to-run-sentiment-analysis-in-python-using-vader/\n",
    "    # or https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/\n",
    "    \n",
    "    # decide sentiment as positive, negative and neutral\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        return \"Positive\"\n",
    " \n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        return \"Negative\" \n",
    " \n",
    "    else :\n",
    "        return \"Neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotebank_brexit['sentiment_score'] = quotebank_brexit['quotation'].apply(sent_score) \n",
    "quotebank_brexit.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge both dataframes to obtain final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_quotebank_brexit = pd.merge(quotebank_brexit, df_attributes, 'inner', left_on='speaker', right_index=True)\n",
    "augmented_quotebank_brexit.head(2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "augmented_quotebank_brexit.speaker.value_counts().to_frame().to_csv('out.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Clean dataset\n",
    "\"\"\"\n",
    "\n",
    "# ensure columns are present\n",
    "columns_to_map = [item for item in columns_to_map if item in list(augmented_quotebank_brexit)]\n",
    "\n",
    "def make_hashable(value):\n",
    "    if isinstance(value, list):\n",
    "      if len(value) >= 1:\n",
    "        value = str(value[0])\n",
    "    return value\n",
    "\n",
    "for label in columns_to_map:\n",
    "  augmented_quotebank_brexit[label] = augmented_quotebank_brexit[label].apply(make_hashable)\n",
    "\n",
    "augmented_quotebank_brexit = augmented_quotebank_brexit.replace(pd.NA, np.nan)\n",
    "\n",
    "augmented_quotebank_brexit.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clustering'></a>\n",
    "\n",
    "## Quotations and speakers clustering\n",
    "\n",
    "The last preprocessing step consist in clustering the quotations as well as the speakers, this clustering will then be used to create a Recommandation Tool in the context of Brexit. The idea would be to first cluster the quotations and then the speakers such that two quotations/speakers that are in the same cluster are quotations/speakers carries on similar things/ideas. Performing such a task can be done following this pipeline:\n",
    "1. The first step is to convert sentences into vectors to be able to further perform the clustering. This task can be achieved using the [SentenceTransformer](https://www.sbert.net/docs/usage/semantic_textual_similarity.html) deep neural network. The vector obtained from this operation cab be then concatenated with the other existing features (that would be converted to one hot vectors if necessary).\n",
    "2. The second step consists in reducing the dimension of the datas before applying the clustering algorithm. This task can be achieved using the [T-stochastic neighbors embeddings](#https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) algorithm or the [Locally Linear Embeddings](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding) algorithm. These two techniques (specially the first one) are efficient non-linear dimensionality reduction methods.\n",
    "3. The third step is specific to speaker clustering. Indeed the vectorization of quotes as well as the reduction of dimensionality is only applied to quotes. Thus we need to perform an **aggregation** to be able to attribute a vector to each speaker. For each speaker, this aggregation can simply be done by taking the mean of the vectors associated with each of their quotations. \n",
    "4. The last step consist in performing the clustering operation. This task can be achieved using [Gaussian Mixture Model](https://scikit-learn.org/stable/modules/mixture.html#mixture) algorithm or  [Spectral Clustering](#https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Special imports for the task\n",
    "\"\"\"\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from tsne_torch import TorchTSNE as TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    User-defined parameters for the task\n",
    "\"\"\"\n",
    "tsne_dim = 2      # TSNE reduction final dimension, default is 2\n",
    "nb_clusters = 8   # Number of clusters to be identified\n",
    "sentence_transformer_type = 'all-MiniLM-L6-v2' # type of the sentence_transformer\n",
    "\n",
    "\"\"\"\n",
    "    As function of augmented_quotebank_brexit table,\n",
    "    prepare useful maps and tools\n",
    "\"\"\"\n",
    "\n",
    "# get a map of unique possible values for all speaker informations\n",
    "# info_label (ex: nationality) -> ['english', 'swiss', 'italian', 'french'] \n",
    "# TODO: deal with Nan in the clustering model\n",
    "speaker_info_map ={}\n",
    "for info_label in columns_to_map:\n",
    "    speaker_info_map[info_label] = list(pd.unique(list(augmented_quotebank_brexit[info_label])))\n",
    "    \n",
    "# Quotation encoder into\n",
    "encoder = SentenceTransformer(sentence_transformer_type)\n",
    "\n",
    "# get the list of available speakers\n",
    "speakers = pd.unique(augmented_quotebank_brexit['speaker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Given a list or dataframe of classes, return a pytorch tensor\n",
    "    corresponding to one-hot transformation of the values relatively\n",
    "    to the passed classes \n",
    "\n",
    "    TODO: manage Nan differently\n",
    "\"\"\"\n",
    "def column_to_one_hot(values, classes):\n",
    "    N = len(values)\n",
    "    onehot = torch.zeros(N, len(classes))\n",
    "\n",
    "    for (i,v) in zip(range(N), values):\n",
    "        if not pd.isna(v):\n",
    "          location = classes.index(v)\n",
    "          onehot[i, location] = 1.\n",
    "      \n",
    "    return onehot\n",
    "\n",
    "\"\"\"\n",
    "    Core function: speaker vectorizer\n",
    "    \n",
    "    Arguments: \n",
    "        df = pandas dataframe containing quotations and speaker informations\n",
    "        speaker = speaker name \n",
    "\"\"\"\n",
    "def encode_speaker(df, speaker):\n",
    "\n",
    "    df = df.loc[df['speaker'] == speaker]\n",
    "\n",
    "    onehot_info_tensor = torch.zeros((len(df['sentiment_score']), 1))\n",
    "\n",
    "    onehot_info_tensor[list(df['sentiment_score']) ==  \"Positive\", 0] = 1.\n",
    "    onehot_info_tensor[list(df['sentiment_score']) ==  \"Negative\", 0] = -1.\n",
    "\n",
    "    for label_info, classes in speaker_info_map.items():\n",
    "        onehot_oneclass = column_to_one_hot(df[label_info].values, classes)\n",
    "        onehot_info_tensor = torch.cat((onehot_info_tensor, onehot_oneclass), 1)\n",
    "    \n",
    "    # get quotation tensor\n",
    "    quotes_tensor = encoder.encode(df['quotation'].values, convert_to_tensor=True)\n",
    "    full_tensor = torch.cat((onehot_info_tensor, quotes_tensor), 1)\n",
    "    \n",
    "    # take mean over rows\n",
    "    return torch.mean(full_tensor, 0, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all data into a pytorch tensor NxD\n",
    "# N = number of speakers (samples)\n",
    "# D = dimension of a full speaker-encoded vector\n",
    "\n",
    "# D is initially unknown\n",
    "full_data_tensor = None\n",
    "\n",
    "# for each speaker, encode it to a vector and attach it to a pytorch tensor\n",
    "for speaker in speakers:\n",
    "    speaker_vector = encode_speaker(augmented_quotebank_brexit, speaker)\n",
    "    \n",
    "    if full_data_tensor is None:\n",
    "        full_data_tensor = speaker_vector\n",
    "    else:\n",
    "        full_data_tensor = torch.cat((full_data_tensor, speaker_vector), 0)\n",
    "\n",
    "print(\"Tensor size: \", full_data_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply T-stochastic neighboor embedding\n",
    "data_tensor_emb = TSNE(n_components=tsne_dim, perplexity=30, n_iter=1000).fit_transform(full_data_tensor) # dim = Nxfinal_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Clustering\n",
    "clustering = SpectralClustering(nb_clusters).fit(speaker_tensor_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last step: data visualization\n",
    "\n",
    "fig, axis = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "results = data_tensor_emb.transpose()\n",
    "\n",
    "# Visualize without clustering\n",
    "axis[0].scatter(results[0], results[1])\n",
    "\n",
    "\n",
    "for label in range(nb_clusters):\n",
    "    # select data by clustering label\n",
    "    points = data_tensor_emb[clustering.labels_ == label]\n",
    "    points = points.transpose()\n",
    "    # plot data\n",
    "    axis[1].scatter(points[0], points[1])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Results'></a>\n",
    "\n",
    "# Generate the results for the final story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Brexit'></a>\n",
    "\n",
    "## Analysis of the way Brexit is perceived\n",
    "\n",
    "Recall that the goal is to analyze the way Brexit is perceived in each Europe country and in each branch (economy) based on the sentiment carried by the quotation. Besides we would like to add the time dimension to this analysis, meaning that we would like to follow the evolution of the overall feelings towards Brexit. A view of the expected result is given below:\n",
    "\n",
    "Sector Analysis | Country Analysis\n",
    "![alt text](Images/brexit_bubbles.png \"Sector analysis\") | ![alt text](Images/brexit_expected_outcomes.png \"Country analysis\")\n",
    "\n",
    "### *Pipeline*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Statistics'></a>\n",
    "\n",
    "## General Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Country'></a>\n",
    "\n",
    "## Analyze the way Brexit is perceived in European countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Sector'></a>\n",
    "\n",
    "## Analyze the way Brexit is perceived in different sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2Dplot'></a>\n",
    "\n",
    "## Visualize speakers orientation trough a 2D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Recommandation'></a>\n",
    "\n",
    "## Recommandation tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Stocks'></a>\n",
    "\n",
    "## Correlation with stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c7ebd9986d9dd66f5c55cec683961b660fa3bc87c5eb159f7033b074e7bd831"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
