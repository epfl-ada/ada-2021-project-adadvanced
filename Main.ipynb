{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone 2\n",
    "\n",
    "Here we will describe the whole pipeline to get all the results we would like to include in the final story (on the final website). We will go through all the different steps and describe in detail the operations needed. \n",
    "\n",
    "For the final story we decided to focus on the influence of the Brexit. More precisely we would like to assess how the Brexit was perceived and how it evolved over the years. The visualizations we aim at providing in the final story are detailed in this [Section](#Results).\n",
    "\n",
    "## **[Preprocessing steps](#Preprocessing)**\n",
    "\n",
    "As usual the first step consists in several substeps that aims at cleaning and transforming the data. By clicking on the task link, you can access the respective pipeline.\n",
    "- *[Data exploration and Sanity check](#Sanity_check)* : Explore the dataset, check its consistency and get familiar with the different features/information provided.\n",
    "- *[Data extraction](#extraction)* : Extract the data of interest that will be used to perform the tasks related to each idea.\n",
    "- *[Data augmentation](#augmentation)* : Perform a data augmentation to get more features about the quotations such as the quote field, the nationality of the speaker and so on... These new features will be used to perform the tasks related to each idea.\n",
    "- *[Data cleaning and merging](#augmentation)* : Perform a final cleaning on the quotations as well as on the speakers and generate 3 main datasets that will be used for the analysis\n",
    "- *[Quotations and speakers clustering](#clustering)* : Cluster the quotations and the speakers according to the quotation vector and the added features in the data augmentation task. This clustering will be mainly used to develop a recommandation tool.\n",
    "\n",
    "## **[Generate the results for the final story](#Results)**\n",
    "\n",
    "- [General Statitics](#Statistics) : Explore the dataset, visualize some first graphs for each new features.  \n",
    "- [Country map](#Country) : Show how brexit is perceived depending on the country.\n",
    "- [Sector map](#Sector) : Show how brexit is perceived depending on the sector.\n",
    "- [Visualize speakers evolution](#2Dplot) : Visualize speakers into an embedding space that should reflect the similarities between speakers **[TO BE COMPLETED]**.\n",
    "- [Recommandation Tool](#Recommandation) : Tool that recommends similar speakers to the one searched by the user **[TO BE COMPLETED]**. \n",
    "- [Correlation with stocks](#Stocks) : Study if a correlation exists between remarkable Brexit peaks and the stock actions from companies of the FTSE100 **[TO BE COMPLETED]**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before diving into the code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run everything from scratch, make sure to have a `Data` folder containing the following files: \n",
    "- The quotebank datasets for each year: `quotes-yyyy.json.bz2`\n",
    "- The speaker attributes folder `speaker-attributes.parquet` as well as the associated lookup table `wikidata_labels_descriptions_quotebank.csv.bz2`\n",
    "\n",
    "To benefit from check points, download `Brexit_datas` from [Google drive](https://drive.google.com/drive/folders/12EgO7E97KcNrZtQhjUmkOp5iDF1V7ufR?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import useful librairies and define useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARD LIBRAIRIES\n",
    "from os.path import exists\n",
    "import bz2 \n",
    "import json\n",
    "import geojson\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "# Dynamic graphs\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import dash\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "# Machine learning librairies\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import SpectralClustering, DBSCAN, AgglomerativeClustering\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "#from tsne_torch import TorchTSNE as TSNE\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Graph algorithms\n",
    "import networkx as nx\n",
    "\n",
    "# import string distances\n",
    "import stringdist\n",
    "\n",
    "# Load the lexicon for sentiment analysis\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data files\n",
    "PATHS_TO_FILE = ['Data/quotes-20%d.json.bz2' % i for i in range(15,21)]\n",
    "\n",
    "# Columns to analyte for one-hot vectorization task\n",
    "columns_to_map = ['nationality', 'gender', 'occupation', 'party', 'academic_degree', 'religion']\n",
    "\n",
    "# time discretization by periods\n",
    "period_labels = [\"< 2018\", \"2018\", \"early 2019\", \"late 2019 - 2020\"]\n",
    "\n",
    "# type of the sentence_transformer\n",
    "sentence_transformer_type = 'all-MiniLM-L6-v2' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Preprocessing'></a>\n",
    "\n",
    "# Preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Sanity_check'></a>\n",
    "\n",
    "## Data exploration and Sanity check\n",
    "\n",
    "We decided to perform the following snaity checks on the original data: \n",
    "\n",
    "- We first check that each entry for each quotation is specified in the right format (e.g. `numOccurences` should be an integer).\n",
    "- We check that the `probas` sum to 1.\n",
    "- We check that the `numOccurences` is superior or equal to the length of the list containing the urls.\n",
    "- The `date` is consistent with the dataset they are coming from\n",
    "- We check that if a `qids` exists then a `speaker` should be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECK FUNCTIONS\n",
    "\n",
    "def check_type(instance,entry,dtype):\n",
    "    return type(instance[entry]) == dtype\n",
    "\n",
    "def check_probas(instance):\n",
    "    if len(instance) > 0:\n",
    "        proba_sum = sum([float(potential[1]) for potential in instance[\"probas\"]])\n",
    "        if proba_sum < 0.98 or proba_sum > 1.02:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_numOcc(instance):\n",
    "    return (len(instance[\"urls\"]) <= instance[\"numOccurrences\"])\n",
    "\n",
    "def check_date(instance,year):\n",
    "    quotation_year = int(instance[\"date\"][:4])\n",
    "    return (quotation_year == year)\n",
    "\n",
    "def check_author_qids(instance):\n",
    "    if len(instance[\"qids\"]) > 0 and instance[\"speaker\"] is None:\n",
    "        return False\n",
    "    else: \n",
    "        return True\n",
    "        \n",
    "# CONVERSION FUNCTIONS\n",
    "\n",
    "# determine the supported device\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu') # don't have GPU \n",
    "    return device\n",
    "\n",
    "# convert a df to tensor to be used in pytorch\n",
    "def df_to_tensor(df):\n",
    "    device = get_device()\n",
    "    return torch.from_numpy(df.values).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the types for each entry\n",
    "TYPES = {\"quoteID\":str,\n",
    "         \"quotation\":str,\n",
    "         \"speaker\":str,\n",
    "         \"qids\":list,\n",
    "         \"date\":str,\n",
    "         \"numOccurrences\":int,\n",
    "         \"probas\":list,\n",
    "         \"urls\":list,\n",
    "         \"phase\":str}\n",
    "\n",
    "error_file = \"Data/error_file.json.bz2\"\n",
    "\n",
    "\n",
    "if not exists(error_file):\n",
    "    with bz2.open(error_file, 'wb') as e_file:\n",
    "        # Loop over the different files that we will read\n",
    "        for quotebank_data in PATHS_TO_FILE:\n",
    "            year = int(quotebank_data[-13:-9])\n",
    "            print(\"Reading \",quotebank_data,\" file...\")\n",
    "            # Open the file we want to read\n",
    "            with bz2.open(quotebank_data, 'rb') as s_file:\n",
    "                # Loop over the samples\n",
    "                for instance in s_file:\n",
    "                    potential_error = \"\"\n",
    "                    # Loading a sample\n",
    "                    instance = json.loads(instance)\n",
    "                    #### CHECK THE TYPES ####\n",
    "                    for key, value in TYPES.items():\n",
    "                        if not check_type(instance,key,value):\n",
    "                            potential_error += \"| Type problem: \" + key + \" |\"\n",
    "                            # Continue because there exists a problem with the type that may affect the other checks\n",
    "                            continue\n",
    "                    #### CHECK THE PROBAS ####\n",
    "                    if not check_probas(instance):\n",
    "                        potential_error += \"| Probas problem |\"\n",
    "                    #### CHECK THE DATE ####\n",
    "                    if not check_date(instance,year):\n",
    "                        potential_error += \"| Date problem |\"\n",
    "                    #### CHECK THE NUMOCCURENCES ####\n",
    "                    if not check_numOcc(instance):\n",
    "                        potential_error += \"| NumOccurences problem |\"\n",
    "                    #### CHECK THE AUTHOR-QIDS ####\n",
    "                    if not check_author_qids(instance):\n",
    "                        potential_error += \"| Author-qids problem |\"\n",
    "                    # WRITE INTO THE FILE FOR POTENTIAL ERRORS #\n",
    "                    if len(potential_error) > 0:\n",
    "                        instance[\"error\"] = potential_error\n",
    "                        e_file.write((json.dumps(instance)+'\\n').encode('utf-8'))\n",
    "\n",
    "pd.read_json('Data/error_file.json.bz2',compression=\"bz2\",lines=True).shape                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='extraction'></a>\n",
    "\n",
    "## Data extraction\n",
    "\n",
    "As mentionned previously, we are planning to analyze the way Brexit is perceived. Thus, we need to extract first the quotations that discuss Brexit. To do so we will follow the following pipeline:\n",
    "\n",
    "1. Select all the quotations that contain the word Brexit.\n",
    "2. Store the new two dataset in the `Brexit_quotes.json.bz2` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists('Brexit_datas/Brexit_quotes.json.bz2'):\n",
    "    # Input file\n",
    "    PATHS_TO_FILE = ['Data/quotes-20%d.json.bz2' % i for i in range(15,21)]\n",
    "    # Output file\n",
    "    PATH_TO_OUT = 'Brexit_datas/Brexit_quotes.json.bz2'\n",
    "\n",
    "    # Open the file where we will write\n",
    "    with bz2.open(PATH_TO_OUT, 'wb') as d_file:\n",
    "        # Loop over the different files that we will read\n",
    "        for quotebank_data in PATHS_TO_FILE:\n",
    "            print(\"Reading \",quotebank_data,\" file...\")\n",
    "            # Open the file we want to read\n",
    "            with bz2.open(quotebank_data, 'rb') as s_file:\n",
    "                # Loop over the samples\n",
    "                for instance in s_file:\n",
    "                    # Loading a sample\n",
    "                    instance = json.loads(instance)\n",
    "                    # Extracting the quotation\n",
    "                    quotation = instance['quotation']\n",
    "                    # Check if the quotation contains at least one word related to Brexit\n",
    "                    if \"brexit\" in quotation.lower():\n",
    "                        # Writing in the new file\n",
    "                        d_file.write((json.dumps(instance)+'\\n').encode('utf-8'))\n",
    "\n",
    "quotebank_brexit = pd.read_json('Brexit_datas/Brexit_quotes.json.bz2',compression=\"bz2\",lines=True)\n",
    "quotebank_brexit.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='augmentation'></a>\n",
    "\n",
    "## Data augmentation\n",
    "\n",
    "When we will generate the results for the final story, we will need more information than the initial features provided. The further analysis will require to have access to other features such as the sentiment carried by the quotation and additional information about the author. To do so, the following pipeline will be performed on each quotation:\n",
    "\n",
    "1. **[Adding features related to the author](#Features_Author)** :  Using the provided file `speaker_attributes.parquet` that was extracted from the Wikidata knowledge base, the following attributes are of interest for each speaker:\n",
    "    - `occupation`: describes the author's occupancy \n",
    "    - `party` identifies the political affiliation of the speaker.\n",
    "    - `academic_degree` gives information about the education of the author as well as their alma mater.\n",
    "    - `nationality` identifies the citizenship(s) of the author.\n",
    "    - `date_of_birth`: identifies the date of birth of the speaker.\n",
    "    - `gender`: identifies the gender of the speaker.\n",
    "    - `ethnic_group`: identifies the ethnic group of the speaker.\n",
    "    - `religion`: identifies the religion of the speaker. \n",
    "\n",
    "    The provided `speaker_attributes.parquet` file contains attributes in terms of QIDs, thereby being uninterpretable by humans. To map the QIDs to meaningful labels, we used the provide the file `wikidata_labels_descriptions_quotebank.csv.bz`.\n",
    "    \n",
    "    The aforementioned attributes may not be available for all authors. When it is the case, a NaN value is assigned.\n",
    "\n",
    "2. **[Adding features issued from a sentiment analysis](#Sentiment_Quote)** : The last feature of interest is the sentiment that is carried by the quotation. For the sake of simplicity, each quotation will be classified into three different categories: *Negative*, *Neutral* and *Positive*. \n",
    "Sentiment Analysis task can be performed using pretrained Deep Neural Networks. We decided to use **Vader** Neural network for its good performance. NLTK's Vader sentiment analysis tool uses a bag of words approach with some simple heuristics. More on it [here](https://github.com/cjhutto/vaderSentiment). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Features_Author'></a>\n",
    "***Loading the speaker_attributes.parquet file***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parquet that contains the information about speakers\n",
    "df_attributes = pd.read_parquet('Data/speaker_attributes.parquet')\n",
    "\n",
    "# we are not interested in the aliases, lastrevid, US_congress_bio_ID, id, candidacy and type.\n",
    "keep_attributes = ['id','label', 'date_of_birth', 'nationality', 'gender', 'ethnic_group', 'occupation', 'party', 'academic_degree', 'religion']\n",
    "# Set the index\n",
    "df_attributes = df_attributes[keep_attributes].set_index('id')\n",
    "# Sanity check for the qids\n",
    "print(\"Sanity check ok ? : \",df_attributes.index.is_unique)\n",
    "# Let's have a look\n",
    "df_attributes.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Features_Author'></a>\n",
    "***Mapping the QIDs to meaningful labels***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionnary to use it as a lookup table \n",
    "df_map = pd.read_csv('Data/wikidata_labels_descriptions_quotebank.csv.bz2', compression='bz2', index_col='QID')\n",
    "# Dictionnary where qids are keys and values are corresponding element\n",
    "map_dict = df_map.Label.to_dict()\n",
    "\n",
    "def mapping(QIDs):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to map all the QIDs to their labels, \n",
    "    using wikidata_labels_descriptions_quotebank.csv\n",
    "    \"\"\"\n",
    "    \n",
    "    if QIDs is None:\n",
    "        return np.nan\n",
    "    else:\n",
    "        QIDs_mapped = []\n",
    "        for QID in QIDs:\n",
    "            try:\n",
    "                # If a correspondance exists\n",
    "                QIDs_mapped.append(map_dict[QID])\n",
    "            except KeyError:\n",
    "                # If no correspondance exits\n",
    "                continue\n",
    "        # If nothing was extracted\n",
    "        if len(QIDs_mapped) == 0:\n",
    "            return np.nan\n",
    "        # Things extracted\n",
    "        else:\n",
    "            return QIDs_mapped\n",
    "\n",
    "\n",
    "\n",
    "# For each column perform the mapping to transform qids to real value\n",
    "for column in columns_to_map:\n",
    "    df_attributes[column] = df_attributes[column].apply(mapping)\n",
    "    \n",
    "df_attributes.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Sentiment_Quote'></a>\n",
    "***Adding sentiment score to each quote***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_score(quote):\n",
    "    \"\"\"The purpose of this function is to use the sentiment analysis tool VADER to find the sentiment associated with a quote.\"\"\"\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sid.polarity_scores(quote)\n",
    "    \n",
    "    # The Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between\n",
    "    # -1(most extreme negative) and +1 (most extreme positive).\n",
    "    # positive sentiment : (compound score >= 0.05) \n",
    "    # neutral sentiment : (compound score > -0.05) and (compound score < 0.05) \n",
    "    # negative sentiment : (compound score <= -0.05)\n",
    "    # see https://predictivehacks.com/how-to-run-sentiment-analysis-in-python-using-vader/\n",
    "    # or https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/\n",
    "    \n",
    "    # decide sentiment as positive, negative and neutral\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        return \"Positive\"\n",
    " \n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        return \"Negative\" \n",
    " \n",
    "    else :\n",
    "        return \"Neutral\"\n",
    "\n",
    "# backup quotebank dataframe with sentiment score if the corresponding file doesn't exists\n",
    "if not exists(\"Brexit_datas/quotebank_brexit_with_sentiment.json.bz2\"):\n",
    "    quotebank_brexit['sentiment_score'] = quotebank_brexit.quotation.apply(sent_score) \n",
    "    quotebank_brexit.to_json(\"Brexit_datas/quotebank_brexit_with_sentiment.json.bz2\")\n",
    "    \n",
    "else:\n",
    "    quotebank_brexit = pd.read_json(\"Brexit_datas/quotebank_brexit_with_sentiment.json.bz2\",compression=\"bz2\")\n",
    "\n",
    "quotebank_brexit.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleaning'></a>\n",
    "\n",
    "## Data merging and cleaning\n",
    "\n",
    "Depending on the different tasks we want to perform we will need to have the dataset in various forms, thus we will generate three types of dataset: \n",
    "- `quotebank_brexit`: original dataset [+sentiment score] where dublicated quotations are removed.\n",
    "- `aug_quotebank_brexit`: dataset with augmented data where both quotations and speakers are cleaned.\n",
    "- `oneh_quotebank_brexit`: copy of `aug_quotebank_brexit` where categorical values are one-hot encoded.\n",
    "\n",
    "Thus we first start by [cleaning the quotations](#cleaning_quotation) in the `quotebank_brexit` dataset and then we [clean the speakers](#cleaning_speaker) to be able to merge with augmented data and generate the `aug_quotebank_brexit`. After a [processing](#aug_preprocessing) of the `aug_quotebank_brexit` we will finally [one hot encode](#one_hot_encoding) to generate the `oneh_quotebank_brexit` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleaning_quotation'></a>\n",
    "\n",
    "### Cleaning the quotations\n",
    "\n",
    "We noticed that some quotations were very similar, actually too similar. They sometimes differ from the fact that one quotation is nested in another or sometimes they only differ by one character. Here is an example of such a quotation:\n",
    "- quoteID: **2018-01-26-042810** - *\"I look at Nigel Farage's example. It took 17 years, but Brexit came,\"*\n",
    "- quoteID: **2018-01-26-042811** - *\"I look at Nigel Farage's example. It took 17 years, but Brexit came. I don't plan to wait that long\"*\n",
    "\n",
    "We need to remove these kind of *duplicates*. To do so we followed this pipeline:\n",
    "- Converting quotations into vectors using [SentenceTransformer](https://www.sbert.net/docs/usage/semantic_textual_similarity.html) deep neural network.\n",
    "- Computing [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between each pair of quotations\n",
    "- Removing quotations that are too similar from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode quotation \n",
    "if not exists(\"Brexit_datas/vector_quotes.csv.gz\"):\n",
    "    encoder = SentenceTransformer(sentence_transformer_type)\n",
    "    # Encode quotations\n",
    "    quotes_encoded = encoder.encode(quotebank_brexit['quotation'].values, convert_to_numpy=True, show_progress_bar=True)\n",
    "    # Convert to df\n",
    "    quotes_df = pd.DataFrame(quotes_encoded, index = quotebank_brexit.index)\n",
    "    # Add significant columns\n",
    "    quotes_df[\"speaker\"] = quotebank_brexit[\"speaker\"].values\n",
    "    # Export into a compressed format\n",
    "    quotes_df.to_csv(\"Brexit_datas/vector_quotes.csv.gz\")\n",
    "    \n",
    "else:\n",
    "    # Read the file\n",
    "    quotes_df = pd.read_csv(\"Brexit_datas/vector_quotes.csv.gz\",index_col=0,compression=\"gzip\")\n",
    "\n",
    "quotes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# compare pairwise similarity\n",
    "def filter_similar(df):\n",
    "    # Get the embeddings computed before\n",
    "    embeddings = df_to_tensor(df.drop(columns='quoteID'))\n",
    "    # Compute cosine similarity\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "    # Convert to df\n",
    "    score = pd.DataFrame(cosine_scores.numpy())\n",
    "    index = set(score[score>0.95].stack().index.tolist())\n",
    "    index = [(a,b) for (a,b) in index if a != b]\n",
    "    # multiple tuples can have common element: need to merge them\n",
    "    graph = nx.Graph(index)\n",
    "    index = list(nx.connected_components(graph))\n",
    "    # map the indices with the Qid of the quote\n",
    "    index = [tuple(df.quoteID.iloc[ind] for ind in path) for path in index]\n",
    "    return index\n",
    "\n",
    "similar_count = quotes_df.assign(quoteID=quotebank_brexit.quoteID.values).groupby('speaker').apply(filter_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of two similar quotations\n",
    "print(\"Yvette Cooper:\", similar_count[\"Yvette Cooper\"][0])\n",
    "quotebank_brexit[(quotebank_brexit.quoteID=='2019-01-27-041304') | (quotebank_brexit.quoteID=='2019-01-27-029003')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices to drop\n",
    "def drop_duplicate_quotes(ids):\n",
    "    return [[quoteID for quoteID in path[1:]] for path in ids]\n",
    "        \n",
    "# generate the list of quoteIDs to be removed\n",
    "to_be_removed = similar_count.apply(drop_duplicate_quotes).values.sum()\n",
    "to_be_removed = list(itertools.chain.from_iterable(to_be_removed))\n",
    "\n",
    "quotebank_brexit = quotebank_brexit[~quotebank_brexit.quoteID.isin(to_be_removed)]\n",
    "\n",
    "print(\"Number of quotations to be removed: \",len(to_be_removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleaning_speaker'></a>\n",
    "\n",
    "### Cleaning the speakers\n",
    "\n",
    "The `aug_quotebank_brexit` provides information about the speaker such as the `nationality`, `occupation`. However one can notice that sometimes the neural network doesn't succeed in finding a speaker and therefore fills `speaker` entry with `None` value. These missing values are difficult to handle as it would require to guess who said the quotation. One could think about training a classifier on the data where the speaker is mentionned but it is actually a fastidious task that we are not able to manage. Unfortunaltely, we decided to remove them from the dataset.\n",
    "\n",
    "An other issue comes from the fact that for one speaker different Qids exist. However, these Qids correspond to the Wikipedia pages of the same person but in different langagues. This could also come from the fact that there exist multiple wikipedia pages that point to different persons who are homonyms. When many qids exist we check if all the attributes are similar for all the qids. If not, then we are not able to determine which qid is the correct one so unfortunately we discard the row from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_consistent_qids(QIDS_original):\n",
    "    QIDS = QIDS_original.copy()\n",
    "    if len(QIDS) == 0:\n",
    "        return pd.NA\n",
    "    elif len(QIDS) == 1:\n",
    "        return QIDS_original[0]\n",
    "    else:\n",
    "        while len(QIDS) > 1:\n",
    "            first_idx = QIDS.pop(-1)\n",
    "            try:\n",
    "                first = df_attributes.loc[first_idx].fillna(0)\n",
    "                second_idx = QIDS.pop(-1)\n",
    "                try:\n",
    "                    second = df_attributes.loc[second_idx].fillna(0)\n",
    "                except KeyError:\n",
    "                    QIDS.append(first_idx)\n",
    "                    continue\n",
    "            except KeyError:\n",
    "                continue\n",
    "            try: \n",
    "                if (first != second).sum() > 0:\n",
    "                    return pd.NA\n",
    "            except ValueError:\n",
    "                return pd.NA\n",
    "        return QIDS_original[0]\n",
    "\n",
    "if not exists(\"Brexit_datas/aug_quotebank.json.bz2\"):\n",
    "    # Remove nan values\n",
    "    aug_quotebank_brexit = quotebank_brexit[quotebank_brexit.speaker != \"None\"]\n",
    "\n",
    "    # Remove speakers with multiple different qids\n",
    "    aug_quotebank_brexit.qids = aug_quotebank_brexit.qids.apply(check_consistent_qids)\n",
    "    aug_quotebank_brexit = aug_quotebank_brexit[~aug_quotebank_brexit.qids.isna()]\n",
    "\n",
    "    # Merge the augmented quotebank brexit with df_attributes on qids\n",
    "    aug_quotebank_brexit = pd.merge(aug_quotebank_brexit, df_attributes, 'inner', left_on=\"qids\", right_index=True)\n",
    "\n",
    "    # Export to json to add check points\n",
    "    aug_quotebank_brexit.to_json(\"Brexit_datas/aug_quotebank.json.bz2\")\n",
    "else:\n",
    "    # Read json if it already exists\n",
    "    aug_quotebank_brexit = pd.read_json(\"Brexit_datas/aug_quotebank.json.bz2\",compression=\"bz2\")\n",
    "\n",
    "# Let's have a look\n",
    "print(\"New shape:\",aug_quotebank_brexit.shape)\n",
    "aug_quotebank_brexit.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute age feature for each speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age(birth_date,current_year=datetime.now().year):\n",
    "    \n",
    "    # Check it is a list \n",
    "    if isinstance(birth_date,list) and len(birth_date) > 0:\n",
    "        # Only get the first 4 digit (year)\n",
    "        birth_year = int(birth_date[0][1:5])\n",
    "        # Return the age\n",
    "        return current_year - birth_year\n",
    "    else:\n",
    "        return pd.NA\n",
    "\n",
    "# Add the age column\n",
    "aug_quotebank_brexit[\"Age\"] = aug_quotebank_brexit.date_of_birth.apply(get_age)\n",
    "\n",
    "aug_quotebank_brexit.loc[:,[\"Age\",\"date_of_birth\",\"speaker\"]].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get unique values of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {}\n",
    "\n",
    "# For each categotical value\n",
    "for col in columns_to_map:\n",
    "    # Get the serie\n",
    "    col_serie = aug_quotebank_brexit[col].copy()\n",
    "    # Get unique values\n",
    "    unique_values[col] = pd.unique(col_serie.apply(pd.Series).stack())\n",
    "    print(col,\" : number of different categories = \",len(unique_values[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aug_preprocessing'></a>\n",
    "\n",
    "### What about the categorical features added\n",
    "\n",
    "### Occupation feature\n",
    "\n",
    "Now that we added new features, we had a look at their values. We noticed that there are more than 800 different occupations. It would be interesting to classify them into *categories*. The problem is that we do not have any label on them and using ML techniques such as pre-trained neural networks would be an over-kill. We rather followed a semi-manual approach that is described below: \n",
    "- Identify which words are the more frequent in the `occupation` names and associate them with a label. We will call them key words.\n",
    "- For each `occupation` match it with any key word labels when applicable.\n",
    "- Label the remaining occupations manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Manage frequent keywords***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame of the occupations\n",
    "occupation_df = pd.DataFrame(unique_values[\"occupation\"],columns=[\"occupation\"])\n",
    "\n",
    "key_words = []\n",
    "\n",
    "# Loop over the occupations\n",
    "for occupation in unique_values[\"occupation\"]:\n",
    "    # Split the occupation string and concatenate\n",
    "    key_words += occupation.split()\n",
    "\n",
    "# Convert to a Dataframe\n",
    "key_words_df = pd.DataFrame(key_words,columns=[\"occupation\"])\n",
    "# Put all strings to lower\n",
    "key_words_df.occupation = key_words_df.occupation.str.lower()\n",
    "# For each key word count the number of occurences and sort by descending\n",
    "key_words_df = key_words_df.groupby(\"occupation\").size().reset_index(name=\"Count\").sort_values(by=\"Count\",ascending=False)\n",
    "\n",
    "# If the classification has not been already done\n",
    "if not exists(\"Brexit_datas/occupation_class/occupation_agg.csv\"):\n",
    "    key_words_df.to_csv(\"Brexit_datas/occupation_class/occupation_agg.csv\")\n",
    "\n",
    "print(\"Look at the most frequent keywords\")\n",
    "print(key_words_df.head(3))\n",
    "\n",
    "answer = input(\"Is the classification of keywords done ?\")\n",
    "\n",
    "if (answer.lower() == \"yes\"):\n",
    "    # Get the classified keywords\n",
    "    key_words_classified = pd.read_csv(\"Brexit_datas/occupation_class/occupation_agg.csv\",index_col=0)\n",
    "    # Get ride of keywords that have not been classified\n",
    "    key_words_classified = key_words_classified.loc[~key_words_classified.Category.isna()]\n",
    "    # Manage the case when several categories have been entered\n",
    "    key_words_classified.Category = key_words_classified.Category.apply(lambda x: x.split(\"-\"))\n",
    "    # let's have a look at the table\n",
    "    print(\"Look at the output table\")\n",
    "    print(key_words_classified.head(3))\n",
    "else:\n",
    "    print(\"Then please classify the keywords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Match occupation and keyword labels***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if keywords are contained in an occupation\n",
    "def check_string_in(occupation):\n",
    "    # Initialize the final list of the supercategories\n",
    "    final_list = []\n",
    "    # Loop over the key_words_classified\n",
    "    for items in key_words_classified.occupation.iteritems():\n",
    "        # If the keyword is contained in the occupation\n",
    "        if items[1] in occupation.lower():\n",
    "            # Concat the supercategories with th existing list\n",
    "            final_list = final_list + key_words_classified.loc[items[0],\"Category\"]\n",
    "    # If no categories return NaN\n",
    "    if len(final_list) == 0:\n",
    "        return pd.NA\n",
    "    # Else return the list without duplicates\n",
    "    else:\n",
    "        return list(set(final_list))\n",
    "        \n",
    "# Apply the function\n",
    "occupation_df[\"Category\"] = occupation_df.occupation.apply(check_string_in)\n",
    "\n",
    "if not exists(\"Brexit_datas/occupation_class/unclassified_occupation.csv\"):\n",
    "    # Export the occupations that have not been classified\n",
    "    occupation_df[occupation_df.Category.isna()].to_csv(\"Brexit_datas/occupation_class/unclassified_occupation.csv\")\n",
    "\n",
    "print(\"Look at the remaining occupations\")\n",
    "print(occupation_df[occupation_df.Category.isna()].head(3))\n",
    "\n",
    "answer = input(\"Is the classification of remaining occupations done ?\")\n",
    "\n",
    "if (answer.lower() == \"yes\"):\n",
    "    # Get the remaining occupations classified\n",
    "    remain_occupations_classified = pd.read_csv(\"Brexit_datas/occupation_class/unclassified_occupation.csv\",index_col=0)\n",
    "    # Merge with the current data frame\n",
    "    occupation_final_df = pd.merge(occupation_df,remain_occupations_classified,how=\"left\",on=\"occupation\",suffixes=(\"\",\"_2\"))\n",
    "    # Split into a list\n",
    "    occupation_final_df.Category_2 = occupation_final_df.Category_2.apply(lambda x: x.split(\"-\") if type(x) == str else pd.NA)\n",
    "    # Merge into a single column\n",
    "    occupation_final_df.loc[~occupation_final_df.Category_2.isna(),\"Category\"] = occupation_final_df.loc[~occupation_final_df.Category_2.isna(),\"Category_2\"]\n",
    "    # Drop the artificial column\n",
    "    occupation_final_df.drop(columns=[\"Category_2\"],inplace=True)\n",
    "    # Drop na values that corresponds to unclassifiable jobs such as nazi hunter\n",
    "    occupation_final_df.dropna(axis=0,inplace=True)\n",
    "    # Let's have a look\n",
    "    print(\"Final data set for the classification of occupations:\")\n",
    "    print(occupation_final_df.head(5))\n",
    "    # Export to a json file\n",
    "    if not exists(\"Brexit_datas/occupation_class/classified_occupation.json\"):\n",
    "        occupation_final_df.to_json(\"Brexit_datas/occupation_class/classified_occupation.json\")\n",
    "else:\n",
    "    print(\"Then please classify the remaining occupations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Label remaining occupations***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupation_final_df = pd.read_json(\"Brexit_datas/occupation_class/classified_occupation.json\").set_index(\"occupation\")\n",
    "\n",
    "# Let's have a look at the supercategories\n",
    "print(list(pd.unique(occupation_final_df.Category.apply(pd.Series).stack())))\n",
    "\n",
    "# Let's replace this into the aug_quotebank dataset\n",
    "def replace_occupation(occupation):\n",
    "    if type(occupation) == list:\n",
    "        if len(occupation) > 0:\n",
    "            new_occupation = []\n",
    "            for job in occupation:\n",
    "                try:\n",
    "                    new_occupation += occupation_final_df.loc[job,\"Category\"]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            if len(new_occupation) > 0:\n",
    "                return list(set(new_occupation))\n",
    "            else:\n",
    "                return pd.NA\n",
    "                \n",
    "    else:\n",
    "        return pd.NA\n",
    "\n",
    "aug_quotebank_brexit.occupation = aug_quotebank_brexit.occupation.apply(replace_occupation)\n",
    "aug_quotebank_brexit.head(2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_levensthein(country,proposal=5):\n",
    "\n",
    "    # Given the initial country, should we deviate from this initial one ?\n",
    "    message = country + \" Any deviation ?\"\n",
    "    deviation = input(message)\n",
    "\n",
    "    # If a deviation was specified then replace\n",
    "    if len(deviation) > 0:\n",
    "        country = deviation\n",
    "    \n",
    "    # Compute the levenshtein distance with the normalized country\n",
    "    value = []\n",
    "    for existing in list(current_countries.Country):\n",
    "        leven_distance = stringdist.levenshtein_norm(country,existing)\n",
    "        value.append(leven_distance)\n",
    "        \n",
    "    value = np.array(value)\n",
    "    # Sort the countries according to their closeness\n",
    "    closer = current_countries.Country.values[np.argsort(value)]\n",
    "    # Get the \"proposal\" closest countries\n",
    "    closer = closer[:proposal]\n",
    "\n",
    "    # Display potential countries\n",
    "    message = country + \" potential candidates \\n\" + \" --- \".join(list(closer))\n",
    "\n",
    "    fine = True\n",
    "    while fine:\n",
    "        try:\n",
    "            # Give the index of the potential country that will replace the initial country\n",
    "            idx_to_keep = int(input(message))\n",
    "            fine = False\n",
    "        except ValueError:\n",
    "            print(\"Please specify an integer\")\n",
    "            fine = True\n",
    "\n",
    "    # If negative index then discard\n",
    "    if idx_to_keep < 0:\n",
    "        return pd.NA\n",
    "    # Else return the closer one\n",
    "    else:\n",
    "        return closer[idx_to_keep]\n",
    "\n",
    "\n",
    "if not exists(\"Brexit_datas/country_class/country_final_mapping.csv\"):\n",
    "\n",
    "    # Get the list of existing countries \n",
    "    current_countries = pd.read_excel(\"Brexit_datas/country_class/countries.xlsx\")\n",
    "\n",
    "    # Remove capital letters and special characters\n",
    "    current_countries.Country = current_countries.Country.str.lower()\n",
    "    current_countries.Country = current_countries.Country.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "\n",
    "    # Country values that we currently have\n",
    "    countries_to_map = pd.DataFrame(unique_values[\"nationality\"],columns=[\"Country\"])\n",
    "    # Remove capital letters and special characters\n",
    "    countries_to_map.Country = countries_to_map.Country.str.lower()\n",
    "    countries_to_map.Country = countries_to_map.Country.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    # Drop eventual duplicates\n",
    "    countries_to_map.drop_duplicates(subset=[\"Country\"],inplace=True)\n",
    "\n",
    "    # Let's perform a first merge\n",
    "    countries_to_map = pd.merge(current_countries,countries_to_map,left_on=\"Country\",right_on=\"Country\",how=\"right\")\n",
    "\n",
    "    # New column new countries\n",
    "    countries_to_map.loc[~countries_to_map.ISO.isna(),[\"real_country\"]] = countries_to_map[~countries_to_map.ISO.isna()].Country\n",
    "    # Fill the remaining countries manually\n",
    "    countries_to_map[countries_to_map.ISO.isna()].real_country= countries_to_map[countries_to_map.ISO.isna()].Country.apply(compare_levensthein)\n",
    "    # Remove countries that didn't find a correspondance\n",
    "    countries_to_map = countries_to_map[~countries_to_map.real_country.isna()].drop(columns=\"ISO\")\n",
    "    # Export the result\n",
    "    countries_to_map.to_csv(\"Brexit_datas/country_class/country_final_mapping.csv\",index_col=0)\n",
    "\n",
    "else:\n",
    "    # Read the already created csv\n",
    "    countries_to_map = pd.read_csv(\"Brexit_datas/country_class/country_final_mapping.csv\",index_col=0)\n",
    "\n",
    "# Set the index \n",
    "countries_to_map.set_index(\"Country\",inplace=True)\n",
    "countries_to_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_category(initial,lookup_table):\n",
    "\n",
    "    # Initialize the potential new_values\n",
    "    new_values = []\n",
    "\n",
    "    # Get the original values\n",
    "    original = list(lookup_table.index.values)\n",
    "    # Check that the element is a list\n",
    "    if isinstance(initial,list):\n",
    "        # For each word in the list\n",
    "        for old_original in initial:\n",
    "            # Check if there exists a corresponding word\n",
    "            if old_original.lower() in original:\n",
    "                # Then get the new value\n",
    "                new_values.append(lookup_table.loc[old_original.lower(),lookup_table.columns[0]])\n",
    "        if len(new_values) == 0:\n",
    "            return pd.NA\n",
    "        else:\n",
    "            return new_values\n",
    "    else:\n",
    "        return pd.NA\n",
    "\n",
    "# Replace initial country by normalized countries\n",
    "aug_quotebank_brexit.loc[:,\"nationality\"] = aug_quotebank_brexit.nationality.apply(match_category,lookup_table=countries_to_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding to remove categories with low number of occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_maps = {}\n",
    "\n",
    "# define threshold for each target column\n",
    "threshold_maps[\"nationality\"] = 80\n",
    "threshold_maps[\"party\"] = 50\n",
    "threshold_maps[\"academic_degree\"] = 20\n",
    "# threshold_maps[\"ethnic_group\"] = 20\n",
    "threshold_maps[\"religion\"] = 50\n",
    "\n",
    "# function filtering \n",
    "def intersection_test(xlist, unique_set):\n",
    "    if not isinstance(xlist, list):\n",
    "        return pd.NA\n",
    "    if len(xlist) == 0:\n",
    "        return pd.NA\n",
    "    new_list = list(set(xlist).intersection(unique_set))\n",
    "    if len(new_list) > 0:\n",
    "        return new_list\n",
    "    else:\n",
    "        return pd.NA\n",
    "\n",
    "\n",
    "for col, threshold in threshold_maps.items():\n",
    "    print(\"Column\", col, \", Keep only values with occurrence >\", threshold)\n",
    "    # get unique quantities inside list objects and their count\n",
    "    unique_count = aug_quotebank_brexit[col].apply(pd.Series).stack().to_frame().rename(columns={0:\"Value\"})\n",
    "    unique_count = unique_count.groupby(\"Value\").size().reset_index(name=\"Count\").sort_values(by=\"Count\",ascending=False)\n",
    "    # define intersection for the specific unique values\n",
    "    intersection_unique = lambda xlist: intersection_test(xlist, unique_count[unique_count[\"Count\"] > threshold].Value.values)\n",
    "    # apply filtering\n",
    "    aug_quotebank_brexit[col] = aug_quotebank_brexit[col].apply(intersection_unique)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Academic degree gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to gather academic degree of people into a higher categories such as PhD, professor, master bachelor and so on. As the number of different remaining categories for the academic degree is quite low, it can be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File where we will match higher categories\n",
    "file_academic = \"Brexit_datas/academic_degree/original_academic.csv\"\n",
    "\n",
    "if not exists(file_academic):\n",
    "    # Get the unique values for academic\n",
    "    Academic_filter = aug_quotebank_brexit[\"academic_degree\"].apply(pd.Series).stack().to_frame().rename(columns={0:\"Value\"})\n",
    "    # Group by value, count and export\n",
    "    Academic_filter.groupby(\"Value\").count().to_csv(file_academic)\n",
    "\n",
    "# Please fill the file \n",
    "answer = input(\"Did you gather academic degrees categories into higher categories ?\")\n",
    "\n",
    "if answer.lower() == \"yes\":\n",
    "    # Read the updated matching file \n",
    "    Academic_filter = pd.read_csv(file_academic).set_index(\"Value\")\n",
    "    # Remove capital letters\n",
    "    Academic_filter.index = Academic_filter.index.str.lower()\n",
    "    # match category\n",
    "    aug_quotebank_brexit.loc[:,\"academic_degree\"] = aug_quotebank_brexit.academic_degree.apply(match_category,lookup_table=Academic_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We want to remove useless information: for instance \n",
    "# someone with bachelor and Phd should be replaced by Phd only\n",
    "def acadedmic_order(degrees,order):\n",
    "    \n",
    "    # Initialize max and argmax\n",
    "    max = -1\n",
    "    new = None\n",
    "\n",
    "    if isinstance(degrees,list):\n",
    "        # Loop over its potential degrees\n",
    "        for degree in degrees:\n",
    "            # Replace if higher degree \n",
    "            if max < order[degree]:\n",
    "                max = order[degree]\n",
    "                new = degree\n",
    "        if new is None:\n",
    "            print(\"Empty list\")\n",
    "            return pd.NA\n",
    "        return [new]\n",
    "    return pd.NA\n",
    "\n",
    "# Define the order between degrees \n",
    "Ordered_list = {\"Professor\":4,\"Phd\":3,\"Master\":2,\"Bachelor\":1,\"Other\":0}\n",
    "\n",
    "# Apply that man :)\n",
    "aug_quotebank_brexit.loc[:,\"academic_degree\"] = aug_quotebank_brexit.academic_degree.apply(acadedmic_order,order=Ordered_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Religion gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to actually do the same thing for the religion feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_religion = \"Brexit_datas/religion/original_religion.csv\"\n",
    "\n",
    "if not exists(file_religion):\n",
    "    Religion_filter = aug_quotebank_brexit[\"religion\"].apply(pd.Series).stack().to_frame().rename(columns={0:\"Value\"})\n",
    "    Religion_filter.groupby(\"Value\").count().to_csv(file_religion)\n",
    "\n",
    "answer = input(\"Did you gather religion categories into higher categories ?\")\n",
    "\n",
    "if answer.lower() == \"yes\":\n",
    "    Religion_filter = pd.read_csv(file_religion).set_index(\"Value\")\n",
    "    Religion_filter.index = Religion_filter.index.str.lower()\n",
    "    Religion_filter.head()\n",
    "    aug_quotebank_brexit.loc[:,\"religion\"] = aug_quotebank_brexit.religion.apply(match_category,lookup_table=Religion_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ethnic Group gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the information contained in ethnic group is actually already contained in the other features so I think we can safely drop it\n",
    "# Ethnic group has been removed from the columns_to_map list\n",
    "# aug_quotebank_brexit.drop(columns=[\"ethnic_group\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate period for each quotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define period cut (6 = 1 year, 12 = 6 months)\n",
    "periods_nb = 12\n",
    "\n",
    "# Visualize with an histogram on the date column\n",
    "fig = px.histogram(aug_quotebank_brexit,x=\"date\", nbins=12)#, color_discrete_sequence = px.colors.cyclical.Phase)\n",
    "fig.update_layout(title=\"Number of quotations about Brexit accross time\")\n",
    "fig.show()\n",
    "\n",
    "# Get period cut\n",
    "period_list = list(range(periods_nb - 1))\n",
    "period_cut = pd.date_range('2015-01-01', freq='6M', periods=periods_nb)\n",
    "\n",
    "aug_quotebank_brexit[\"period\"] = pd.cut(aug_quotebank_brexit.date.astype(np.int64)//10**9,\n",
    "                   bins=period_cut.astype(np.int64)//10**9,\n",
    "                   labels=period_list)\n",
    "\n",
    "aug_quotebank_brexit.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually select and eventually merge periods in order to have almost a equal distribution between periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually merge periods (WARNING: it depends on periods )\n",
    "\n",
    "def reset_periods(x):\n",
    "    if x <= 5:\n",
    "        return period_labels[0]\n",
    "    elif x in [6, 7]:\n",
    "        return period_labels[1]\n",
    "    elif x == 8:\n",
    "        return period_labels[2]\n",
    "    else:\n",
    "        return period_labels[3]\n",
    "    \n",
    "if (type(aug_quotebank_brexit[\"period\"].dtypes) is pd.core.dtypes.dtypes.CategoricalDtype):\n",
    "    # print counts\n",
    "    print(\"Count occurrences for each periods, they must correspond to the histogram above.\")\n",
    "    for i in period_list:\n",
    "        print(\"Size of period\", i, \":\", len(aug_quotebank_brexit.loc[aug_quotebank_brexit.period == i]))\n",
    "\n",
    "    # apply period simplification\n",
    "    aug_quotebank_brexit[\"period\"] = aug_quotebank_brexit[\"period\"].apply(reset_periods)\n",
    "\n",
    "\n",
    "print(\"Check the occurrences after the period recalibration\")\n",
    "for i in period_labels:\n",
    "    print(\"Size of period\", i, \":\", len(aug_quotebank_brexit.loc[aug_quotebank_brexit.period == i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='one_hot_encoding'></a>\n",
    "\n",
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot vectorization of columns cotaining categorical values\n",
    "dummy_col = \"AAADummy column for the sake\"\n",
    "# Make a copy\n",
    "oneh_quotebank_brexit = aug_quotebank_brexit.copy()\n",
    "\n",
    "# Check that the element is a list that contains only one string\n",
    "def ensure_list(value):\n",
    "  if isinstance(value, list):\n",
    "    for i in range(len(value)):\n",
    "      value[i] = str(value[i])\n",
    "  elif not pd.isna(value):\n",
    "    value = [value]\n",
    "  return value\n",
    "\n",
    "# Loop over categorical columns\n",
    "for col in columns_to_map:\n",
    "  # Get the serie\n",
    "  col_serie = aug_quotebank_brexit[col].copy().apply(ensure_list)\n",
    "  # Change nan values to a list containing a dummy column\n",
    "  col_serie[col_serie.isna()] = col_serie[col_serie.isna()].apply(lambda x: [dummy_col])\n",
    "    \n",
    "  # One hot vectorize\n",
    "  categorical_df = pd.get_dummies(col_serie.apply(pd.Series).stack()).groupby(level=0).sum()\n",
    "    \n",
    "  # Drop the dummy column\n",
    "  categorical_df.drop(columns=[dummy_col],inplace=True)\n",
    "  # Refresh unique values\n",
    "  unique_values[col] = categorical_df.columns\n",
    "    \n",
    "  # Join with quotebank brexit\n",
    "  oneh_quotebank_brexit = oneh_quotebank_brexit.join(categorical_df,how=\"left\",rsuffix=col[:3])\n",
    "  print(\"One hot vectorizing : \",col,\n",
    "        \"| NaN values : \",categorical_df.isna().apply(lambda x: x*1).sum().sum(),\n",
    "        \"| Number of different categories : \",len(categorical_df.columns),\n",
    "        \"| Shape reduced ? \",categorical_df.shape,oneh_quotebank_brexit.shape)\n",
    "  # Drop the categorical column\n",
    "  oneh_quotebank_brexit.drop(columns=col,inplace=True)\n",
    "  # Check for NaN values\n",
    "  print(\"Any NA in the final dataframe: \",oneh_quotebank_brexit.isna().apply(lambda x: x*1).sum().sum())\n",
    "\n",
    "print(\"Shape of the final data frame\",oneh_quotebank_brexit.shape)\n",
    "print(\"Any NA in the final dataframe: \",oneh_quotebank_brexit.isna().apply(lambda x: x*1).sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_onehot = \"Brexit_datas/one_hot_quotebank_brexit.json.bz2\"\n",
    "\n",
    "\n",
    "if not exists(file_onehot):\n",
    "    oneh_quotebank_brexit.to_json(file_onehot)\n",
    "else:\n",
    "    oneh_quotebank_brexit = pd.read_json(file_onehot,compression=\"bz2\")\n",
    "oneh_quotebank_brexit.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clustering_task'></a>\n",
    "\n",
    "# Quotations and speakers clustering\n",
    "\n",
    "The last preprocessing step consists of clustering the quotations as well as the speakers. This clustering will later be used to create a Recommandation Tool in the context of Brexit. Quotations and speakers that carry similar attributes/ideas will belong to the same cluster. Performing such task can be performed using the following pipeline:\n",
    "1. The first step is to convert sentences into vectors. This task can be achieved using the [SentenceTransformer](https://www.sbert.net/docs/usage/semantic_textual_similarity.html) deep neural network. The vector obtained from this operation cab be then concatenated with the other existing features (that would be converted to one hot vectors if necessary) (ALREADY DONE).\n",
    "2. The second step consists in reducing the dimension of the data before applying the clustering algorithm. This task can be achieved using the [Locally Linear Embeddings](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding) algorithm. This algorithm is considered to be an efficient non-linear dimensionality reduction method.\n",
    "3. The third step is specific to speaker clustering. Indeed, the vectorization of quotes as well as the reduction of dimensionality are only applied to quotes. Thus, we need to perform an **aggregation** to be able to attribute a vector to each speaker. For each speaker, this aggregation can simply be done by taking the mean of the vectors associated with each of their quotations. \n",
    "4. The last step consists in performing the clustering operation. This task can be achieved using [Spectral Clustering](#https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering) method.\n",
    "\n",
    "### Sentiment amplification\n",
    "\n",
    "In order to calibrate the importance of the *sentiment score* in the quotes vector space, a coefficient of amplification `sentiment_amplification` is applied to the integer coded as `sentiment_score`.\n",
    "\n",
    "This process aims at increasing the distance among the vectors grouping vectors with the same sentiment score. A priori, this boosts the labelling process.\n",
    "\n",
    "### Locally linear embedding\n",
    "\n",
    "This algorithm aims at preserving the neighbouring points. The process is described as follows: \n",
    "- For each point, its nearest neighbors are determined. \n",
    "- Then it tries to project the new point in the embedded space such that its neighbors are preserved\n",
    "This spectral dimensionality reduction technique is non-linear, fast and reliable enough to handle big and complex dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    User-defined parameters for the task\n",
    "\"\"\"\n",
    "# coefficient of amplification of sentiment\n",
    "# auto = give sentiment the same weight as the other quantities combined together\n",
    "sentiment_amplification = 'auto'\n",
    "\n",
    "\n",
    "\n",
    "# Restrict dataframe size of tsne_debug is True\n",
    "emb_debug = True\n",
    "emb_size = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataframe for clustering (keep only involved columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_to_int(value):\n",
    "  return int(value == \"Positive\") - int(value == \"Negative\")\n",
    "\n",
    "def standardize(df):\n",
    "    return (df - df.mean(axis=0)) / df.std(axis=0)\n",
    "\n",
    "  \n",
    "columns_to_drop = [\"Age\", \"date\", \"quoteID\",\"qids\",\"phase\",\"probas\",\"urls\",\"date_of_birth\",\"label\"]\n",
    "\n",
    "# restrict dataframe to the one needed\n",
    "oneh_cluster_df = oneh_quotebank_brexit.drop(columns=columns_to_drop)\n",
    "\n",
    "print(\"Is there any na values ?\",oneh_cluster_df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='One-hot'></a>\n",
    "\n",
    "## Compose the vectorized dataframe\n",
    "\n",
    "1. Convert sentiment score into signed integer format: \"Positive\" = 1, \"Negative\" = -1, \"Neutral\" = 0\n",
    "2. For each column concerning the speaker information, generate a dummy dataframe (see DataFrame.get_dummy)\n",
    "3. Concatenate along columns all obtained dataframe\n",
    "4. Average all rows matching the same speaker (which is set as index)\n",
    "5. Normalize dataset by row and amplify sentiment_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the filtered dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode quotation if necessary\n",
    "if not exists(\"Brexit_datas/aug_vector_quotes.csv.gz\"):\n",
    "    encoder = SentenceTransformer(sentence_transformer_type)\n",
    "    # Encode quotations\n",
    "    quotes_encoded = encoder.encode(aug_quotebank_brexit['quotation'].values, convert_to_numpy=True, show_progress_bar=True)\n",
    "    # Convert to df\n",
    "    quotes_df = pd.DataFrame(quotes_encoded, index = aug_quotebank_brexit.index)\n",
    "    # Add significant columns\n",
    "    quotes_df[\"speaker\"] = aug_quotebank_brexit[\"speaker\"].values\n",
    "    # Export into a compressed format\n",
    "    quotes_df.to_csv(\"Brexit_datas/aug_vector_quotes.csv.gz\")\n",
    "    \n",
    "else:\n",
    "    # Read the file\n",
    "    quotes_df = pd.read_csv(\"Brexit_datas/aug_vector_quotes.csv.gz\",index_col=0,compression=\"gzip\")\n",
    "\n",
    "#print(\"Size of the dataset to encode: \", len(aug_quotebank_brexit))\n",
    "#print(\"Size of the one-hot dataframe: \", len(oneh_cluster_df))\n",
    "print(\"Size of the vectorized quotes: \", len(quotes_df))\n",
    "\n",
    "quotes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compose a vectorized dataframe\n",
    "\n",
    "Merge `oneh_cluster_df` with `quotes_df` and take the average over all quotations belonging to the same *speaker* and *period*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Compose a vectorized dataframe\n",
    "\"\"\"\n",
    "\n",
    "# normalize quotations vector\n",
    "if 'speaker' in quotes_df.columns:\n",
    "    quotes_df.drop(columns=['speaker'], inplace=True) # speaker is not needed, already contained in oneh_cluster_df\n",
    "quotes_df = standardize(quotes_df)\n",
    "\n",
    "# Merge the two data frames\n",
    "cluster_full_df = pd.concat([oneh_cluster_df.drop(columns=\"quotation\"), quotes_df.loc[oneh_cluster_df.index]],axis = 1)\n",
    "\n",
    "# convert sentiment_score to int format\n",
    "cluster_full_df['sentiment_score'] = cluster_full_df['sentiment_score'].apply(sentiment_to_int)\n",
    "\n",
    "# add period column\n",
    "#cluster_full_df['period'] = oneh_cluster_df['period']\n",
    "\n",
    "# average over the same speaker and period\n",
    "# treat each period differently\n",
    "cluster_full_df = cluster_full_df.groupby(['speaker', 'period']).agg(np.mean)\n",
    "\n",
    "# normalize numOccurences df by row\n",
    "cluster_full_df.numOccurrences = standardize(cluster_full_df.numOccurrences)\n",
    "\n",
    "#amplify sentiment\n",
    "# determine sentiment amplification if automatic\n",
    "if sentiment_amplification == 'auto':\n",
    "    sentiment_amplification = cluster_full_df.drop(columns = ['sentiment_score']).sum(axis=1).mean(axis=0)\n",
    "    print(\"Automatically determined sentiment score amplification:\", sentiment_amplification)\n",
    "    \n",
    "cluster_full_df['sentiment_score'] *= sentiment_amplification\n",
    "\n",
    "print(\"Is there any na values ?\",cluster_full_df.isna().sum().sum())\n",
    "cluster_full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TSNE'></a>\n",
    "\n",
    "## First analysis aggregation per time period\n",
    "   - Group by period\n",
    "   - Apply Locally Linear Embedding aggregation and visualize\n",
    "   - Detect outliers with a DBSCAN clustering and filter them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply T-stochastic neighboor embedding\n",
    "# NOT USED: one-hot vectorization gave undefined results\n",
    "#data_np_emb = TSNE(n_components=2, perplexity=30, n_iter=1000, verbose=True).fit_transform(full_data_tensor) # dim = Nxfinal_dim\n",
    "\n",
    "fig,axes = plt.subplots(len(period_labels)//2,2, figsize=[15,8])\n",
    "\n",
    "# copy the dataframe and create outlier column\n",
    "cluster_filtered_df = pd.DataFrame(columns=cluster_full_df.columns)\n",
    "\n",
    "for ax, period in zip(axes.flatten()[0:len(period_labels)], period_labels):\n",
    "    \n",
    "    # restrict by period\n",
    "    period_cluster_df = cluster_full_df.xs(period, level=1)\n",
    "    \n",
    "    # embed data\n",
    "    print(\"Size of the data which has been embedded: \", period_cluster_df.shape, \", period: \", period)\n",
    "    embedder = LocallyLinearEmbedding(n_components=2, n_neighbors=15, max_iter=500, method='standard')\n",
    "    data_np_emb = embedder.fit_transform(df_to_tensor(period_cluster_df)).transpose()\n",
    "    print(\"Embedded done, estimated reconstruction error: \", embedder.reconstruction_error_)\n",
    "    \n",
    "    # standardize X,Y distribution for embedded data\n",
    "    std = standardize(data_np_emb.transpose())\n",
    "    \n",
    "    # apply DBSCAN clustering and detect outliers\n",
    "    print(\"\\nClustering 2D sample\")\n",
    "    dbscanner = DBSCAN(eps = 1.5, min_samples=10).fit(std)\n",
    "    print(\"Outliers found: \", len(std[dbscanner.labels_ == -1]))\n",
    "    \n",
    "    # plot \n",
    "    ax.set_title('Period: %s' % period)\n",
    "    #ax.tick_params(left=False,bottom=False,labelleft=False,labelbottom=False) \n",
    "    \n",
    "    for label in set(dbscanner.labels_):\n",
    "        points = std[dbscanner.labels_ == label].transpose()\n",
    "        label = \"Label %d\" % label if label != -1 else \"Outliers\"\n",
    "        ax.scatter(points[0], points[1], label=label)\n",
    "    \n",
    "    ax.legend()\n",
    "        \n",
    "    print(\"Length before:\", len(period_cluster_df))\n",
    "    # remove outliers\n",
    "    period_cluster_df.drop((period_cluster_df[dbscanner.labels_ == -1]).index, inplace=True)\n",
    "    print(\"Length after:\", len(period_cluster_df))\n",
    "    \n",
    "    # correct index and append to new set \n",
    "    period_cluster_df['period'] = period\n",
    "    period_cluster_df.set_index('period', drop=True, append=True, inplace=True)\n",
    "\n",
    "    cluster_filtered_df = pd.concat([period_cluster_df, cluster_filtered_df], verify_integrity=True, copy=False)\n",
    "   \n",
    "\n",
    "plt.show()\n",
    "cluster_filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup filtered data set\n",
    "\n",
    "file_cluster = \"Brexit_datas/vector_cluster.json.bz2\"\n",
    "\n",
    "\n",
    "if not exists(file_cluster):\n",
    "    cluster_filtered_df.to_json(file_cluster)\n",
    "else:\n",
    "    cluster_filtered_df = pd.read_json(file_cluster,compression=\"bz2\")\n",
    "    \n",
    "cluster_filtered_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Clustering'></a>\n",
    "\n",
    "## Apply Spectral (or Agglomerative) clustering algorithm\n",
    "    - Checking silhouette score, find optimal number of clusters\n",
    "    - Store optimal clustering for each period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters to be identified \n",
    "nb_clusters = range(2,5)\n",
    "\n",
    "# buffer clustering optimum parameters\n",
    "cluster_opt = {}\n",
    "\n",
    "for period in period_labels:\n",
    "    \n",
    "    silhouette_scores = {}\n",
    "    clust_labels = {}\n",
    "    \n",
    "    # Compute silhouette distance each period\n",
    "    print(\"Validating period: \", period)\n",
    "    for n_clust in nb_clusters:\n",
    "        \n",
    "        # define model\n",
    "        #model = AgglomerativeClustering(n_clust)\n",
    "        model = SpectralClustering(n_clust)\n",
    "        X = cluster_filtered_df.xs(period, level=1)\n",
    "        print(\"Clustering a\", X.shape, \" shaped data. With \", n_clust, \" clusters\")\n",
    "        model.fit(X)\n",
    "    \n",
    "        # compute and store silhouette score\n",
    "        silhouette_scores[n_clust] = metrics.silhouette_score(X, model.labels_, metric='euclidean')\n",
    "        print(\"Got a silhouette score of \", silhouette_scores[n_clust])\n",
    "        # store labels\n",
    "        clust_labels[n_clust] = model.labels_\n",
    "        \n",
    "    # get configuration maximising silhoutte distance\n",
    "    max_score = max(silhouette_scores)\n",
    "    optimum_nb = max(silhouette_scores, key=silhouette_scores.get)\n",
    "    \n",
    "    print(\"Got a maximum silhouette score:\", max_score, \", with optimum:\", optimum_nb)\n",
    "    \n",
    "    # store the optimum\n",
    "    cluster_opt[period] = [optimum_nb, clust_labels[optimum_nb]]\n",
    "\n",
    "pd.DataFrame.from_dict(cluster_opt, columns=['n_clusters', 'labels'], orient='index').drop(columns=['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize embedded optimum clustering results\n",
    "    - Embed data into two dimensions using Locally Linear Embedding\n",
    "    - Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(len(period_labels)//2,2, figsize=[15,8])\n",
    "\n",
    "for ax, period in zip(axes.flatten()[0:len(period_labels)], period_labels):\n",
    "    \n",
    "    # Embed into two dimension\n",
    "    period_cluster_df = cluster_filtered_df.xs(period, level=1)\n",
    "    print(\"Size of the data which has been embedded: \", period_cluster_df.shape, \", period: \", period)\n",
    "    \n",
    "    embedder = LocallyLinearEmbedding(n_components=2, n_neighbors=15, max_iter=500, method='standard')\n",
    "    data_np_emb = embedder.fit_transform(df_to_tensor(period_cluster_df))\n",
    "    print(\"Embedded done, estimated reconstruction error: \", embedder.reconstruction_error_)\n",
    "    \n",
    "    # plot different clusters\n",
    "    labels = cluster_opt[period][1]\n",
    "    \n",
    "    # plot \n",
    "    ax.set_title('Period: %s' % period)\n",
    "    ax.tick_params(left=False,bottom=False,labelleft=False,labelbottom=False) \n",
    "    \n",
    "    for lab in set(labels):\n",
    "        points = data_np_emb[labels == lab].transpose()\n",
    "        ax.scatter(points[0], points[1], label=lab)\n",
    "        \n",
    "    ax.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Results'></a>\n",
    "\n",
    "# Generate the results for the final story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Statistics'></a>\n",
    "\n",
    "## General Statistics\n",
    "\n",
    "Now that we had preprocessed the datas let's have a look at different basic statistics to explore deeply the dataset. Let's first look at the distribution of the quotations accross time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(quotebank_brexit,x=\"date\")\n",
    "fig.update_layout(title=\"Number of quotations about Brexit accross time\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the top 20 countries that are providing the most quotations about Brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df = oneh_quotebank_brexit.loc[:,unique_values[\"nationality\"]].sum(axis=0).T.to_frame().reset_index()\n",
    "country_df = country_df.sort_values(by=0,ascending=False)\n",
    "fig = px.bar(country_df,y=0,x=\"index\",log_y=True)\n",
    "fig.update_layout(title=\"Number of quotations about Brexit accross time\",\n",
    "                  xaxis_title=\"Country\",yaxis_title=\"Count\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the top sectors that are providing the most quotations on Brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df = oneh_quotebank_brexit.loc[:,unique_values[\"occupation\"]].sum(axis=0).T.to_frame().reset_index()\n",
    "job_df = job_df.sort_values(by=0,ascending=False)\n",
    "fig = px.bar(job_df,y=0,x=\"index\",log_y=True)\n",
    "fig.update_layout(title=\"Number of quotations about Brexit accross time\",\n",
    "                  xaxis_title=\"Sector\",yaxis_title=\"Count\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the top 10 speakers that are providing the most quotations on Brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_quotebank_brexit.groupby(\"speaker\").size().reset_index(name=\"count\").loc[:,[\"speaker\",\"count\"]].sort_values(by=\"count\",ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_by_year(low_year,up_year, col=None):\n",
    "    year_col = pd.DatetimeIndex(oneh_quotebank_brexit.date).year\n",
    "    if col is None:\n",
    "        cols = [\"sentiment_score\"]\n",
    "    else:\n",
    "        cols = list(unique_values[col]) + [\"sentiment_score\"]\n",
    "    filter_df = oneh_quotebank_brexit.loc[(year_col >= low_year) & (year_col <= up_year),cols]\n",
    "    filter_df = filter_df.groupby(\"sentiment_score\").sum()\n",
    "    count = filter_df.sum(axis=0)\n",
    "    filter_df = (filter_df * 100/ filter_df.sum(axis=0)).T.reset_index()\n",
    "    filter_df[\"count\"] = count.values\n",
    "    return filter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pie charts [Introduction] specific to United Kingdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UK_df = oneh_quotebank_brexit[oneh_quotebank_brexit[\"united kingdom\"] == 1]\n",
    "years = list(range(2015,2020))\n",
    "annotations = []\n",
    "stepsize = 1/len(years)\n",
    "horizontal_spacing = 0.03\n",
    "\n",
    "fig = make_subplots(rows=1, cols=6, specs=[[{'type':'domain'}]*6],horizontal_spacing=horizontal_spacing)\n",
    "\n",
    "for i,pie_year in enumerate(years):\n",
    "    year_col = pd.DatetimeIndex(UK_df.date).year\n",
    "    filter_df = UK_df.loc[year_col == pie_year,[\"sentiment_score\"]]\n",
    "    filter_df = filter_df.groupby(\"sentiment_score\").size().reset_index(name=\"Count\")\n",
    "    pie_trace = go.Pie(labels=filter_df.sentiment_score,values=filter_df.Count,name=str(pie_year),title=str(pie_year))\n",
    "    fig.add_trace(pie_trace,1,i+1)\n",
    "    x_value = fig.data[i][\"domain\"][\"x\"]\n",
    "    # annotations.append(dict(text=str(pie_year),x=(x_value[1] + 2*x_value[0])/3,y=0.5, font_size=20, showarrow=False))\n",
    "\n",
    "fig.update_traces(hole=.4, hoverinfo=\"label+percent+name\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Evolution of the sentiment towards Brexit over the years [United Kingdom]\",\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Country'></a>\n",
    "\n",
    "## Analyze the way Brexit is perceived in European countries\n",
    "\n",
    "Recall that the goal is to analyze the way Brexit is perceived by each Europe country based on the sentiment carried by the quotation. Besides, we would like to add the time dimension to this analysis, meaning that we would like to follow the evolution of the overall feelings towards Brexit. A view of the expected result is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Brexit_datas\\country_class\\countries.geojson\") as file:\n",
    "    country_gj = geojson.load(file)\n",
    "\n",
    "current_countries = pd.read_excel(\"Brexit_datas\\country_class\\countries.xlsx\")\n",
    "\n",
    "# Remove capital letters and special characters\n",
    "current_countries.Country = current_countries.Country.str.lower()\n",
    "current_countries.Country = current_countries.Country.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "\n",
    "ISO_mapping = pd.read_csv(\"Brexit_datas\\country_class\\ISO2_ISO3.csv\")\n",
    "current_countries = pd.merge(ISO_mapping,current_countries,left_on=\"ISO2\",right_on=\"ISO\",how=\"right\").drop(columns=[\"ISO\",\"ISO2\"])\n",
    "\n",
    "current_countries = current_countries[~current_countries.ISO3.isna()]\n",
    "\n",
    "country_analysis = select_by_year(2015,2020,\"nationality\").rename(columns={\"index\":\"country\"})\n",
    "country_analysis[\"Sentiment\"] = -1*country_analysis.Negative + country_analysis.Positive\n",
    "\n",
    "country_analysis = pd.merge(country_analysis,current_countries,left_on=\"country\",right_on=\"Country\",how=\"left\").drop(columns=[\"country\"])\n",
    "\n",
    "fig = px.choropleth_mapbox(country_analysis, geojson=country_gj, locations='ISO3', color='Sentiment',\n",
    "                           featureidkey=\"properties.ISO_A3\",\n",
    "                           color_continuous_scale=\"delta\",\n",
    "                           mapbox_style=\"carto-positron\",\n",
    "                           zoom=2.5, center = {\"lat\": 48.856613, \"lon\": 2.352222},\n",
    "                           opacity=1,hover_data=[\"Country\",\"count\"]\n",
    "                          )\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Evolution of the sentiment towards Brexit over the countries [2015 - 2020]\",\n",
    "        'y':0.96,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "\n",
    "fig.update_layout(height=800,width=1300)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Sector'></a>\n",
    "\n",
    "## Analyze the way Brexit is perceived in different sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_df = select_by_year(2015,2020,\"occupation\").rename(columns={\"index\":\"occupation\"})\n",
    "\n",
    "specs = [[{\"rowspan\": 3, \"colspan\": 3,'type':'domain'}] + [None]*2 + [{'type':'domain'}]*6,\n",
    "        [None]*3 + [{'type':'domain'}]*6,\n",
    "        [None]*3 + [{'type':'domain'}]*6,]\n",
    "\n",
    "fig = make_subplots(rows=3,cols=9,specs=specs)\n",
    "\n",
    "selected = \"Art\"\n",
    "labels = sector_df.columns[-4:-1]\n",
    "trace_selected = go.Pie(labels=labels,values=sector_df[sector_df.occupation == selected][labels].values.reshape(-1),\n",
    "                        title=selected,titleposition=\"top center\")\n",
    "fig.add_trace(trace_selected,row=1,col=1)\n",
    "\n",
    "start = 0\n",
    "for index, row in sector_df.iterrows():\n",
    "    if selected != row[\"occupation\"]:\n",
    "        trace = go.Pie(labels=labels,values=sector_df.loc[index,labels].values,title=row[\"occupation\"])\n",
    "        fig.add_trace(trace,row = start//6+1,col=4+start%6)\n",
    "        start += 1\n",
    "\n",
    "fig.update_traces(hoverinfo='label+percent+name', textinfo='none')\n",
    "fig.update_layout(height=600,width=1500)\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"Evolution of the sentiment towards Brexit over the different sectors [2015 - 2020]\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_df = select_by_year(2015,2020,\"occupation\")\n",
    "\n",
    "fig = px.bar(sector_df, x=\"index\",text=\"count\",y=sector_df.columns[-4:-1])\n",
    "fig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\n",
    "fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide',\n",
    "                  title=\"Sector analysis\",\n",
    "                  xaxis_title=\"Sector\",yaxis_title=\"Count\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Dynamic visualization with Dash, going to the url output by the cell below should give you a plot like that with a range slider to select the year range:***\n",
    "\n",
    "![Dash render](Images\\dash_sector.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    dcc.Graph(id=\"scatter-plot\"),\n",
    "    html.P(\"Year:\"),\n",
    "    dcc.RangeSlider(\n",
    "        id='range-slider',\n",
    "        min=2015, max=2020, step=1,\n",
    "        marks={2015: '2015', 2016:'2016',2017:'2017',2018:'2018',2019:'2019',2020: '2020'},\n",
    "        value=[2015,2020]\n",
    "    ),\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"scatter-plot\", \"figure\"), \n",
    "    [Input(\"range-slider\", \"value\")])\n",
    "def update_bar_chart(slider_range):\n",
    "    low, high = slider_range\n",
    "    sector_df = select_by_year(low,high,\"occupation\")\n",
    "    fig = px.bar(sector_df, x=\"index\",text=\"count\",y=sector_df.columns[-4:-1])\n",
    "    return fig\n",
    "\n",
    "app.run_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2Dplot'></a>\n",
    "\n",
    "## Visualize speakers orientation trough a 2D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Recommandation'></a>\n",
    "\n",
    "## Recommandation tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Stocks'></a>\n",
    "\n",
    "## Correlation with stocks\n",
    "It would be interesting to investigate a correlation between Brexit and the evolution of the british stock market. To do so, data from the FTSE100 was obtained: (TO BE CONTINUED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "FTSE_companies = pd.read_excel(\"Brexit_datas\\FTSE_100_list.xlsx\")\n",
    "tickers_FTSE = list(FTSE_companies.Ticker)\n",
    "\n",
    "# Get the data for the FTSE companies\n",
    "stock_action_FTSE = yf.download(tickers_FTSE,'2015-01-01','2020-08-01')['Adj Close']\n",
    "\n",
    "stock_action_FTSE = stock_action_FTSE.dropna(axis=1).reset_index()\n",
    "\n",
    "fig = px.line(stock_action_FTSE,x=\"Date\",y=stock_action_FTSE.columns[1:10],log_y=True)\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c7ebd9986d9dd66f5c55cec683961b660fa3bc87c5eb159f7033b074e7bd831"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
